[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "At Londogard we find order in chaos by creating structures and understanding out of unstructured data. Our belief is that by creating understanding of data new possibilities open up and a lot of automation is possible where today manual tedious work is applied.\nOur goal is to employ Machine Learning models that makes sense and provide value rather than being made to tick of a check-box. The aim is to provide Efficient, Performant, Measurable & Understandable models.\n\nEfficient should truly run on a single small machine (the edge).\n\nPerformant achieving very close to State-of-the-Art performance, striking a good balance of performance & efficiency.\n\nMeasurable in a way that makes sense for your personal use-case. Not some random metrics.\n\nUnderstandable to the level where you understand why a prediction was made as it was.\n\nThrough our blog & demos we try to show-case powerful models that all run on the same single Raspberry Pi 4 (4GB)\nEmail us @ dev@londogard.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Londogard Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nBaby Monitor pt 2\n\n\n\n\n\n\n\npython\n\n\n\n\nI built a baby monitor! Built using Python, Svelte & Raspberry Pi.\n\n\n\n\n\n\nFeb 6, 2023\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolars - A Refreshingly Great DataFrame Library\n\n\n\n\n\n\n\ndata-engineering\n\n\npipeline\n\n\ndata\n\n\n\n\nPolars is a DataFrame library written from ground-up to not only have a sensible API but also very efficient operations using multiple cores and clever optimizations such as predicate pushdown & much more!\n\n\n\n\n\n\nNov 30, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nProbabilistic Forecasting Made Simple\n\n\n\n\n\n\n\nmachine-learning\n\n\ntimeseries\n\n\n\n\nWhile researching probabilistic forecasting in a client project I managed to find a paper which opens the door to any neural network with dropout - which is the majority. That is, we can do probabilistic forecasting with essentially any network!\n\n\n\n\n\n\nNov 28, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nTimeseries Learnings at AFRY\n\n\n\n\n\n\n\nmachine-learning\n\n\ntimeseries\n\n\n\n\nSharing knowledge based on working a lot with timeseries\n\n\n\n\n\n\nNov 23, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBabymonitor #1\n\n\n\n\n\n\n\npython\n\n\nraspberrypi\n\n\n\n\nI’m building a babymonitor!\n\n\n\n\n\n\nNov 6, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker (Presentation)\n\n\n\n\n\n\n\npresentation\n\n\nmachine-learning\n\n\ndocker\n\n\n\n\nWhat is Docker? I went through Docker during our Journal Circle at AFRY X. This is a simple explanation for people not knowledgable about Docker.\n\n\n\n\n\n\nSep 7, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nGPT2-snapsvisor - Generating Swedish Drinking Songs\n\n\n\n\n\n\n\nmachine-learning\n\n\nnlp\n\n\nfun\n\n\n\n\nSnapsvisor is traditional Swedish Drinking Songs, sometimes they need some refreshing which I try to do through AI here! ;)\n\n\n\n\n\n\nJul 7, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Crypto Prices using Deep Learning (Time Series #3)\n\n\n\n\n\n\n\nmachine-learning\n\n\ntimeseries\n\n\nworkshop\n\n\n\n\nIn this post/Jupyter Notebook we’ll forecast Cryptocurrency prices using Deep Learning (PyTorch, TF/Keras & darts) and we’ll use both both simpler networks and more complex ones like NBEATs.\n\n\n\n\n\n\nMar 13, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Stock Prices using classical machine Learning (Time Series #2)\n\n\n\n\n\n\n\nmachine-learning\n\n\ntimeseries\n\n\nworkshop\n\n\n\n\nIn this post/Jupyter Notebook we’ll look at stocks, forecasting and predictions using classical machine learning (sklearn) approaches.\n\n\n\n\n\n\nMar 12, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposing & Working with Time Series (Time Series #1)\n\n\n\n\n\n\n\nmachine-learning\n\n\ntimeseries\n\n\nworkshop\n\n\n\n\nIn this post/Jupyter Notebook we’ll look at Time Series and theory surrounding them.\n\n\n\n\n\n\nMar 11, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nKotlinJS, ONNX and Deep Learning in the browser\n\n\n\n\n\n\n\nkotlin\n\n\nweb\n\n\ndeep-learning\n\n\n\n\nEver wanted to deploy State-of-the-Art Deep Learning models in the browser? In this blog you’ll learn about how to run inference through onnx webruntime directly inside the browser!\n\n\n\n\n\n\nJan 28, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelease nlp (londogard-nlp-toolkit) 1.1.0\n\n\n\n\n\n\n\nnlp\n\n\njvm\n\n\nkotlin\n\n\n\n\nnlp (londogard-nlp-toolkit) has had it’s 1.1.0 release recently with a lot of new functionality and multiple improvements to efficiency, dive in to understand more!\n\n\n\n\n\n\nJan 16, 2022\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nSeam Carving (Presentation & Workshop)\n\n\n\n\n\n\n\npresentation\n\n\njvm\n\n\nkotlin\n\n\nworkshop\n\n\n\n\nSeam Carving is the task to remove empty room in a image. Have you ever wished to do ‘Content Aware Scaling’? Learn it now!\n\n\n\n\n\n\nMay 17, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge Distillation (Presentation)\n\n\n\n\n\n\n\npresentation\n\n\nmachine-learning\n\n\n\n\nWhat is Knowledge Distillation? How can this allow us to further utilize models and increase efficiency manifold?\n\n\n\n\n\n\nApr 18, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: GitHub Pages + Progressive Web App (PWA) = ❤️\n\n\n\n\n\n\n\npwa\n\n\nTIL\n\n\n\n\nWhat are PWAs? How do I create or transform my web app into one? How can I deploy them (freely) via GitHub Pages?\n\n\n\n\n\n\nApr 11, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)\n\n\n\n\n\n\n\nnlp\n\n\nmachine-learning\n\n\nworkshop\n\n\n\n\nLearn how to fine-tune a Flair NER model and quantize a BERT model from Huggingface to achieve SotA performance & a much more efficient model.\n\n\n\n\n\n\nMar 29, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen to use what - RegExp, String Replace & Character Replace (JVM/Kotlin)\n\n\n\n\n\n\n\njvm\n\n\n\n\nA simple quick comparison on which method to use and in what case when replacing characters or strings in strings.\n\n\n\n\n\n\nMar 17, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nTransformers From Scratch\n\n\n\n\n\n\n\nnlp\n\n\nmachine-learning\n\n\ndeep-learning\n\n\nworkshop\n\n\n\n\nIn this post I walk through Self-Attention Transformers from scratch with demos at the end for Text Classification & Generation, where the PyTorch-code is wrapped by fast.ai to simplify end-2-end.\n\n\n\n\n\n\nFeb 18, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: GitPod - your editor in the cloud\n\n\n\n\n\n\n\njvm\n\n\nkotlin\n\n\nTIL\n\n\n\n\nGitPod is a Cloud IDE where you can run everything from Kotlin to Python/JS. In this TIL how to launch native programs and more is shown in the GitPod IDE.\n\n\n\n\n\n\nJan 21, 2021\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObject Detection (Workshop)\n\n\n\n\n\n\n\npresentation\n\n\nmachine-learning\n\n\n\n\nHow to do object detection using Transformers?\n\n\n\n\n\n\nDec 16, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTransfer Learning (Presentation)\n\n\n\n\n\n\n\npresentation\n\n\nmachine-learning\n\n\n\n\nWhat is Transfer Learning? Can I have the cake and eat it too?\n\n\n\n\n\n\nDec 12, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nHow to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)\n\n\n\n\n\n\n\ngradle\n\n\nkotlin\n\n\nworkshop\n\n\nmultiplatform\n\n\n\n\nA three part blog (all included in this one) that goes through 1) How Kotlin Multiplatform works, 2) How to build a game (Snake) and finally 3) how to make it multiplatform.\n\n\n\n\n\n\nNov 7, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: SDKMan - The Software Development Kit Manager\n\n\n\n\n\n\n\njvm\n\n\nTIL\n\n\n\n\nSDKMan is a tool to make JDK swapping, and installation, simple. It’s really good!\n\n\n\n\n\n\nSep 4, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: ‘The Badass Runtime Plugin’, jpackage & jlink - create a ‘native’ installable executable from your JVM-app that isn’t huge\n\n\n\n\n\n\n\njvm\n\n\nTIL\n\n\n\n\n‘The Badass Runtime Plugin’ is a plugin that allows you to package a stripped down JRE and modules from your program into a ‘native’ installable program which doesn’t require the user to have Java installed. It takes help of JPackage & JLink to achieve this, my own small program ended up at 35 MB including JRE, which is pretty crazy. This is like Electron, but better! ;)\n\n\n\n\n\n\nSep 3, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: fastutil - fast & compact type-speciic collections for JVM (no autobox!)\n\n\n\n\n\n\n\njvm\n\n\nTIL\n\n\n\n\nDiscusses what autoboxing is, why it might hit your performance (and memory). Finally some alternatives are also provided. Learn how to use effective data collections today!\n\n\n\n\n\n\nSep 3, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nCoViD-19 FAQ Search Engine 2.0\n\n\n\n\n\n\n\nnlp\n\n\ndeep-learning\n\n\nmachine-learning\n\n\nworkshop\n\n\n\n\nIn this post I improve the previous FAQ search engine by some low hanging fruits. The requirements stay the same thus SotA is not achieved but rather it’s simply generic & easy on hardware (Raspberry Pi capable).\n\n\n\n\n\n\nAug 1, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSQL - Different Abstraction Levels (& how I came to love SQLDelight)\n\n\n\n\n\n\n\njvm\n\n\nkotlin\n\n\ndb\n\n\nmultiplatform\n\n\n\n\nIn this post different abstraction levels of SQL is discussed with the final SQLDelight which turns the abstraction into the reverse.\n\n\n\n\n\n\nJun 1, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency\n\n\n\n\n\n\n\nnlp\n\n\nmachine-learning\n\n\n\n\nA search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings & Smooth Inverse Frequency to embed sentences. Basically scratched an itch I’ve had for a while\n\n\n\n\n\n\nMay 13, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I created a email generator in Kotlin (for Afry Tipsrundan)\n\n\n\n\n\n\n\nkotlin\n\n\nhtml\n\n\n\n\nThis blog goes through how to use Kotlin to generate good looking responsive emails. It’ll handle CSS, kotlin html DSL & kotlin serialization.\n\n\n\n\n\n\nMar 31, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGradle, JVM and GitHub Packages\n\n\n\n\n\n\n\ngradle\n\n\njvm\n\n\n\n\nA simple show-case on how to publish libraries to both GitHub Packages & JitPack with a one-button release using gradle & GitHub Actions.\n\n\n\n\n\n\nFeb 10, 2020\n\n\nHampus Londögård\n\n\n\n\n\n\n  \n\n\n\n\nAFRY NLP Competence Meeting: Text Classification IMDB\n\n\n\n\n\n\n\nmachine-learning\n\n\nnlp\n\n\nworkshop\n\n\n\n\nThis blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment).\n\n\n\n\n\n\nFeb 4, 2019\n\n\nHampus Londögård\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-01-21-til-gitpod-kotlin-jvm.html",
    "href": "posts/2021-01-21-til-gitpod-kotlin-jvm.html",
    "title": "TIL: GitPod - your editor in the cloud",
    "section": "",
    "text": "TIL: GitPod - your editor in the cloud\nGitPod.io is a editor in the cloud based on Theia which is a IDE built for the cloud & desktop. It implements the same language servers and allow you to use the same extensions as in VS Code - which is great. \n\nFor more about GitPod and how to combine it with languages such as Kotlin, Java & more, read 👇.\n\nWhat\nGitPod.io from the start worked for the normal “low-barrier” languages such as JavaScript & Python and still works great, and it really is where it excels. Lately though they’ve stepped up and added more & more targets with further customization.\nYou can now: - Customize your docker build by installing new packages (through brew or apt install). - Customize your extensions (majority, if not all, from VS Code available) - Add VNC (see native desktop programs) - Prebuilt containers (save a lot of time to not rebuild everything each time) - … & more\nWith a free account you get 50h / month of time to use this (as of 2021-01-22), which is very kind of GitPod!\n\n\nHow\nFirst of all you need either a GitHub, GitLab or Bitbucket account. Once that is done you need to find a repository that you want to work on, it can be both a private or public repository.\nOnce you’ve got your account & repository set up then head to gitpod.io/#{Repository: e.g. https://github.com/londogard/snake-js-jvm-native} and it will open up. This will ask you to create a PR with some new config adding a new file called .gitpod.yml and further you can add .gitpod.Dockerfile which configures your Docker setup.\nSetting up .gitpod.yml:\nimage:\n  file: .gitpod.Dockerfile # OBS: only use this if you have a custom image\n\n# List the ports you want to expose and what to do when they are served. See https://www.gitpod.io/docs/config-ports/\nports:\n  - port: 3000\n    onOpen: open-preview\n\n# List the start up tasks. You can start them in parallel in multiple terminals. See https://www.gitpod.io/docs/config-start-tasks/\ntasks:\n  - init: ./gradlew clean build\n\nvscode:\n  extensions:\n    - random.extension # (automatically added by adding extensions through interface). \nSetting up .gitpod.Docker:\nFROM gitpod/workspace-full-vnc  # For NoVNC to work\n\nUSER gitpod\n\nRUN sudo apt-get -q update && \\\n    sudo apt-get install -y libgtk-3-dev && \\ # For NoVNC to work\n    sudo apt-get install -yq gcc-multilib && \\\n    sudo apt install -y libtinfo5 && \\ # For Kotlin Native to work\n    sudo rm -rf /var/lib/apt/lists/*\nWith this setup you can push the changes to your repo and re-open the GitPod to reload a brand new Docker image with the updated packages.\nThis configuration allows you to do all I’ve mentioned such as running Native Kotlin, run with VNC to view native application windows and more!\nBy default you’re able to view a in-place/preview browser window so doing web development is working from the get-go and as such also Kotlin/JS worked pretty well without any configuration.\nIf you’re wondering, the NoVNC is on port 6080.\n\n\nWhy\nFor me there’s actually not just one reason to why I’d like to do this.\n\nBe able to do some quick coding / reviewing on my tablet.\nWorkshops at AFRY (no installation required for users)\n\nJust in a few days (2021-01-26) I’ll do a workshop at AFRY with Kotlin Multiplatform where GitPod.io will be a great alternative for colleagues who have computers where they’re not allowed to install their own applications.\n\n\nAlternatives\n\nGitHub Codespaces is coming, but is still in a private BETA.\nVS Code Remote (obs this actually require your own computer somewhere to host the vs code server)\n\nI think for now GitPod.io is the simplest and best out of them, unless you have your own computer to use as a server for VS Code Remote!\nThanks\nHampus"
  },
  {
    "objectID": "posts/2020-08-01-faq-search-covid-2/index.html",
    "href": "posts/2020-08-01-faq-search-covid-2/index.html",
    "title": "CoViD-19 FAQ Search Engine 2.0",
    "section": "",
    "text": "In this post I improve the previous FAQ search engine by some low hanging fruits. The requirements stay the same thus SotA is not achieved but rather it’s simply generic & easy on hardware (Raspberry Pi capable)."
  },
  {
    "objectID": "posts/2020-08-01-faq-search-covid-2/index.html#improvements-to-be-done",
    "href": "posts/2020-08-01-faq-search-covid-2/index.html#improvements-to-be-done",
    "title": "CoViD-19 FAQ Search Engine 2.0",
    "section": "Improvements to be done",
    "text": "Improvements to be done\nIn the previous blog & notebook I first implemented a basic FAQ search based on finding the nearest neighbour from the embedded sentences, in the end I used Smooth Inverse Frequency Embeddings (A Simple but Tough-to-Beat Baseline for Sentence Embeddings) to embed the sentence which is an improvement from simply averaging the embeddings of the words in the sentence.\nIn the end I discussed some potential improvements which I wished to investigate. In this notebook I’ll deliver these “improvements” based on grabbing some low hanging fruit. The total “improvements” to try out:\n\nLowercase\nBetter tokenization\nLemmatizing\nStop words\nNgram & Custom Embeddings (will not be done because of time)\n\nTo improve further I’d say that either A) a lot of time to understand the data in depth and apply heuristics or B) a supervised approach, which in turn require labeled data (a.k.a sweet valued time). A larger dataset would also be helpful.\nAll which I don’t have currently."
  },
  {
    "objectID": "posts/2020-08-01-faq-search-covid-2/index.html#re-adding-the-old-code",
    "href": "posts/2020-08-01-faq-search-covid-2/index.html#re-adding-the-old-code",
    "title": "CoViD-19 FAQ Search Engine 2.0",
    "section": "Re-adding the old code",
    "text": "Re-adding the old code\nFirst I’ll add the code from “part one” and it’ll not be commented as it has been walked through.\nFurther I’ve removed the download & parsing of FAQ, now the data is directly downloaded as a tsv-file allowing us to skip some libraries / code-cells.\nSome new dependencies are also added, e.g. stanza which is Stanfords new NLP-lib in Python (inspired by spaCy).\n%%capture\n!pip install -U gensim\n!pip install -U fse\n!pip install stanza\n!pip install stop-words\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\ntqdm.pandas()\n\nfrom pathlib import Path\n\nimport os\nimport random \nimport operator \nimport regex as re\n\n# gensim + fasttext\nfrom gensim.models.fasttext import FastText, load_facebook_vectors\nfrom gensim.models import KeyedVectors\n\nfrom stop_words import get_stop_words # stop-words from basically all languages\n\nimport stanza\n\nfrom fse import IndexedList\nfrom fse.models import uSIF\nfrom fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH\nprint(MAX_WORDS_IN_BATCH)\nprint(FAST_VERSION)\n10000\n1\n# Download models etc\nstanza.download('sv', logging_level='ERROR')\nprint(\"OBS!!\\nPlease download the Swe fastText model & the CoViD FAQ data from links in this code cell!\")\n# Swe fastText reduced dimensions -->   https://drive.google.com/open?id=1vaWtiSlRAZ3XCdtnSce_6dwQ0T5x0OEJ\n# CoViD FAQ data -->                    https://github.com/londogard/nlp-projects/blob/master/datasets/covid.tsv\nOBS!!\nPlease download the Swe fastText model & the CoViD FAQ data from links in this code cell!\n\nLoading all the models\nThis might take a little while, even though the dimensions are reduced the model is pretty large.\nft_wv = load_facebook_vectors('~/git/nlp-projects/models/cc.sv.100.bin')\ndf = pd.read_csv('~/git/nlp-projects/datasets/covid.tsv', sep='\\t')\nnlp = stanza.Pipeline(lang='sv', processors='tokenize', logging_level='ERROR')\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\n\nflatten = lambda l: [item for sublist in l for item in sublist] # Helper function to flatten a list"
  },
  {
    "objectID": "posts/2020-08-01-faq-search-covid-2/index.html#going-forward",
    "href": "posts/2020-08-01-faq-search-covid-2/index.html#going-forward",
    "title": "CoViD-19 FAQ Search Engine 2.0",
    "section": "Going forward",
    "text": "Going forward\nLet’s get on to adding our improvements\n\n1. Tokenization & lower-case\nThe first and forthmost improvement is to lowercase the text and then tokenize it using a better method of tokenization.\nLet’s take a look at how stanza helps us out by applying a much better tokenization.\nq = \"Hej där borta! Jag känner igen dig, Johan's kompis? Eller är det Johannas?\"\nstanza_tokenize = lambda x: [token.text for sentence in nlp(x).sentences for token in sentence.tokens]\nprev = q.split()\nnew = stanza_tokenize(q)\n\nprint(f\"Previously:\\t{prev[:12]}..\")\nprint(f\"After:\\t\\t{new[:12]}..\")\nPreviously: ['Hej', 'där', 'borta!', 'Jag', 'känner', 'igen', 'dig,', \"Johan's\", 'kompis?', 'Eller', 'är', 'det']..\nAfter:      ['Hej', 'där', 'borta', '!', 'Jag', 'känner', 'igen', 'dig', ',', 'Johan', \"'\", 's']..\nSo, what are we looking at?\nStanza handled our tokenization and increased the number of tokens, can this really be good!?\nYes! Keep calm and don’t jump the ship yet, the increased number of tokens will be followed by a decrease of unique tokens, and indirectly out of vocobulary (OOV) tokens. Unlike what we set out to do we still don’t lower-case the output, this will follow later, now let me explain what the tokenization helps us achieve:\n\nPunctuation, e.g. [!,?..], is tokenized into its own token.\nSome compound words are split up, e.g. Johan’s is now Johan, ’, s which is three (3) separate tokens rather than one.\n\nBecause of the updated tokenization fredag and fredag! is now tokenized as [fredag] and [fredag, !], this in fact turns fredag into the same token in both thus achieving the same vector when embedded which is great, because it really means the same. The exclamation mark itself also applies the same meaning to all places it’s applied, which in itself is an improvement now also as we embed it separately.\nWhy is this good?\nEven though we see a direct increase in number of tokens we see a decrease of number of unique tokens because we now tokenize borta, borta?, & borta! as the same token, with one additional for the punctuation in the two latter cases rather than 3 separate tokens which would map to different data.\nThe coverage of our Word Embeddings also increase because we now tokenize the text better. Perhaps borta! does not exist but borta surely do exist in the embedding dictionary / lookup.\n# A bit ugly, that's what happens when you're lazy\ndef test_dimensions(preprocessing=[stanza_tokenize]):\n    prev = flatten(df['question'].apply(lambda x: x.split()).tolist())\n    post = flatten(df['question'].apply(lambda x: preprocess(x, preprocessing)).tolist())\n\n    print(f\"Previously: {len(prev)} tokens ({len(set(prev))} unique)\")\n    print(f\"Post: {len(post)} tokens ({len(set(post))} unique)\")\n    print(f\"Token reduction by ~{100 * (1- len(set(post))/len(set(prev))):.1f} %\")\n    labels = ['#Tokens', '#Unique Tokens']\n    width = 0.35\n    x = np.arange(len(labels))\n    fig, ax = plt.subplots()\n    rects1 = ax.bar(x - width/2, [len(prev), len(set(prev))], width, label='Before')\n    rects2 = ax.bar(x + width/2, [len(post), len(set(post))], width, label='After')\n    ax.set_ylabel('Tokens')\n    ax.set_title('Tokens before and after')\n    ax.set_xticklabels(labels)\n    ax.set_xticks(x)\n    ax.legend()\n    fig.tight_layout()\n\n    plt.show()\n\n# preprocessing is a list of lambda functions to apply\ndef preprocess(text, preprocessing):\n    for f in preprocessing:\n        text = f(text)\n    return text\nLet’s take a look how much this actually mattered!\ntest_dimensions()\nPreviously: 629 tokens (289 unique)\nPost: 713 tokens (273 unique)\nToken reduction by ~5.5 %\n\n\n\nsvg\n\n\nThe expectations set up has been achieved and we can clearly see that the raw number of tokens grew while the unique token count shrinked.\nApplying lower-case to the text will further reduce the number of unique tokens, and obviously keep the number of tokens at the same count.\nLet’s add lower-casing and see what happens!\nlowercase = lambda x: x.lower()\npreprocess_funcs = [lowercase, stanza_tokenize]\ntest_dimensions(preprocessing=preprocess_funcs)\nPreviously: 629 tokens (289 unique)\nPost: 712 tokens (260 unique)\nToken reduction by ~10.0 %\n\n\n\nsvg\n\n\n\nLower-casing\nGoing from 5.5 to 10 % reduction is nothing to sneeze at, by applying these two simple techniques we now have the same data in a better format which allows us to have a lower number of unique tokens.\nPretty awesome right?\nLet’s get on with this and apply the preprocessing to the questions and test it out with the FAQ-search!\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocess_funcs))\ndf['X'].head()\n0                            [vad, är, coronavirus, ?]\n1                               [vad, är, covid-19, ?]\n2    [vad, skiljer, covid-19, från, säsongsinfluens...\n3               [vilka, är, symtomen, på, covid-19, ?]\n4    [hur, vet, jag, om, mina, symtom, beror, på, p...\nName: X, dtype: object\n\n\n\nTesting the new input-data\nNow that we’ve created our input data we need to test our model on this!\nBy applying the IndexedList which is the dataformat SFE wants as input we can train the model and then test it.\nsfe_format = IndexedList(df['X'].tolist())\nmodel.train(sfe_format)\n(75, 712)\n# Helper method to test the closest questions\ndef get_n_closest_questions(question, preprocessing, n=4):\n    q_fixed = preprocess(question, preprocessing)\n    resp = model.sv.similar_by_sentence(q_fixed, model=model, indexable=df['question'].tolist()) # [([tokens], score)]\n    resp = [f'{result[2]:.2f}: {result[0]}' for result in resp]\n    print('\\n'.join(resp[:n]))\nget_n_closest_questions(\"kan min hamster bli smittad?\", preprocess_funcs)\n0.67: Kan man bli smittad av en person som har covid-19 men som inte har några symtom?\n0.63: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset?\n0.54: Kan viruset smitta till människa via post och paket?\n0.42: Kan smitta överföras från mygg till människa?\nget_n_closest_questions(\"Hur får jag min son att förstå?\", preprocess_funcs)\n0.82: Hur pratar man med barn om det nya coronaviruset?\n0.80: Vad är covid-19?\n0.78: Hur sjuk blir man av covid-19?\n0.77: Hur länge är man sjuk av covid-19?\n\n\n2. Lemmatization and Stop Words\nLet’s try to further improve this by actually lemmatizing and applying stop-words!\n\nLemmatization\nSo what is Lemmatization? Quoting Stanfords description:\n\nFor grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n\n\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\n\n     am, are, is => be\n     car, cars, car's, cars' => car \n\nThe result of this mapping of text will be something like:\n\n     the boy's cars are different colors =>\n     the boy car be differ color \n\n\nWhat is stop-words?\nStop-words are words we want to throw away as they add no real purpose. In older Machine Learning approaches it was way more important to add stop-words but in newer Deep Learning with Neural Networks stop-words often can be a negative thing, removing understanding of the sentence and perhaps minor differences which makes the world for understanding.\nA example of a stop-word list could be [\"hej\", \"vem\", \"då\", \"och\", ...] which means that these words would be removed from a sentence.\nIn our case it makes sense to remove words like ‘vad’, ‘varför’ and so on because the return of the FAQ seems to be very weighted towards these words.\nnlp = stanza.Pipeline(lang='sv', processors='tokenize,mwt,pos,lemma', logging_level='ERROR')\nstanza_lemma = lambda x: [token.lemma for sentence in nlp(x).sentences for token in sentence.words]\npreprocess_funcs_lemma = [lowercase, stanza_lemma]\n\nprint(f'Previously:\\t{preprocess(\"hur förklarar jag för min dotter och son?\", preprocess_funcs)}')\nprint(f'After:\\t\\t{preprocess(\"hur förklarar jag för min dotter och son?\", preprocess_funcs_lemma)}')\nPreviously: ['hur', 'förklarar', 'jag', 'för', 'min', 'dotter', 'och', 'son', '?']\nAfter:      ['hur', 'förklara', 'jag', 'för', 'jag', 'dotter', 'och', 'son', '?']\nSome interesting notes\nSeeing ‘min’ getting converted to ‘jag’ is both good and bad, in this case we reduce dimensionality of the problem but we loose context and understanding. jag and min certainly does not mean the same thing.\nLet’s see how it pans out…\ntest_dimensions(preprocess_funcs_lemma)\nPreviously: 629 tokens (289 unique)\nPost: 712 tokens (228 unique)\nToken reduction by ~21.1 %\n\n\n\nsvg\n\n\ndel model\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocess_funcs_lemma))\nsfe_format = IndexedList(df['X'].tolist())\n\nmodel.train(sfe_format)\n(75, 712)\nget_n_closest_questions(\"kan min hamster bli smittad?\", preprocess_funcs_lemma)\n0.75: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset?\n0.69: Hur smittar covid-19?\n0.68: Kan man smittas flera gånger av det nya coronaviruset?\n0.63: Smittar covid-19 via vatten och mat?\nget_n_closest_questions(\"Hur får jag min son att förstå?\", preprocess_funcs_lemma)\n0.79: Vad är covid-19?\n0.75: Hur sjuk blir man av covid-19?\n0.74: Hur länge är man sjuk av covid-19?\n0.66: Om en person i familjen är sjuk - måste alla stanna hemma då?\n\n\nAnalyzing the results\nImprovements?\nNot really, the model has an improved response to the ‘hamster-question’ but it’s way off when asking about the son.\nWhy?\nThe most likely explanation is that even though we reduce the input dimensions an awful lot we remove dimensions that brings value, and removing value is bad - just as was touched upon previously. It might be helpful in some cases, perhaps this could prove helpful for a supervised approach such as TF-IDF + Support Vector Machine.\nAny good parts?\nYes, we can see some pretty hefty memory-requirement reductions when working with other types of models by applying this. Actually, in the case of this we could reduce the memory requirement by lemmatizing the dictionary of the embeddings and removing all non-lemmas. All in all, this could lead to a small performance loss but great memory win.\n\n\nStop words\nAs promised we shall apply stop-words, but as we saw no performance gain with lemmatization we’ll keep the old tokenization.\nstop_words = get_stop_words('sv')\nclean_stop = lambda x: [word for word in x if word not in stop_words]\npreprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop]\n\ndel model\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don't need to remove stop-words in the sentences in our \nsfe_format = IndexedList(df['X'].tolist())\n\nmodel.train(sfe_format)\n\npreprocess(\"hur förklarar jag för min dotter och son?\", preprocessing_func_stop)\n['förklarar', 'dotter', 'son', '?']\ntest_dimensions(preprocessing_func_stop)\nPreviously: 629 tokens (289 unique)\nPost: 417 tokens (206 unique)\nToken reduction by ~28.7 %\n\n\n\nsvg\n\n\nget_n_closest_questions(\"kan min hamster bli smittad?\", preprocessing_func_stop)\n0.66: Kan man bli smittad av en person som har covid-19 men som inte har några symtom?\n0.64: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset?\n0.54: Kan viruset smitta till människa via post och paket?\n0.41: Kan smitta överföras från mygg till människa?\nget_n_closest_questions(\"Hur får jag min son att förstå?\", preprocessing_func_stop)\n0.83: Vad är covid-19?\n0.83: Hur pratar man med barn om det nya coronaviruset?\n0.80: Hur sjuk blir man av covid-19?\n0.80: Hur länge är man sjuk av covid-19?\n\n\nFurther analyzing\nIn my mind we’ve some pretty good responses, in a way better and another way worse than lemmatizaton. Certainly not a set-back but neither a step forward.\nTesting different approaches and turning things on and off is a great way to increase data understanding and also gives a better sense of what different preprocessing functions actually does.\nIn fact this is actually part of the most common Machine Learning development approach, working much like agile, which is iteratively circular and called CRISP-DM. I won’t go deeply into CRISP-DM (already did once in my Master Thesis), but the following image gives you the gist.\n\nFinally, as we see no great impact by applying either lemmatization nor stop-words we might just give up at the lower-case + stanza tokenization, but I’d like to make one last shot in the dark - custom stop words! Let’s see how it fares…\n\nCustom Stop Words (breaking the rules)\nSo I decided to break the rules and create a small simple heuristic by applying custom stop words.\nLet’s figure out which words we should remove using the following steps (which could in fact be automated)!\n\nFind the most common words\nRemove the ones which does not give any greater value\n\nfrom collections import Counter\n\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocess_funcs))\n\ncounter = Counter(flatten(df['X'].tolist()))\nsorted(counter.items(), key=lambda item: item[1], reverse=True)[:15]\n[('?', 75),\n ('covid-19', 28),\n ('vad', 25),\n ('och', 22),\n ('hur', 21),\n ('för', 20),\n ('det', 15),\n ('kan', 14),\n ('i', 14),\n ('jag', 13),\n ('av', 13),\n ('gäller', 12),\n ('som', 12),\n ('är', 11),\n ('en', 11)]\nstop_words = ['?', 'och', 'jag', 'i', 'är', 'en', 'min', '?']\nclean_stop = lambda x: [word for word in x if word not in stop_words]\npreprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop]\n\ndel model\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don't need to remove stop-words in the sentences in our \nsfe_format = IndexedList(df['X'].tolist())\n\nmodel.train(sfe_format)\n\npreprocess(\"hur förklarar jag för min dotter och son?\", preprocessing_func_stop)\n['hur', 'förklarar', 'för', 'dotter', 'son']\nget_n_closest_questions(\"Hur får jag min son att förstå?\", preprocessing=preprocess_funcs)\n0.83: Hur pratar man med barn om det nya coronaviruset?\n0.83: Vad är covid-19?\n0.80: Hur sjuk blir man av covid-19?\n0.79: Hur länge är man sjuk av covid-19?\nget_n_closest_questions(\"kan min hamster bli smittad?\", preprocessing=preprocess_funcs)\n0.66: Kan man bli smittad av en person som har covid-19 men som inte har några symtom?\n0.63: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset?\n0.54: Kan viruset smitta till människa via post och paket?\n0.41: Kan smitta överföras från mygg till människa?\nNot bad, not amazing - I feel pretty happy about this.\nSo what can be done from now on if time and resources where available?\n\nAdd a classifier + TF-IDF\nBERT / ALBERT QA (the State-of-the-Art right now)\n\nThanks for this time,\n- Hampus Londögård"
  },
  {
    "objectID": "posts/2020-09-04-til-sdkman.html",
    "href": "posts/2020-09-04-til-sdkman.html",
    "title": "TIL: SDKMan - The Software Development Kit Manager",
    "section": "",
    "text": "SDKMan - Swapping JDK made simple\nI’ve decided to not only write blogs but also small snippets, here comes the first one. \n\nWhat\nSDKMan is a tool to install, set and swap JDK.\nSDKMan actually supports more than the Java JDK, among supported tooling is Java, Groovy, Scala, Kotlin and Ceylon. Ant, Gradle, Grails, Maven, SBT, Spark, Spring Boot, Vert.x and many others also supported.\nIt’s written in Bash, only requires curl & zip/unzip.\nSo what SDKMan simplifies is 1. Installation of different JDKs, Gradle versions and so on 2. Swapping between JDKs 3. Allowing local (by folder basis) JDK-versions\n\n\nHow\nWe start by installation #### Installation If you need a more detailed guide go to this page.\nDownloading SDKMan\n$ curl -s \\\"https://get.sdkman.io\\\" | bash\nInstalling $ source \\\"$HOME/.sdkman/bin/sdkman-init.sh\\\"\nVerification $sdk version - should return something along sdkman X.Y.Z\n\nUsage\n\n\n\n\n\n\n\n\nWhat\nCommand\nComment\n\n\n\n\nInstall JDK\n$sdk install java\nInstalls the latest stable version of Java JDK\n\n\nInstall specific version\n$sdk install scala 2.12.1\nInstall scala 2.12.1\n\n\nInstall local version\n$sdk install groovy 3.0.0-SNAPSHOT /path/to/groovy-3.0.0-SNAPSHOT\nInstalls a JDK you have locally to the SDKMan. The version name must be unique!\n\n\nRemove version\n$sdk uninstall scala 2.11.6\n\n\n\nList candidates\n$sdk list java\nLists all java candidates that are installable through SDKMan\n\n\nUse version\n$sdk use scala 2.12.1\nUse the version said, this only changes the current shell\n\n\nDefault version\n$sdk default scala 2.11.1\nChanges version for all subsequent shells\n\n\nCurrent version\n$sdk current\nLists all currently selected versions\n\n\n\nRemember to point your JDK to the ./sdkman/candidates/java/current path. Do the same for your IDE, such as IntelliJ-IDEA.\n\n\n\nWhy\nI’ve got different projects where I need to use different java versions. In one project I need JDK 14 to include jpackage and another one I’m forced to use JDK 8 (legacy system), to swap between these has never been simpler!\n\n\nAlternatives\njEnv is a great alternative. According to some more JDK versions exists (haven’t checked myself), but overall it seems that SDKMan is the preferred alternative.\nLooking at GitHub one can clearly see that SDKMan is more popular, both by stars, latest commit and forks - which should be a decent enough to make a choice.\nOne thing I’ve learned both through work and my personal projects is that often it’s better to make an non-optimal decision rather than trying to find the perfect solution, because diving into the pile of research to perfection will take much more time than just getting started.\n-Hampus Londögård"
  },
  {
    "objectID": "posts/2020-12-16-object-detection-transformer.html",
    "href": "posts/2020-12-16-object-detection-transformer.html",
    "title": "Object Detection (Workshop)",
    "section": "",
    "text": "I did a presentation/workshop on Object Detection using Transformers. It’s a little bit confusing as the experience of the people that joined was very different, some even not knowing Python. \nThought it could make sense to share anyways. :)"
  },
  {
    "objectID": "posts/2023-02-06-baby-monitor-pt-p2/index.html",
    "href": "posts/2023-02-06-baby-monitor-pt-p2/index.html",
    "title": "Baby Monitor pt 2",
    "section": "",
    "text": "Back in action and finalizing the baby monitor!👶\nTL;DR Built a baby monitor that included the following features:\nThe project was born the day I met an old friend and saw his expensive baby monitor that he had been gifted, I needed to match it! 🤓\nResult:  I’m very happy about the results, my wife asked me to draw a smile on the creepy monitor, hence the smile! 😜 Video of it running live can be found at the end!"
  },
  {
    "objectID": "posts/2023-02-06-baby-monitor-pt-p2/index.html#implementation-details",
    "href": "posts/2023-02-06-baby-monitor-pt-p2/index.html#implementation-details",
    "title": "Baby Monitor pt 2",
    "section": "Implementation Details",
    "text": "Implementation Details\nTo implement and build this camera I had to combine both hardware and software into a package.\n\nHardware Details\nMost of my hardware was bought through Aliexpress, with few parts being from an old Pi.\n\n\n\nHardware\nFunctionality\nSoftware Required/Used\nNotes\n\n\n\n\nRaspberry Pi 3B+\nThe Brain which powers everything\nRaspberry Pi OS Lite (Bullseye)\nThis OS uses the new Open Source camera-stack, Libcamera!\n\n\nDS18B20\nTemperature Sensor\nW1ThermSensor\nI wish I found this earlier, at first I parsed the raw file myself. And it was hard to find set-up instructions!\n\n\nNylon FPV Servo\nServo Motor (moving the camera)\ngpiozero\nA brilliant library. It has to be noted that this servo works through Pulse Width Modulation (PWM) and to make the servos quite we need to set servo.value=None after setting it to a value. Complicates the configuration a little.\n\n\nRaspberry Pi 4 Camera 5MP\nCamera with IR-cut (IR on/off via hardware automatically)\nlibcamera / picamera2\nVery simple to use over all. Tricky that you needed to focus it yourself, I thought it was broken first! 😆\n\n\nMicrophone from Google AIY v1\nRecord sound\nThis is tricky because of the HAT, requires custom installation.\n\n\n\nSpeaker from Google AIY v1\nPlay sound\n\n\n\n\nPi HAT from Google AIY v1\nCombine sensors, microphone & speakers\n\n\n\n\n\n\n\nSoftware Stack\nTo make use of my beautiful hardware I need software! Keeping things simple (KISS) I decided to use a Python backend and show it through a simple webapp. That way I can view the baby monitor from my PC, Smartphone & anything that has a browser really.\nThe end result became as follows 👇 \n\nWebapp Client\nOver all I really enjoyed playing around with Svelte. It felt very straight-forward and simple, although there’s less community and libraries compared to React. All in all I’d give it one up compared to React because of simplicity, but I’m just a ordinary Backend Dev / Data Engineer+Scientist.\n\n\nServer/Backend\nFastAPI as always is a blessing to work with! The auto-generated swagger page, superb type integration and much more makes me feel right at home as someone who’s really a Scala-dev.😉 FastAPI has its drawbacks though, the streaming component definitely showed some rather large overhead. I had to fall back to raw http to have good performance 😰\nThe end result became two backends, but I tried to keep the responsibilities clear and it worked out fine!"
  },
  {
    "objectID": "posts/2023-02-06-baby-monitor-pt-p2/index.html#end-result",
    "href": "posts/2023-02-06-baby-monitor-pt-p2/index.html#end-result",
    "title": "Baby Monitor pt 2",
    "section": "End Result",
    "text": "End Result\n And a video to show how real-time it is!\n\n\nVideo\nTest Video\n\n\nI’m very happy about the results!\n\nImages of the Building Process\nAnd some images of when I built the monitor!\n\n\n\n\n\n\n\nWhat\nImage\n\n\n\n\nBuilding the Camera\n\n\n\nConnecting the final piece of Camera\n\n\n\nBuilding Temperature Sensor\n\n\n\nConnecting Temperature, Pi & Camera\n\n\n\nManual Temperature Validation\n\n\n\nTesting the Servo\nTesting the Servo\n\n\nConnecting all in a paper box\n\n\n\nFirst Wooden Baby Monitor Prototype\n\n\n\nFinal Wooden Baby Monitor"
  },
  {
    "objectID": "posts/2023-02-06-baby-monitor-pt-p2/index.html#a-sad-ending",
    "href": "posts/2023-02-06-baby-monitor-pt-p2/index.html#a-sad-ending",
    "title": "Baby Monitor pt 2",
    "section": "A sad ending",
    "text": "A sad ending\nThe servo motors showed to be too weak which interestingly means they’re too strong. As they try to move the housing it works slowly until it move everything at once which creates a force stronger than the pad that the monitor was standing on.\nThe end result was… Sweet release of machine breakage 😢\n \nThat’s it for this time! Now I look forward to become a father! 👨‍👩‍👧‍👦 ~Hampus Londögård"
  },
  {
    "objectID": "posts/2021-04-18-knowledge-distillation-presentation.html",
    "href": "posts/2021-04-18-knowledge-distillation-presentation.html",
    "title": "Knowledge Distillation (Presentation)",
    "section": "",
    "text": "Just a week ago I did a presentation on Knowledge Distillation and how it can help improve efficiency of models. \nI simply thought I would share the slides with the world. :)\n(OBS: Presentation in Swedish)"
  },
  {
    "objectID": "posts/2022-01-16-nlp-toolkit-release.html",
    "href": "posts/2022-01-16-nlp-toolkit-release.html",
    "title": "Release nlp (londogard-nlp-toolkit) 1.1.0",
    "section": "",
    "text": "The 1.1.0 release of nlp (londogard-nlp-toolkit) by londogard is finally here!\n\nI’m writing this small blog-post mainly to showcase some of the new things possible now that we’re moving into classifer-space!\nThis release took some time to complete because there was some big restructuring and custom implementations required. One thing that I wasn’t expecting was to implement my own Sparse Matrix on top of multik because there’s currently no support. Without sparsity text features will make your memory dissapear before you take your second breath! 😅\nLuckily I managed to get something up and running. The code is now cleaner and more efficient than previously on top of all the new features.\nN.B.\nMost of the examples are taken from /src/test."
  },
  {
    "objectID": "posts/2022-01-16-nlp-toolkit-release.html#classifiers",
    "href": "posts/2022-01-16-nlp-toolkit-release.html#classifiers",
    "title": "Release nlp (londogard-nlp-toolkit) 1.1.0",
    "section": "Classifiers",
    "text": "Classifiers\nAnd the first feature built on top of the new vectors… classifiers!\nTo be able to figure out if a tweet is negative or positive we need to classify the text, based on the vectorized data.\nThe following classifiers are added for now:\n\nLogistic Regression using Stochastic Gradient Descent as optimizer\nNaïve Bayes classifier\nHidden Markov Model to classify sequences with a sequence output, e.g. Part of Speech (PoS) or Named Entitiy Recognition (NER).\n\n\nUsage of Classifiers\nval tfidf = TfIdfVectorizer<Float>()  \nval naiveBayes = NaiveBayes() // replace by LogisticRegression if needed  \n  \nval out = tfidf.fitTransform(simpleTexts)  \nnaiveBayes.fit(out, y)  \n  \nnaiveBayes.predict(out) shouldBeEqualTo y\nand for sequences:\nval (tokensText, tagsText) = text  \n    .split('\\\\n')  \n    .map {  \n        val (a, b) = it.split('\\\\t')  \n        a to b  \n    }.unzip()  \nval tokenMap = (tokensText).toSet().withIndex().associate { elem -> elem.value to elem.index }  \nval tagMap = (tagsText + \"BOS\").toSet().withIndex().associate { elem -> elem.value to elem.index }  \nval reversetagMap = tagMap.asIterable().associate { (key, value) -> value to key }  \nval hmm = HiddenMarkovModel(  \n    tagMap.asIterable().associate { (key, value) -> value to key },  \n    tokenMap.asIterable().associate { (key, value) -> value to key },  \n    BegginingOfSentence = tokenMap.getOrDefault(\"BOS\", 0)  \n    )  \n  \nval x = listOf(mk.ndarray(tokensText.mapNotNull(tokenMap::get).toIntArray()))  \nval y = listOf(mk.ndarray(tagsText.mapNotNull(tagMap::get).toIntArray()))  \n  \n  \nhmm.fit(x, y)  \n// predict.map { t -> t.data.map { reversetagMap\\[it\\] } } to get the real labels!  \nhmm.predict(x) shouldBeEqualTo y"
  },
  {
    "objectID": "posts/2022-01-16-nlp-toolkit-release.html#unsupervised-keyword-extraction",
    "href": "posts/2022-01-16-nlp-toolkit-release.html#unsupervised-keyword-extraction",
    "title": "Release nlp (londogard-nlp-toolkit) 1.1.0",
    "section": "Unsupervised Keyword Extraction",
    "text": "Unsupervised Keyword Extraction\nI couldn’t keep my release small enough… so I added a little gem, automatic keyword extraction! This tool is very fast and efficient at doing what it’s doing and is based on a Co-Occurrence Statistical Information algorithm proposed by Y. Matsuo & M. Ishizuka in the following paper.\nI think this is incredibly useful when you need something fast, cheap and that takes you 90% of the way!\n\nUsage of Keyword Extraction\nval keywords = CooccurrenceKeywords.keywords(\"Londogard NLP toolkit is works on multiple languages.\\\\nAn amazing piece of NLP tech.\\\\nThis is how to fetch keywords! \")  \n  \nkeywords shouldBeEqualTo listOf(listOf(\"nlp\") to 2)"
  },
  {
    "objectID": "posts/2022-01-16-nlp-toolkit-release.html#embedding-improvements",
    "href": "posts/2022-01-16-nlp-toolkit-release.html#embedding-improvements",
    "title": "Release nlp (londogard-nlp-toolkit) 1.1.0",
    "section": "Embedding Improvements",
    "text": "Embedding Improvements\nLightWordEmbeddings  have had their cache updated into a optimal cache by caffeine , which instead of being randomly deleted from cache takes the least used and remove. This will improve performance greatly!\n\nThat’s it, I’m hoping to release a spaCy-like API during 2022, including Neural Networks. Here’s to the future! 🍾"
  },
  {
    "objectID": "posts/2020-09-03-til-badass-runtime.html",
    "href": "posts/2020-09-03-til-badass-runtime.html",
    "title": "TIL: ‘The Badass Runtime Plugin’, jpackage & jlink - create a ‘native’ installable executable from your JVM-app that isn’t huge",
    "section": "",
    "text": "JPackage, JLink and how to pack a modern Java App\nJPackage is a way to package a modern JVM-program as a installable binary, in a small format. \n\nWhat\nJPackage was finally included in the JDK by JDK-14, originally from the JavaFX-world (to bundle your desktop apps). JPackage combines itself with JLink, which builds upon ‘project jigsaw’, and together they form a way to create \"native\" binaries for JVM-projects.\n\nWhat is JLink?\nJLink is a way to assemble and optimize a set of modules and their dependencies into a custom runtime image (JRE). In other words we can take a ordinary JRE, ~200 MB, and chop it down to a total size of 25-40 MB for smaller project.\nJLink is only possible thanks to ‘project jigsaw’ which introduced modules and modularized the whole JRE starting from JDK-9. The Java standard library (stdlib) was modularized into 75 modules. As you might guess it is even better if your own code is also modularized, but not enforced.\n\n\nWhat is JPackage\nJPackage is the packaging suite that allows you to package your code, dependencies and the JLink-created JRE. I ended up with installation files, with a natively executable file on 60 MB for one of my smaller projects, which is really good in comparison to Electron! In comparison to a C-program this might not be amazing, but you’ve to remember that this is completely cross-platform!\nSide-note all sized discussed is without any major optimizations - and there exists a lot! Finally, if you exclude the JRE you can reach sizes of KB rather than MB! But excluding the JRE enforces the user to have it locally, which might not be good UX.\n\n\n\nHow\nJPackage & JLink is made easy thanks to The Badass Runtime Plugin or The Badass JLink Plugin where the latter require a modular project and the former works with any project! :happy:\n\nInstallation\nMake sure you use & target JDK 14 or higher, JPackage was first included in this version. I recommend SDKMan to install & swap JDKs.\nThen to add the Badass Runtime Plugin I recommend using gradle, which makes it as simple as the following.\nplugins {\n    ...\n    id(\\\"org.beryx.runtime\\\") version \\\"1.11.3\\\" // latest version August 2020\n    ...\n}\nruntime {\n    options.set(listOf(\\\"--strip-debug\\\", \\\"--compress\\\", \\\"2\\\", \\\"--no-header-files\\\", \\\"--no-man-pages\\\"))\n    jpackage {\n        installerType = \\\"deb\\\" // https://badass-runtime-plugin.beryx.org/releases/latest/\n    }\n}\nThis addition now creates the tasks required to build & bundle your app. The options added make sure that you reduce the total size by a lot. I highly recommend reading the documentation, there’s so many incredibly useful options - I only provide the minimum!\n\n\nUsage\nBy editing our building.gradle.kts to include everything from the Installation we can run the ./gradlew jpackage task to build our installer!\nI want to note again, please make sure to read the homepage - a ton of optimizations and customization exist. There exists a lot of low hanging fruit for sure, so make sure to grab it! :wink:\n\n\n\nWhy\nIt’s really cool to see your JVM application installable using a .msi, .deb or even a .dmg while retaining a decent enough size. By using JPackage rather than GraalVM you make sure that you don’t loose anything in the form of performance or functionality. As a cherry on the top, it’s not just a executable file, but also includes a installer which is much better UX in my opinion. GraalVM will be discussed a bit more in Alternatives.\nI want to re-iterate about the UX and size, which are the two main points of this.\n\nWe bundle a JRE with the JVM-app, allowing executables without requiring Java, of your version, to be installed on the user computer already.\nThe JRE is minified to only contain required modules, about 30-40 MB on a smaller project.\nAll required dependencies are bundled also\nInstaller which makes the whole JVM program really like any program on the computer\nBasically a download, install run program that isn’t huge in size!\n\n\n\nAlternatives\nI see two alternatives that are worth mentioning\n\nFAT-JAR / Uber-JAR / Shadow-JAR\nGraalVM Native Image\n\n\n\"Fat-JAR\"\nA FAT-jar is a jar that bundles all dependencies and also includes a shell script, or .bat if Windows, to run the whole JVM-application. It’s pretty small in size, even though called FAT, as it doesn’t include a JRE to run the JVM.\nThis means that if your JVM-app requires Java 11 but the user only has Java 8 you need to have them download the JRE required, which sucks.\n#### GraalVM The probably best alternative, it’s even smaller in size as SubstrateVM (their runtime) is really small and GraalVM allows AOT compile.\nGraalVM has much faster startup-times than a JPackage program, but GraalVM is not as good when running for a long duration as there isn’t the incredibly good JIT from JVM.\nI’d say something along the following - for long running apps choose JPackage, for lambda etc certainly choose GraalVM.\nBut GraalVM has further negatives, you can’t just code as you usually do. Reflection etc is not supported as usual, meaning there comes a lot of caveats using GraalVM.\nExtra: I managed to end up with, after some minor trial-and-error, a binary file on ~ 12 MB for my file-sending program - pretty darn amazing!\nI’ll write more about GraalVM and its SubstrateVM which is used to create the native binaries in a new TIL.\n-Hampus Londögård"
  },
  {
    "objectID": "posts/2022-01-28-kotlinjs-onnx-deep-learning-in-browser/index.html",
    "href": "posts/2022-01-28-kotlinjs-onnx-deep-learning-in-browser/index.html",
    "title": "KotlinJS, ONNX and Deep Learning in the browser",
    "section": "",
    "text": "One day I had the crazy idea to try two non-mainstream things out at the same time. On top of that I figured I’d combine them in the same project, imagine that! \nPreview of final result running model inference in the browser using KotlinJS, ONNX & fritz2: \n\n\nKotlinJS resembles TypeScript (TS) in the sense that it’s typed and transpiles into JavaScript (JS) at the end of the day. The final JS code runs directly in the browser or through Node.js.\nWhat makes KotlinJS stand out? In my optinion it picks up where TS leaves. By adding (almost) all of the Kotlin ecosystem we get a really superb toolbox out-of-the-box, which is more than simply types. Some of the awesome perks are coroutines and collections.\nAs someone who has done a lot of backend development in Scala, with some Java, it feels like home because of the familiar apperance and interaction.\nHaving sweet syntax, superb typing I feel a great preference toward KotlinJS even if TS is closer to JS making transpiled code easier to reason about.\n\n\n\n\n\n\n\n\n\nOpen Neural Network Exchange (ONNX)\n\n\n\n\nOpen Neural Network Exchange (ONNX) Runtime is a open format created by Facebook, Microsoft & others, and is part of Linux Foundation AI.\n\n\n\nONNX is an open polyglot format, meaning that you can run Neural Networks from multiple coding languages. This in turn promotes innovation and collaboration, especially through the fact that you can run your State-of-the-Art model almost everywhere, including C# and Java.\nONNX is a impressive feat that allows companies to reduce their inference time by magnitudes, cherry on top it reduces code complexity when models are deployed directly in the original backend.\nRecently ONNX added a new runtime, ONNX-webruntime, which enables ONNX models to run directly inside the browser. Simply take your PyTorch/Tensorflow model, convert to ONNX and then run! Incredible! 🎉.\nONNX-webruntime leverages WebGL as GPU and WASM with SIMD as CPU.\nSimple edge deployment is here!\n\n\n\nThe set up is simple, - Kotlin JS project - fritz2 as web framework - onnx-webruntime as deep learning inference tool\nFor this demo I could’ve used raw html elements in the Kotlin JS code, but it’s more fun to use something enjoyable, as such I chose fritz2 that I introduce below 👇.\n\n\nIntroducing fritz2, a small but impressive framework.\nBecause of the size you can understand the full idea and implementation, which is something you cannot say about React. Through simple DSLs, superb usage of Flow<T> you end up with a simple yet powerful model that maps perfectly to my own mind.\nIn my opinion fritz2 feels less magic while very powerful and simple. Everything works with full typing and no hacks. Cherry on the top? No virtual dom!\nFritz2 has a extra components library which you can additionally install. This library contains simple components to make your development much faster, with things like File input, Data Tables and much more!\nPersonally I even did my own wedding website using fritz2, and it ended up pretty great!  > My personal wedding site created in fritz2\n\n\n\nUsing dukat (included by default in Kotlin > 1.6 or perhaps earlier) it’s possible to generate external types/bindings for any TypeScript project.\nGuess what, ONNXRuntime Web is full TypeScript - awesome!\nUnfortunately ONNX has some really weird structure which I’d call non-standard, this ends up not working great in dukat-generation…\nLuckily enough it is easy to make your own bindings. Keep your breath for now, I’ll share them later in this post, but for now let’s say that it’s like a .d.ts-file.\n\n\n\n\nWe need to create our project, I usually do it by scratch but if you want to keep it easy setting up the MPP project for friz2 you can make sure to use their template project. Make sure to include the fritz2 component library, as it’ll be used in the implementation.\nPlease note that the focus will be ONNX, as such I’ll save some fritz2 details for another post.\n\n\nGetting the skeleton UI up\nIn the “main” file of the js-folder, but not as in js-code 😉, you’ll need to set up a file and image element.\nfun main() {\n    val imgSrc = RootStore(\"\")\n    render {\n        val srcImg = img(id = \"img-from\") {\n            src(imgSrc.data)\n        }\n    }\n    \n    file {\n        accept(\"image/*\")\n        button { text(\"Single select\") }\n    }\n    .map { file -> \"data:${file.type};base64,${file.content}\" } handledBy imgSrc.update\n}\nBreaking down what’s done\n\nA RootStore is a abstraction on top of a (Mutable)StateFlow which is a Flow with a state.\n\nIn simple terms a Flow is a collection of asynchronously computed values just like you have Sequence and List being collections of synchronously computed values.\n\nA Store is a reactive component that contains our apps state, it can do bidirectional communication with the DOM/GUI.\n\nWe update imgSrc through the file-component, whenever file is updated.\n\n<img> listens on changes from imgSrc, hence it’s updated as imgSrc is updated\nAll in all we get typed and no-magic dynamical updates in our GUI. This is something I love, compared to react and svelte where it seems more magical.\n\n\nThe connector between file and imgSrc is dirty, I hoped to be able to load the b64 content directly into a UInt8ClampedArray to have optimal performance, but because the b64-string actually contains PNG/JPEG headers and other things the perfomance gains versus simplicity is not worth it. Hence I transform it from the b64-string (data:image/pdf;base64,<content>) to image and then extract ImageData - annoying but clean.\nThe detail that <content> in b64-string is only the pixel data haunted me for a long time… I couldn’t figure why my arrays had the wrong dimensions! 😅\nThe next step: transfer image from this component to another, while allowing a transformation (neural network inference) in-between.\n// highlight-start\nfun loadImgToCanvas(img: Image, canvas: Canvas, context: CanvasRenderingContext2D) {\n    if (img.domNode.src.isNotEmpty()) {\n        canvas.width = img.domNode.naturalWidth\n        canvas.height = img.domNode.naturalHeight\n        context.drawImage(img, 0.0, 0.0)\n    }\n}\n// highlight-end\n\nfun main() {  \n    val imgSrc = RootStore(\"\")  \n  \n    render {\n        val srcImg = img(id = \"img-from\") {\n            src(imgSrc.data)\n        }\n        val targetCanvas = canvas(id = \"img-to\") { }\n        val imgContext = targetCanvas.domNode.getContext(\"2d\") as CanvasRenderingContext2D\n    \n        file { /** same as before ... */ \n        \n        // highlight-next-line\n        srcImg.domNode.onload { loadImgToCanvas(srcImg, targetCanvas, imgContext) }\n    }\n}\nWhenever srcImg.onload event happens we call loadImgToCanvas which loads img on our canvas.\nWhy did I choose to not have a new <img>? Because we later need to use ImageData and this is the way to have the minimum number of data transitions, trust me 😉.\nLet’s start adding bindings for ONNX!\n\n\n\nBinding TS/JS is as simple as a .d.ts-file in TS. You define the component to bind, declare the types, e.g. function name, input and outout. Simple as that!\n@file:JsModule(\"onnxruntime-web\")   // npm-package\n@file:JsNonModule\n  \nimport kotlin.js.Promise  \n  \nexternal abstract class InferenceSession {  \n    fun run(feeds: FeedsType): Promise<ReturnType> // FeedsType / ReturnType separately defined the same way as InferenceSession & run.\n}\nMoving further we’ll add a method to extract ImageData’s UInt8ClampedArray from a img-element using a canvas-element with its CanvasRenderingContext2D (lots of JS/web words, the most I’ll have in a sentence, peeew! 😅)\nfun imgToUInt8ClampedArray(img: HTMLImageElement, ctx: CanvasRenderingContext2D): Uint8ClampedArray {\n    val canvas = ctx.canvas\n    canvas.width = img.naturalWidth\n    canvas.height = img.naturalHeight\n    ctx.drawImage(img, 0.0, 0.0)\n\n    return ctx.getImageData(0.0, 0.0, img.naturalWidth.toDouble(), img.naturalHeight.toDouble()).data // extract data from ImageData\n}\nThe UInt8ClampedArray has to be transformed into a Float32Array that the model expects.\nSounds easy? Think again!\nBecause JS is not a data science language it’s not surprising that the data is “incorrectly” ordered. The model expects the data to be formed as [3,width,height] where 3 is the number of dimensions, in our case RGB, but in JS it’s the reverse way. On top of the wrong ordering JS has a fourth dimension, namely transparency. Following all that knowledge we can transform the array.\nfun uInt8ClampedToFloat32Array(data: Uint8ClampedArray): Float32Array {\n    val floats = Float32Array(data.length / 4 * 3)\n    val rgb =listOf(0, data.length / 4, data.length / 4 * 2)\n\n    for (i in 0untildata.lengthstep4) {\n        floats[rgb[0] + i / 4] = data[i + 0] / 255f\n        floats[rgb[1] + i / 4] = data[i + 1] / 255f\n        floats[rgb[2] + i / 4] = data[i + 2] / 255f // Skip i+3 as that's ALPHA\n}\n\n    return floats\n}\nAs ONNX expects Tensor we need to transform the Float32Array into a Tensor and then into FeedsInput which is a Object of the data, luckily that’s very easy after our binding is done.\nfun tensorToInput(tensor: Tensor, inputName: String = \"input\"): FeedsType {\n    val input: dynamic = object {} // To hack JS Objects\n    input[inputName] = tensor\n\n    return input.unsafeCast<FeedsType>()\n}\n\nval tensor = Tensor(\"float32\", floats, arrayOf(1, 3, srcImg.domNode.naturalWidth, srcImg.domNode.naturalHeight))\nval input = tensorToInput(tensor)\n…and it’s time to run the model! 🥳\nval ir = InferenceSession.create(\"./dce2.onnx\").await()\nval out = ir.run(input).await()\n\nval outTensor = out[\"output\"] as Tensor\nval outData = outTensor.data as Float32Array\nThe output then needs to have the reverse transform applied to be viewable in the browser. That is, reverse axis, add fourth dimension and cast into int.\n// Calling on the output data, before converting to UInt8Clamped..\nfor (i in 0untiloutData.length) {\n    outData[i] = min(outData[i], 1f) * 255f // `min` to not go above 255\n}\n\nfun float32ToUInt8Clamped(data: Float32Array): Uint8ClampedArray {\n    val rgb =arrayOf(0, data.length / 3, data.length / 3 * 2)\n    val intOut = Uint8ClampedArray(data.length / 3 * 4)\n\n    for (i in 0untilintOut.length / 4) {\n        intOut.asDynamic()[i * 4 + 0] = data[rgb[0] + i].toInt()\n        intOut.asDynamic()[i * 4 + 1] = data[rgb[1] + i].toInt()\n        intOut.asDynamic()[i * 4 + 2] = data[rgb[2] + i].toInt()\n        intOut.asDynamic()[i * 4 + 3] = 255 }\n    \n    return intOut\n}\nAs you might notice we cast a lot asDynamic(), this is because of a current bug in Kotlin JS where it sends signed Byte when it should be an unsigned Byte.\nSee the current issue at youtrack.jetbrains.com.\nWe finally got all the pieces, how about gluing it all together? 😄\n\n\n\nThe model I wish to use has a dynamic input/output size, e.g. the image dimensions, I need to recreate the session or else it will throw as ONNX expects the last used shape on new runs. This is not true as images are of different sizes.\nOne solution would be to preprocess the image to always be the same size, but I prefer to return the image in the original dimensions for this use-case.\n\n\nView code!\n\nfun imgToUInt8ClampedArray(img: HTMLImageElement, ctx: CanvasRenderingContext2D): Uint8ClampedArray {  \n    /** same code as previously */\n}  \n  \nfun float32ToUInt8Clamped(data: Float32Array): Uint8ClampedArray {  \n    /** same code as previously */\n}  \n  \nfun tensorToInput(tensor: Tensor, inputName: String = \"input\"): FeedsType {  \n    /** same code as previously */  \n}  \n  \nfun uInt8ClampedToFloat32Array(data: Uint8ClampedArray): Float32Array {  \n    /** same code as previously */\n}  \n  \n@OptIn(ExperimentalTime::class, ExperimentalCoroutinesApi::class)  \nsuspend fun main() {  \n    val flow = RootStore(\"\")  \n    val isLoaded = RootStore(\"\")  \n    val webgl: dynamic = object {}  \n    webgl[\"executionProviders\"] = arrayOf(\"webgl\")  // want that WebGL GPU power\n  \n    render {  \n         val srcImg = img(id = \"img-from\") {  \n            src(flow.data)  \n            domNode.onload = { isLoaded.update(domNode.src) }  \n         }\n        val targetCanvas = canvas(id = \"img-to\") {}  \n        val imgContext = targetCanvas.domNode.getContext(\"2d\") as CanvasRenderingContext2D  \n \n        isLoaded.data  \n        .distinctUntilChanged()  \n        .filter { b64 -> b64.isNotEmpty() }\n        .map {  \n            val ir = runCatching { InferenceSession.create(\"./dce2.onnx\", webgl).await() }  \n                .onFailure { showAlertToast { alert { title(\"Could not load WebGL, using WASM.\") } } }  \n                .getOrDefault(InferenceSession.create(\"./dce2.onnx\").await())  \n            val intData = imgToUInt8ClampedArray(srcImg.domNode, imgContext)  \n            val floats = uInt8ClampedToFloat32Array(intData)  \n\n            val tensor = Tensor(\"float32\", floats, arrayOf(1, 3, srcImg.domNode.naturalWidth, srcImg.domNode.naturalHeight))  \n            val input = tensorToInput(tensor)  \n        \n            val out = ir.run(input).await()  \n            val outTensor = out[\"output\"] as Tensor  \nval outData = outTensor.data as Float32Array  \n\n            for (i in 0 until outData.length) {  \n                outData[i] = min(outData[i], 1f) * 255f  \n            }  \n            val intOut = float32ToUInt8Clamped(outData)  \n\n            ImageData(intOut, srcImg.domNode.naturalWidth, srcImg.domNode.naturalHeight)  \n        } handledBy { imageData -> imgContext.putImageData(imageData, 0.0, 0.0) }\n  \n    file {  \n        accept(\"image/*\")  \n        button { text(\"Single select\") }  \n     }.map { file -> \"data:${file.type};base64,${file.content}\" } handledBy flow.update  \n    }  \n}\n\nWith the joining bindings for ONNX.\n\n\n\n\nWrapping it all together I feel like I want to leave with the sentiment that KotlinJS is a player, ONNX Webruntime certainly is capable and I’ll continue creating small MVP:s and demos using this setup!\n\n\nRegarding KotlinJS I believe it’s still behind TypeScript in terms of compatibility. I need to do more plumbing than someone using TS would, especially as dukat don’t solve all my problems magically. Luckily it’s very easy to make those bindings!\nIn terms of how usable it is I find it much better than TypeScript, the experience when working with KotlinJS-code (e.g. interfacing std-lib, pure kotlin code or bindings) is so much better than TypeScript - it’s just like when I write my good ol’ JVM applications. I’m not sure if I’m missing something, but TypeScript’s typesystem always felt a bit choppy, just like Pythons. Sometimes I don’t get the intellisense I’m expecting.\n\n\n\nThe performance when using WebGL is definitely better than I expected, but not as good as using the usual runtime. Something I did notice during my testing is that it scales badly with size, using a high-res image (3000x4000) ends up slowing my whole computer. I know I’m not really working on a separate thread or anything, but it’s too bad it doesn’t scale well. Further there’s an internal max-limit somewhere around the same dimensions, which I hit once with another image.\n\nPersonally, including these issues, I’m left impressed about how easy it is to set up a completely custom model to run inside the browser (“on the edge”), where we don’t have to care about architecture, OS or anything and that it works efficiently enough to use.\n\nI can see this as a key tool to start-ups and larger companies to reduce costs & inference-time (as computation happens on the edge). On top of the $’s I see a win for privacy as the data will never leave the users device, which in turn simplifies GDPR compliance and much more!\nEven moving inference to the edge through a common simple interface that is the browser we’ll still have plenty of need for servers, not only serving larger models for complex problems, old devices and batch inference of larger amounts of data.\nThe future is indeed still moving fast for Deep Learning and I can’t wait to see where we’re moving!\nMy own prediction: Deep Learning will simply ignore serverless computing and jump straight to edge computing in an effort to reduce costs.\n\n\n\nCombining ONNX & KotlinJS (perhaps testing Compose rather than fritz2) is something I’m gonna keep on doing in the future to deploy demos. Either deploying through Github Pages or my own Raspberry Pi this will be piece a cake as my devices don’t have to do the inference, keeping my costs down for fun demos.\nDemo: A live demo can be found here.\nAnd the code can be found on github.com/londogard/photo-fritz2, but be careful - it’s not that beautiful right now 😰\nThat’s it for now.. 🥳\n~Hampus Londögård"
  },
  {
    "objectID": "posts/2020-12-12-transfer-learning-presentation.html",
    "href": "posts/2020-12-12-transfer-learning-presentation.html",
    "title": "Transfer Learning (Presentation)",
    "section": "",
    "text": "This is a presentation I did on Transfer Learning. \nTransfer Learning is the task to transfer knowledge from one task to another where we’ll end up with better performance using little data.\nSharing slides & presentation with the world. :)"
  },
  {
    "objectID": "posts/2022-11-28-probabilistic-forecasting/index.html",
    "href": "posts/2022-11-28-probabilistic-forecasting/index.html",
    "title": "Probabilistic Forecasting Made Simple",
    "section": "",
    "text": "Probabilistic Forecasting is something very cool, but it is not approachable in the current state of affairs. \nWhile researching probabilistic forecasting in a client project I managed to find a paper which opens the door to any neural network with dropout - which is the majority. That is, we can do probabilistic forecasting with essentially any network!\n\nDarts, a brilliant timeseries library, includes a very competent probabilistic forecasting but it’s not really applicable to all models. This is the reason that I started diving into the whole space of probabilistic forecasting. A probabilistic model includes not only a raw prediction value but a distribution of possible points, which ends up with a prediction like:\n\n\nProbabilistic Model by unit8/darts\n\nAdditionally models like ARIMA and ExponentialSmoothing allows to do this kind of thing very easily, simply sample running simulations of their state-spaced models with a bit of randomly sampled errors. To solve this on their deep learning models darts decided to model distribution using a Likelihood  class. What does this mean?\nThe model does not actually predict a value but a distribution, using Gaussian  we’d predict two values - mean  and std ."
  },
  {
    "objectID": "posts/2022-11-28-probabilistic-forecasting/index.html#how-to-do-probabilistic-forecasting-on-any-deep-learning-model",
    "href": "posts/2022-11-28-probabilistic-forecasting/index.html#how-to-do-probabilistic-forecasting-on-any-deep-learning-model",
    "title": "Probabilistic Forecasting Made Simple",
    "section": "How to do probabilistic forecasting on any deep learning model",
    "text": "How to do probabilistic forecasting on any deep learning model\nBy combining the knowledge in Deep and Confident Prediction Time Series at Uber by L. Zhi & N. Laptev (2017) with What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? by A. Kendall & Y. Gal (2017) one can conclude that it’s possible to model distributions using dropout during inference. In the Uber paper they use a special variant they call “Monte Carlo dropout”, which I don’t believe is required to achieve interesting results. Using the pure dropout-module which randomly zeroes some elements by a probability \\(p\\)  sampling from a Bernoulli Distribution.\nHow do we do this?\n\nActivate Dropout during Inference.\nDo \\(x\\)  predictions with a dropout probability \\(p\\).\nBased on these x predictions we have a distribution of data.\nBuild a confidence interval from the points.\n\n  outs = torch.vstack([model.predict(in_data) for i in range(x)])\n  # Defined by confidence coefficients\n  Z_TABLE = {0.8: 1.28, 0.85: 1.44, 0.9: 1.65, 0.95: 1.96, 0.99: 2.58, 0.999: 3.29, 0.9999: 3.89}\n\n  # Confidence Interval with mean as line\n  mean = outs.mean()\n  lower = mean - Z_TABLE[confidence] * outs.std()\n  upper = mean + Z_TABLE[confidence] * outs.std()"
  },
  {
    "objectID": "posts/2022-11-28-probabilistic-forecasting/index.html#the-possibilities",
    "href": "posts/2022-11-28-probabilistic-forecasting/index.html#the-possibilities",
    "title": "Probabilistic Forecasting Made Simple",
    "section": "The possibilities",
    "text": "The possibilities\nThere’s a lot of possiblities, I’ll share two of our biggest ones.\n\n1. Model Understanding (Weakness/Strength)\nBy returning a probabilistic forecast, i.e. a distribution/confidence interval, we can learn more about the model and its strengths/weaknesses. \nIn our project(s) we’ve seen that it opens a door to really figure out how to improve our models by focusing on the areas were the model is the most uncertain. This has proved to improve performance by a substantial amount which makes the effort worth it.\n\n\n2. Downstream Consumer Happiness\nWe see that our clients trust the model further by being able to see how confident they are. Building trust between model and downstream consumer is really important to deliver an actual successful project, which once again makes the effort totally worth it!\nBonus: we also found that it opens new possibilities to chain of the inference power if you keep it in production, as your downstream tasks can now make use of a confidence interval rather than a raw data point. But the inference is very expensive compared to the usual (remember we do x predictions per prediction)!"
  },
  {
    "objectID": "posts/2022-11-28-probabilistic-forecasting/index.html#sources",
    "href": "posts/2022-11-28-probabilistic-forecasting/index.html#sources",
    "title": "Probabilistic Forecasting Made Simple",
    "section": "Sources",
    "text": "Sources\nDeep and Confident Prediction Time Series at Uber by L. Zhi & N. Laptev (2017) - https://arxiv.org/pdf/1709.01907.pdf\nWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? by A. Kendall & Y. Gal (2017) - https://arxiv.org/pdf/1703.04977.pdf"
  },
  {
    "objectID": "posts/2022-11-23-timeseries-learnings/index.html",
    "href": "posts/2022-11-23-timeseries-learnings/index.html",
    "title": "Timeseries Learnings at AFRY",
    "section": "",
    "text": "Intro\nThis is a blog based on a presentation I did at Foo Café, where we shared a lot of our hard-earned wisdom at AFRY X, mainly based on working with timeseries.\nThere’s both simple and more advanced, hopefully in an easy to understand fashion! \n\n\nWe’re currently working with two (2) assignments involving timeseries right now. \n\nHelping a telecom company to embrace Data Driven Testing of 5G-antennas\nHelping a automotive company to forecast weird brake behavior.\n\nSo what have we learned? Let’s see…"
  },
  {
    "objectID": "posts/2022-11-23-timeseries-learnings/index.html#learning-to-embrace-kiss",
    "href": "posts/2022-11-23-timeseries-learnings/index.html#learning-to-embrace-kiss",
    "title": "Timeseries Learnings at AFRY",
    "section": "1. Learning to embrace KISS 💋",
    "text": "1. Learning to embrace KISS 💋\nWe learned this by a few different experiences.\n\nMLOps Tooling\nYes, it’s real. We all say it, all the time. But when the dust settles a lot of us ended up optimizing some part of our code anyways or even adding the “better tool” which involves just a “tiny bit” more complexity and code.\nWell, we did end up adding that “better tool” and it bit us back by clearly reducing our innovation pace. The story…\nTo build our MLOps pipeline we used a tool (DVC) which is way better than the “default” of MLFlow. It has multiple bonuses:\n\nData Version Control\nReproducible Pipelines\nCI/CD custom-made for Machine Learning\nCloud & Tool Agnostic\nSimply git\n\nThere’s no real drawbacks, except the KISS principle. Don’t get me wrong DVC is an excellent tool but we ended up having to write a lot of code to download datasets, track models, keep things tidy in git. To top it off? It was cumbersome to test a new task inside the same repository.\nThe biggest drawback was the additional code, albeit not complex in itself it added a total complexity, each time we wanted to update something in the pipeline we had to make sure to keep DVC working.\nAnd to get new features it was a little bit of adding lego bricks, which I love but ended up adding more complexity again.\nI want to note that the complexity was from lines of code rather than complicated code and this complexity made it harder for us to innovate.\nBiting the bullet and migrating to MLFlow, an inferior tool on the paper, made our pipelines more lean and easier to innovate upon.\nMLFlow has magic integrations into all cloud providers and libraries which makes it very easy to add with basically 0 code.\nThis simplification led to further gains which are hard to show on paper.\nIn the end with our small team-size and project-size the gains of DVC isn’t worth the cost, as such the KISS approach leads us to MLFlow.\n\n\nLocal ↔︎ Cloud\nCloud compute brings a ton of goodies such as defaulting to containerization, which should be done locally to really, and having powerful computers at your fingertips.\nBut what is forgotten is that with great powers comes great responsibility. Cloud enables training heavy models but with that it hides a lot of problems.\n\nInefficient pipelines\nHard to debug\n…\n\nWe’ve learned that using a Local First Approach gives us the best of both worlds. Our pipelines are able to run fully local, including unit testing, but are just as simple to run on a cluster.\nThis is enabled by using a local pipeline rather than fully embracing the ecosystem. \nWe are able to run:\n\n⁠pipeline.py\npipeline_on_azure.py --exp_name <NAME> --compute <COMPUTE>\npipeline_mlflow_azure.py\n\nAllows local experiments to be tracked on Azure MLFlow server rather than locally\n\n\nWhich makes our life incredibly easy!\n\n\nAutomate Boring Checks\nOf course code reviews should, and is, mandatory. Based on our software engineering principles we’ve made sure to also add CI/CD verifications which validates that everything is nice and tidy with no breaking changes. It reduces our cognitive load, which is awesome.\nOur current set-up:\n\npre-commit: local validation on each commit\n\n[flake8](https://github.com/pycqa/flake8 \"https://github.com/pycqa/flake8\"): Code Style Checker\n\nValidates that we don’t break code-styles such as unused imports, unused variables, too complicated functions etc\n\n[black](https://github.com/psf/black \"https://github.com/psf/black\"): Code Formatter\n\nIt’s uncompromising and makes sure our repository has a standard stylistic with correct indenting and much more\n\n[mypy](https://github.com/python/mypy \"https://github.com/python/mypy\"): Static Type Checker\n\nMakes sure that our types are valid and we’re not simply lucky in the duck-typing 🦆 world of Python 🐍!\n\n^ All above also runs in CI/CD\n\nCI/CD\n\npytest: unit tests\n\nTest that your functions, neural networks etc works as expected\n\npre-commit - see above\ncypress: E2E frontend testing\n\nOnly for a user-facing analysis tool\n\n\n\nThis is running on both GitHub Actions and GitLab Pipelines.\nWe deploy our containers through Azure and experiment through the Azure ML Studio or locally."
  },
  {
    "objectID": "posts/2022-11-23-timeseries-learnings/index.html#interactive-validation",
    "href": "posts/2022-11-23-timeseries-learnings/index.html#interactive-validation",
    "title": "Timeseries Learnings at AFRY",
    "section": "2. Interactive Validation 👨‍💻",
    "text": "2. Interactive Validation 👨‍💻\nThis is the killer deal. A lot of people out there makes heavy use of what I call “Static Analysis”, where metrics and static images are viewed. \nViewing static results isn’t enough, it barely scraps the surface and we’re Data * right? So why not dive into the data!? 🕵️\nWe’re extending our analyses to include custom built Streamlit Application(s) which allows us to work exploratory. Static validation is still important to keep track of actual objective metrics, but exploration is not only very satisfying but also awesome as you learn so much more about the model(s) and data.\nInteractive analysis gives the following: Promote Explorability, Gives End-User a Great Experience, Increase Explainability, Efficient Feedback-Loop. Awesome? Hell yeah! 😎\nStatic Validation:\n\nInteractive Validation:"
  },
  {
    "objectID": "posts/2022-11-23-timeseries-learnings/index.html#additional-wisdom",
    "href": "posts/2022-11-23-timeseries-learnings/index.html#additional-wisdom",
    "title": "Timeseries Learnings at AFRY",
    "section": "3. Additional Wisdom 🤓",
    "text": "3. Additional Wisdom 🤓\n\nProbabilistic Forecasting\nThis is a powerful tool to have in your arsenal, and with clever tricks you can have it with all  neural networks that uses dropout! 🤯\nThe idea is that rather than forecast/predict point by point we forecast a confidence interval! This way we capture uncertaintied of the model.  This  assists debugging your model to fix its weaknesses! 🔍\n\n\nCustom Loss Functions\nUsing a custom loss function makes a lot of sense in almost any real problem. It’s very rare that you actually care about the overall impact, but rather specific regions actually matters in time-series. In our case we don’t care about forecasting noise nor do we care about forecasting the bad behavior after it happened, we need to find it before it happens - forecasting.\nIn our case it’s an oscillating behavior that builds amplitude over time, as we want the model to do the same we need to penalize undershooting heavily to not fit it to the noice.\nThis ended up being a big improvement, but what lessons did we learn?\n\nAlways invert scale before calculating test-metrics\n\nTo not allow scaling functions to impact the final results\n\nNever optimize a loss function that you use as a metric\n\nThis will play the model\n\nDecide on validation metrics before  you start\n\nA moving target is impossible to hit and compare against"
  },
  {
    "objectID": "posts/2022-11-23-timeseries-learnings/index.html#unexpected-learnings",
    "href": "posts/2022-11-23-timeseries-learnings/index.html#unexpected-learnings",
    "title": "Timeseries Learnings at AFRY",
    "section": "4. Unexpected Learnings 🤔",
    "text": "4. Unexpected Learnings 🤔\n\nModel weights badly instantiated\nWe found this to be true in both papers and (5k⭐️) Open Source libraries. \nMake sure that you validate code you’re using in-depth and don’t expect it to work as expected from the get-go!\n\n\nBest Practices is contextual\nBest Practices are certainly not true for any example or user. This was shown in multiple cases, such as our Local ↔︎ Cloud story.\nMake sure to validate that what you’re doing is actually required for your use-case. A lot of the tools out there is built for huge scale while we’re working on small-medium scale. To use the same tooling is to introduce a large overhead and disconnect from the developer."
  },
  {
    "objectID": "posts/2022-11-23-timeseries-learnings/index.html#generally-winning-concepts",
    "href": "posts/2022-11-23-timeseries-learnings/index.html#generally-winning-concepts",
    "title": "Timeseries Learnings at AFRY",
    "section": "Generally winning concepts",
    "text": "Generally winning concepts\n\nType hints, Type Hints everywhere\n\nIt really assists you greatly. As a Scala/Kotlin and FP-enthusiast I’d like to talk even further about it, but I might grow boring.\n\nPlotly/Altair rather than matplotlib – interactivity is king.\n\n\nI cannot emphasize how much is learnt by the kinder Garten style of panning, zooming and playing around\nIt gives data and model understanding\n\nPolars – efficient speedy DataFrame’s with a logical API\n\nI can’t be alone thinking that whoever designed pandas API must’ve been a masochist\n\nPolars makes sense, includes lazy and it’s fast! 🏎️\nCon: it’s not lingua franca like pandas and thereby isn’t supported automatically by all different libraries\n\nSolved by using to_pandas()\n\n\n  >>> df.sort(\"fruits\").select(\n  ...     [\n  ...         \"cars\",\n  ...         pl.col(\"B\").filter(pl.col(\"cars\") == \"beetle\").sum(),\n  ...         pl.col(\"A\").filter(pl.col(\"B\") > 2).sum().over(\"cars\").alias(\"sum_A_by_cars\"),\n  ...         pl.col(\"A\").sum().over(\"fruits\").alias(\"sum_A_by_fruits\"),\n  ...         pl.col(\"A\").reverse().over(\"fruits\").alias(\"rev_A_by_fruits\"),\n  ...         pl.col(\"A\").sort_by(\"B\").over(\"fruits\").alias(\"sort_A_by_B_by_fruits\"),\n  ...     ]\n  ... )\n\n\n\nStreamlit quick interactive apps that makes a huge difference\n\nThis is how we build our interactive validation and analysis tools. \n\nCI/CD on all projects\nQuarto reports\n\nThey’re amazing, think markdown + code cells + all the goodies of LaTeX in a package 😍\n\nUse Path-lib from the get-go\n\nDon’t waste a full working day of headache to help that Windows-user to run your project"
  },
  {
    "objectID": "posts/2022-03-12-timeseries-pt-2/index.html#predicting-time-series",
    "href": "posts/2022-03-12-timeseries-pt-2/index.html#predicting-time-series",
    "title": "Predicting Stock Prices using classical machine Learning (Time Series #2)",
    "section": "Predicting Time Series 📈",
    "text": "Predicting Time Series 📈\nToday we will move from learning how to analyze Time Series to actually predicting them using simple models and data.\nWe’ll be predicting Stocks from the top tech companies like Apple & Google.\nIn part #3 we’ll move back to the crypto world!\nTo be able to predict the data we must understand it and we’ll make a minor analysis.\n\nInstallation & Imports\nFeel free to ignore the cells and simply run them, the lazy style 🥱\nInstalling the important libraries…\n\nfrom IPython.display import clear_output\n!pip install -U pandas_datareader\n!pip install plotly\n!pip install matplotlib==3.1.3\n\nclear_output()\n\nAnd importing them…\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np  # linear algebra\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nfrom datetime import datetime\n\n\n\nMinor Analysis\n\ndf = pdr.get_data_yahoo(['AAPL', 'GOOGL', 'AMZN', 'MSFT', 'GE'])\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      Attributes\n      Adj Close\n      Close\n      ...\n      Open\n      Volume\n    \n    \n      Symbols\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n      GE\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n      GE\n      ...\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n      GE\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n      GE\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2017-03-13\n      32.856758\n      864.580017\n      854.590027\n      60.405796\n      214.961868\n      34.799999\n      864.580017\n      854.590027\n      64.709999\n      229.692307\n      ...\n      34.712502\n      860.830017\n      851.770020\n      65.010002\n      232.538467\n      69686800.0\n      1166600.0\n      1909700.0\n      20100000.0\n      3924414.0\n    \n    \n      2017-03-14\n      32.807198\n      865.909973\n      852.530029\n      60.125748\n      212.658188\n      34.747501\n      865.909973\n      852.530029\n      64.410004\n      227.230774\n      ...\n      34.825001\n      863.750000\n      853.549988\n      64.529999\n      228.923080\n      61236400.0\n      1061700.0\n      2130600.0\n      14280200.0\n      2964208.0\n    \n    \n      2017-03-15\n      33.154182\n      868.390015\n      852.969971\n      60.443142\n      214.241943\n      35.115002\n      868.390015\n      852.969971\n      64.750000\n      228.923080\n      ...\n      34.852501\n      867.940002\n      854.330017\n      64.550003\n      227.307693\n      102767200.0\n      1332900.0\n      2562200.0\n      24833800.0\n      3268564.0\n    \n    \n      2017-03-16\n      33.208458\n      870.000000\n      853.419983\n      60.340446\n      214.169952\n      35.172501\n      870.000000\n      853.419983\n      64.639999\n      228.846161\n      ...\n      35.180000\n      870.530029\n      855.299988\n      64.750000\n      229.230774\n      76928000.0\n      1104500.0\n      1842300.0\n      20674300.0\n      2756910.0\n    \n    \n      2017-03-17\n      33.043243\n      872.369995\n      852.309998\n      60.555161\n      215.105865\n      34.997501\n      872.369995\n      852.309998\n      64.870003\n      229.846161\n      ...\n      35.250000\n      873.679993\n      853.489990\n      64.910004\n      229.615387\n      175540000.0\n      1868300.0\n      3384400.0\n      49219700.0\n      5673070.0\n    \n  \n\n5 rows × 30 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLooks fine, but how much data did we download?\nWe can view the .index which is a DateTimeIndex and figure out how it stretches.\n\ndf.index[0],df.index[-1]\n\n(Timestamp('2017-03-13 00:00:00'), Timestamp('2022-03-11 00:00:00'))\n\n\nHmm, 5 years, that should be enough to find some kind of patterns.\nNow let us analyze this data further by looking at if the stocks correlate somehow! 🤠\n\nN.B. this analysis was first done by Heidi Mach, it’s something I would never have done myself. Really cool results incoming!\n\n\ndf['Adj Close'].corr().style.background_gradient(cmap=\"Blues\")\n\n\n\n\n  \n    \n      Symbols\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n      GE\n    \n    \n      Symbols\n       \n       \n       \n       \n       \n    \n  \n  \n    \n      AAPL\n      1.000000\n      0.951499\n      0.935116\n      0.978268\n      -0.283789\n    \n    \n      GOOGL\n      0.951499\n      1.000000\n      0.866575\n      0.960291\n      -0.192826\n    \n    \n      AMZN\n      0.935116\n      0.866575\n      1.000000\n      0.944313\n      -0.499768\n    \n    \n      MSFT\n      0.978268\n      0.960291\n      0.944313\n      1.000000\n      -0.374930\n    \n    \n      GE\n      -0.283789\n      -0.192826\n      -0.499768\n      -0.374930\n      1.000000\n    \n  \n\n\n\nHoly macaron, that’s a lot more correlated data than I expected! 🙀\nThe seaborn library has a function called pairplot which plots this correlation, but using the points which is visually interesting in comparison to simply seeing the table above.\n\ndf = df['Adj Close']\ndf = df.drop(columns=\"GE\")\n\n\nsns.pairplot(df)\n\n<seaborn.axisgrid.PairGrid at 0x7f784a7b8a10>\n\n\n\n\n\nDoes this in fact mean what that we can predict prices of a stock based on their competition? The correlation does suggest so.\nLet’s try it!\nFirst we’ll try using a LinearRegression which simply said fits a line to be as close to all points as possible.\n > Source: Wikipedia.org\nFirst we import LinearRegression through scikit-learn and then we add train_test_split which allows us to split our data into a training and testing dataset.\nWhenever you test your Machine Learning or Deep Learning Models you never want to test it on data that it has trained on, as you might’ve overfitted the data and have a really good result until you see new data points.\nThe end-goal of a model is to generalize a problem and find the local minima which optimizes the funtion for the data points. By only looking at the same data we can’t be sure we generalized correctly.\nAnd the code 👩‍💻\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nnon_google_df = df.drop(columns=\"GOOGL\")\nX_train, X_valid, y_train, y_valid = train_test_split(non_google_df, df['GOOGL'], test_size=0.2)\n\nclf = LinearRegression()\n\nWe got our data divided into valid and train, we got a regression model in our clf.\nLet us predict the data and view our r2_score and mean_absolute_error.\n\n💡\nr2_score: (coefficient of determination) regression score function.\nBest possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a score of 0.0.\nmean_absolute_error: Mean absolute error regression loss.\n\n\nclf.fit(X_train, y_train)\npreds = clf.predict(X_valid)\n\nr2_score(y_valid, preds), mean_absolute_error(y_valid, preds)\n\n(0.9495278677136834, 120.88640110098741)\n\n\n\\(R^2 = 93 \\%\\) 🥳\nThat’s actually not bad at all, the mean_absolute_error being 129.7 is not very telling. Either we have to view the data to understand the magnituide, or we can apply MAPE which is the Mean Absolute Percentage Error.\nNot sure if I’m lazy or simply want to show you the other function 🤔, but I’ll use MAPE!\n\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmean_absolute_percentage_error(y_valid, preds)\n\n0.07891126366405354\n\n\n\\(< 9\\%\\)\nPretty acceptable considering we have not done anything except deliver data to one of the simplest models that exists!\nLet’s show this visually!\n\nimport plotly.express as px\n\npx.line(y=[y_valid, preds])\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nShow Plotly Chart (code cell only visible in active notebook)\n\n\n\nLooks pretty good, but it is very messy… Something is off right?\nThe index is not a DateTimeIndex anymore because we shuffled the data in train_test_split – a big difference is thereby applied.\n\ny_valid.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f784b45c8d0>\n\n\n\n\n\n\ny_valid.plot(legend=\"Valid\")\npd.Series(preds, index=y_valid.index).plot(legend=\"Pred\")\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f784b3206d0>\n\n\n\n\n\nLooks pretty fly, but can we take it further?\n…yes we can! 😎\nI see a few options, the two first being:\n\nScaling the data as errors at the end are larger than in the beggining based on stocks rising.\nLinearRegression is a very simple yet efficient model that we can try to replace.\n\nLet’s start with the second point, scikit-learn has a multitude of regression-models, one being RandomForestRegressor that’s pretty strong.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nclf = RandomForestRegressor()\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_valid)\nr2_score(y_valid, preds), mean_absolute_percentage_error(y_valid, preds)\n\n(0.9963567300739637, 0.016329169939121136)\n\n\n😲\\(R^2 >99\\%\\)\nThat’s actually crazy. And MAPE is not even 2%.\nLet’s view it!\n\ny_valid.plot(legend=\"Valid\")\npd.Series(preds, index=y_valid.index).plot(legend=\"Pred\")\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f7849f82190>\n\n\n\n\n\nThat’s an incredibly fitted curve.\nHow is this possible?\n1. We most likely overfit the data.\n2. We are looking at AMZN, AAPL and more data that is highly correlated during the same day as the one we wish to predict. - In the end this is a useless task, if we know the prices of today we’d also know GOOGL’s prices! 3. We’re using shuffled data, meaning that in a way we’ve seen the future and past values surrounding the predicted one. This is a regression problem and not really a forecasting problem, which is simpler than forecasting.\nImpressive nontheless\nEven as I’m aware of all the drawbacks I’m thouroughly impresed by the results we’re seeing.\nMaking it more interesting\nWe should make use of the previous days data to make sure we are not “cheating”.\nLet’s get on it! 🎯\n\nWe’ll be able to move, or shift, the data using ˋpd.DataFrame.shiftˋ which shifts the data either forwad (\\(+X\\)) or backwards (\\(-X\\)).\nAnd while we’re at it, let’s group this up into a function.\n\nℹ️\npd.DataFrame.shift: Shift index by desired number of periods with an optional time freq.\n\n\ndef fit_validate_plot(X_train, X_valid, y_train, y_valid):\n  clf = RandomForestRegressor()\n  clf.fit(X_train, y_train)\n\n  preds = clf.predict(X_valid)\n  pd.DataFrame({'Valid': y_valid, 'Preds': preds}, index=y_valid.index).plot()\n  \n  print(f\"\"\"\n  $R^2$: {r2_score(y_valid, preds)}\n  MAPE: {mean_absolute_percentage_error(y_valid, preds)}\n  MAE: {mean_absolute_error(y_valid, preds)}\n  \"\"\")\n\nAnd making use of it will now be easy! 😍\nRefactoring and abstractions are incredibly important.\n\nX_train, X_valid, y_train, y_valid = train_test_split(df.drop(columns=\"GOOGL\").shift(1).iloc[1:], df['GOOGL'].iloc[1:], test_size=0.2)\n\nfit_validate_plot(X_train, X_valid, y_train, y_valid)\n\n\n  $R^2$: 0.993020486528333\n  MAPE: 0.022105878493046974\n  MAE: 32.473562898181726\n  \n\n\n\n\n\n🤯 this is crazy impressive!\nWe made the task at hands legit by only using historical data of GOOGL’s competitors. The \\(R^2\\) and MAPE is incredible.\nIt’d be interesting to investigate how badly we overfit the data, but that’s for another day.\nAnd how about if we don’t shuffle the data? E.g. we do an actual forecast and not regression!\n\nX_train, X_valid, y_train, y_valid = train_test_split(df.drop(columns=\"GOOGL\").shift(1).iloc[1:], df['GOOGL'].iloc[1:], test_size=0.2, shuffle=False)\n\nfit_validate_plot(X_train, X_valid, y_train, y_valid)\n\n\n  $R^2$: -7.266054917956771\n  MAPE: 0.24703039286319634\n  MAE: 675.8480126274956\n  \n\n\n\n\n\n🤯😭\nWhat are we seeing and why?\nRegression algorithms/models try to fit a line to multiple points and it should be able to guess what point the data has depending on its features. In our case the regression algorithm has never seen data as high as above y_train.max(), which means it can’t guess the data.\nDon’t trust me? Simply validate by looking at the chart 👆.\n\nWhat’s one way to fix this? Scaling\nHow will we try to achieve this practically? LogReturn\n\n💡 You can also take the %-difference, which according to Taylors Theorem will approximate the LogReturn.\n\n\ndef log_return(x: pd.DataFrame) -> pd.DataFrame:\n  return x.apply(lambda x: np.log(x/x.shift(1))).dropna()\n\nlog_return(df).head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      Symbols\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2017-03-14\n      -0.001510\n      0.001537\n      -0.002413\n      -0.004647\n    \n    \n      2017-03-15\n      0.010521\n      0.002860\n      0.000516\n      0.005265\n    \n    \n      2017-03-16\n      0.001636\n      0.001852\n      0.000527\n      -0.001700\n    \n    \n      2017-03-17\n      -0.004987\n      0.002720\n      -0.001301\n      0.003552\n    \n    \n      2017-03-20\n      0.010446\n      -0.005126\n      0.005453\n      0.000925\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf_lr = log_return(df)\nX_train, X_valid, y_train, y_valid = train_test_split(df_lr.drop(columns=\"GOOGL\").shift(1).iloc[1:], df_lr['GOOGL'].iloc[1:], test_size=0.2, shuffle=False)\n\nfit_validate_plot(X_train, X_valid, y_train, y_valid)\n\n\n  $R^2$: -0.13772674709610588\n  MAPE: 3.1419364874467086\n  MAE: 0.01222694796828883\n  \n\n\n\n\n\nMost certainly not perfect… Forecasting seems harder than expected based on our initial results…\nAnd that’s really because we weren’t forecasting before, we were solving a regression-problem\nPerhaps we need to use more data than simply the previous day?\n\n\nPredicting Based on historical performance\nWe might predict based on historical performance.\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      Symbols\n      AAPL\n      GOOGL\n      AMZN\n      MSFT\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2017-03-13\n      32.856758\n      864.580017\n      854.590027\n      60.405796\n    \n    \n      2017-03-14\n      32.807198\n      865.909973\n      852.530029\n      60.125748\n    \n    \n      2017-03-15\n      33.154182\n      868.390015\n      852.969971\n      60.443142\n    \n    \n      2017-03-16\n      33.208458\n      870.000000\n      853.419983\n      60.340446\n    \n    \n      2017-03-17\n      33.043243\n      872.369995\n      852.309998\n      60.555161\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf = df[['GOOGL']]\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      Symbols\n      GOOGL\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2017-03-13\n      864.580017\n    \n    \n      2017-03-14\n      865.909973\n    \n    \n      2017-03-15\n      868.390015\n    \n    \n      2017-03-16\n      870.000000\n    \n    \n      2017-03-17\n      872.369995\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n✅ Only Google Data\n❌ Historical Data\nSo what should we do? One way to solve this is to use shift multiple times.\n\ndef build_history(df: pd.DataFrame, num_back: int) -> pd.DataFrame:\n  for i in range(num_back):\n    df.loc[:, f\"t_{i}\"] = df['GOOGL'].shift(i + 1)\n  \n  return df\n\nbuild_history(df, 3).head()\n\n/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      Symbols\n      GOOGL\n      t_0\n      t_1\n      t_2\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2017-03-13\n      864.580017\n      NaN\n      NaN\n      NaN\n    \n    \n      2017-03-14\n      865.909973\n      864.580017\n      NaN\n      NaN\n    \n    \n      2017-03-15\n      868.390015\n      865.909973\n      864.580017\n      NaN\n    \n    \n      2017-03-16\n      870.000000\n      868.390015\n      865.909973\n      864.580017\n    \n    \n      2017-03-17\n      872.369995\n      870.000000\n      868.390015\n      865.909973\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nNotice how \\(t_0\\) is the previous value, \\(t_1\\) two steps back, and so on.\nThis is actually very memory intense as our data grows X times, one time per time step we build. In part #3 we’ll go through how one can solve this issue.\nNo we need to drop all places where we don’t have any history. That is easily achieved by dropping NaN.\n\nℹ️\npd.DataFrame.dropna: Remove missing values.\naxis attribute tells if you wish to drop rows or columns based on NaN, default is row.\n\n\ndf = build_history(df, 7)\ndf = df.dropna()\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      Symbols\n      GOOGL\n      t_0\n      t_1\n      t_2\n      t_3\n      t_4\n      t_5\n      t_6\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2017-03-22\n      849.799988\n      850.140015\n      867.909973\n      872.369995\n      870.000000\n      868.390015\n      865.909973\n      864.580017\n    \n    \n      2017-03-23\n      839.650024\n      849.799988\n      850.140015\n      867.909973\n      872.369995\n      870.000000\n      868.390015\n      865.909973\n    \n    \n      2017-03-24\n      835.140015\n      839.650024\n      849.799988\n      850.140015\n      867.909973\n      872.369995\n      870.000000\n      868.390015\n    \n    \n      2017-03-27\n      838.510010\n      835.140015\n      839.650024\n      849.799988\n      850.140015\n      867.909973\n      872.369995\n      870.000000\n    \n    \n      2017-03-28\n      840.630005\n      838.510010\n      835.140015\n      839.650024\n      849.799988\n      850.140015\n      867.909973\n      872.369995\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nLGTM ✅\n\nLet’s scale our data and then make predictions.\nAs previously,\n\nScale data\nSplit data\nFit data\nPredict data\nValidate\n\n\ndf_lr = log_return(df)\nX_train, X_valid, y_train, y_valid = train_test_split(df_lr.iloc[:, 1:], df_lr['GOOGL'], test_size=0.2, shuffle=False)\n\nfit_validate_plot(X_train, X_valid, y_train, y_valid)\n\n\n  $R^2$: -0.12436497765470134\n  MAPE: 2.020486730297106\n  MAE: 0.011961155812517109\n  \n\n\n\n\n\nNot great, not awful. Some self-exercises:\n\nHow would we do without scaling?\nHow would we do without shuffling?\nAny other ideas? Try ’em out!\n\n\n# Test your own ideas\n\nIf you didn’t try previously, try appling a rolling mean and rerun fit_validate_plot as this should reduce the “swings” and thereby be a little bit more predictable.\n\n💡 pd.DataFrame.Rolling: Provide rolling window calculations.\nIn other words: We slide a window on our data and do calculations, in our case mean. This window includes window, min_periods, center & more attributes which impacts size of window, how large minimal window can be, and more.\n\nValidating what rolling.mean() does to our data:\n\ndf['GOOGL_ROLLING'] = df['GOOGL'].rolling(3).mean()  # Rolling over 3 days mean\ndf[-100:].plot(y=['GOOGL', 'GOOGL_ROLLING'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f7848008410>\n\n\n\n\n\nZooming 🔍\n\ndf_last_months = df[df.index > datetime(2021, 6, 6)]\n\ndf_last_months.plot(y=['GOOGL', 'GOOGL_ROLLING'], backend='plotly')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nShow Plotly Chart (code cell only visible in active notebook)\n\n\n\nThe curve is very similar, but different.\n\nSelf-exercise: Test applying different functions like min, max and expanding window size into more days.\n\nAnd validating what this does to our prediction.\n\ndf_lr = df.pct_change().dropna().rolling(3).mean().dropna()\nX_train, X_valid, y_train, y_valid = train_test_split(df_lr.iloc[:, 1:], df_lr['GOOGL'], test_size=0.2, shuffle=False)\n\nfit_validate_plot(X_train, X_valid, y_train, y_valid)\n\n\n  $R^2$: 0.8925898146393952\n  MAPE: 0.7957020086330117\n  MAE: 0.0020138783337579776\n  \n\n\n\n\n\nWe’re back! 🥳\nIt’s not perfect, but we got something. And we can work with something. We can work with something… :)\n\nSelf-exercise: Validat how rolling would affect our non-history-based forecasting\n\nLet’s reverse our transformation to see what we’d actually predict in the end.\n\ny_rolling = df['GOOGL'].rolling(3).mean().dropna()\ny_train_non_scaled, y_valid_non_scaled = train_test_split(y_rolling, test_size=0.2, shuffle=False)\n\n\nclf = RandomForestRegressor()\nclf.fit(X_train, y_train)\npreds = clf.predict(X_valid)\n\npreds = (preds + 1).cumprod() # Cummulative multiplication, first day + 1%, but then we got -1%, that's 1.01 * 0.99\npreds = preds * y_train_non_scaled.iloc[-1] # Scaling it up based on the last training value\n\npd.DataFrame({'Preds': preds, 'Valid Rolling': y_valid_non_scaled[1:], 'Valid': df['GOOGL'].iloc[-len(preds):]}).plot(backend='plotly')\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nShow Plotly Chart (code cell only visible in active notebook)\n\n\n\nSeems as we’re a little low in our predictions, but the curve is followed after all.\nWhat issues are left?\n\nWe are not using an AutoRegressive model which might be interesting.\n\n\nMore about this in the next session\n\n\nWe are not using the “better” models, e.g. Neural Networks or statistic-model for Time Series like ARIMA.\n\nPersonally I’m very pleased with the results and can’t wait to get started on part #3!\n\nTo learn more about Time Series and how one can analyze them please view the other parts,\n\nPart One - Decomposing & Working with Time Series (theoretical)\nPart Two - Predicting Stock Prices (Time Series) using classical Machine Learning\nPart Three -Forecasting Cryptocurrency Prices (Time Series) using Deep Learning (PyTorch, Tensorflow/Keras & darts)\n\n\n\nExtra Self Exercises\n\nTry different window-sizes with rolling\nTry different length of history to predict new result on\nTest new architectures\nFind your own way to improve the results"
  },
  {
    "objectID": "posts/2022-03-11-timeseries-pt-1/index.html#time-series-concepts",
    "href": "posts/2022-03-11-timeseries-pt-1/index.html#time-series-concepts",
    "title": "Decomposing & Working with Time Series (Time Series #1)",
    "section": "Time Series Concepts",
    "text": "Time Series Concepts\nTime Series has some important attributes that are unique compared to other data types such as Text, Image and Tabular.\nTime Series can be decomposed into multiple other time series that together compose the decomposed one (composition baby!).\n\n\n\n\n\n\n\n\nTrend\nSeasonality\nCombined\n\n\n\n\n\n\n\n\n\n\nAs shown above we can build a time series out of a Trend and Seasonality, that means we can also decompose the Combined into Trend and Seasonality.\nThis can be done in a few ways, mostly either through a additive or multiplicative decomposition.\n\nAdditive means that if we add Trend and Seasonality together we create Combined.\nMultiplicative means that if we multiply Trend and Seasonality together we create Combined.\n\nBut these two are not enough to compose a full time series, usually we have noise too.\n\n\n\n\n\n\n\nNoise\nTrend+Seasonality+Noise\n\n\n\n\n\n\n\n\n\nIt seems we’re onto something. But still it’s not really how we usually see time series!\nWhat else is left? Autocorrelation!\nAutocorrelation is a correlation between two observation at different time steps, if values separated by an interval have an strong positive or negative correlation it’s indicated that past values influence the current value.\nTo keep it simple, if a time series is autocorrelated it’s new state has a correlation with a earlier step.\n\n\n\n\n\n\n\nAutocorrelation\nTrend+Autocorrelation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrend+Seasonality+Autocorrelation\nTrend+Seasonality+Autocorrelation+Noise\n\n\n\n\n\n\n\n\n\nAnd there we go! 😎\nWe got a legit timeseries, pat yourself on your back and be proud. We did it.\nLet’s get started with real data… 🧑‍💻\n\nInstallation & Importing\nWe need to have libraries which allows us to simplify operations and focus on the essential.\n\npandas is a DataFrame library that allows you to visualize, wrangle, and investigate data in different ways\nnumpy is a math library which is implemented in C and Fortran and combined with vectorized code it’s incredibly fast!\n\nThis is key in Python as Python is a really slow language.\n\npandas_datareader gives us easy access to different data formats, directly in pandas\nplotly gives your plots the next level interactivity that you’ve always wanted\n\n\nfrom IPython.display import clear_output\n!pip install -U pandas_datareader\n!pip install plotly\n\nclear_output()\n\nAnd importing our libraries\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np  # linear algebra\nimport pandas_datareader as pdr\n\nfrom datetime import datetime\n\n\n\nReading Cryptocurrency Data\nUsing pandas_datareader we can query multiple API:s, and among them Yahoo.\nYahoo as a financial API that follows stocks, currencies, cryptocurrencies and much more!\nLet’s see what we can do.\n\ndf = pdr.get_data_yahoo('BTC-USD', start=datetime(2020, 1, 1), end=datetime(2022, 1, 1))\ndf.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-01\n      7254.330566\n      7174.944336\n      7194.892090\n      7200.174316\n      18565664997\n      7200.174316\n    \n    \n      2020-01-02\n      7212.155273\n      6935.270020\n      7202.551270\n      6985.470215\n      20802083465\n      6985.470215\n    \n    \n      2020-01-03\n      7413.715332\n      6914.996094\n      6984.428711\n      7344.884277\n      28111481032\n      7344.884277\n    \n    \n      2020-01-04\n      7427.385742\n      7309.514160\n      7345.375488\n      7410.656738\n      18444271275\n      7410.656738\n    \n    \n      2020-01-05\n      7544.497070\n      7400.535645\n      7410.451660\n      7411.317383\n      19725074095\n      7411.317383\n    \n  \n\n\n\n\nThe data seems great, but one should always validate it visually and programmatically.\nHow about plotting it first?\nPlotting can be done in multiple ways, let’s start using the pandas way. 🐼\n\npandas/docs/plot\nMake plots of Series or DataFrame.\nIncludes options like x, y, kind (e.g. line, bar or density) and much more.\n\n\ndf['Close'].plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\nLet’s add some interactivity to this, and plot Low/High on top of that!\nSuper simple using plotly as the backend! 😎\n\ndf[['Close', 'High', 'Low']].plot(backend='plotly')\n\n\n                                                \n\n\n\n\nShow plotly chart (as it’s invisible in blog-mode)\n\n\n\nYou can also plot a candle-stick chart by using\nimport plotly.graphical_objects as go\n\nfig = go.Figure(data=[go.Candlestick(x=df.index,\n                open=df['Open'],\n                high=df['High'],\n                low=df['Low'],\n                close=df['Close'])])\nThe data does look nice, but we need to figure if there’s any issues in the data.\nGraphs might be missing some values that are not clearly visible as there’s so much data. Our final data validation should always be programatical."
  },
  {
    "objectID": "posts/2022-03-11-timeseries-pt-1/index.html#data-validation",
    "href": "posts/2022-03-11-timeseries-pt-1/index.html#data-validation",
    "title": "Decomposing & Working with Time Series (Time Series #1)",
    "section": "Data Validation",
    "text": "Data Validation\nValidating that data makes sense and that there’s no errors is very important when you’re building models. Having outliers, errors and other things can lead to some really weird behaviour.\nThere’s a few tools we can use:\n\npd.DataFrame.isna:\nDetect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values.\n💡The reverse, notna also exists.\n\n\ndf.isna().head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-01\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2020-01-02\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2020-01-03\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2020-01-04\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2020-01-05\n      False\n      False\n      False\n      False\n      False\n      False\n    \n  \n\n\n\n\nCan we we make this more readable?\nYes! Using .any(), or even ‘cheating’ using .sum()\n\ndf.isna().any()\n\nHigh         False\nLow          False\nOpen         False\nClose        False\nVolume       False\nAdj Close    False\ndtype: bool\n\n\n\ndf.isna().any().any() # 👀\n\nFalse\n\n\nLGTM ✅\nNext up: Validating that there’s no missing days\nThis can be done in multiple ways, but for now I choose to use .diff.\n\npd.DataFrame.diff: First discrete difference of element.\nBonus: Diff can also handle periods and axis arguments, period being how far to diff.\n\n\ndf.index.to_series().diff().dt.days.head()\n\nDate\n2020-01-01    NaN\n2020-01-02    1.0\n2020-01-03    1.0\n2020-01-04    1.0\n2020-01-05    1.0\nName: Date, dtype: float64\n\n\nOk, so we’ve got a series. Try to validate that no diff is greater than 1 day using a broadcasted/vectorized operation.\n\nhint: pandas automatically broadcast operations by operator overloading, e.g. >, + etc\n\nI think we can call quits on the validation part for now.\nLet’s move on to different ways we can format the data, a common format is LogReturn."
  },
  {
    "objectID": "posts/2022-03-11-timeseries-pt-1/index.html#transforming-data",
    "href": "posts/2022-03-11-timeseries-pt-1/index.html#transforming-data",
    "title": "Decomposing & Working with Time Series (Time Series #1)",
    "section": "Transforming Data",
    "text": "Transforming Data\nBecause data is very different moving in time we wish to normalize the data somehow. This can be done in multiple ways, some common ways are:\n\npd.DataFrame.diff which takes the difference between x_1 and x_2\n\n\nThe negative aspect of this is that the difference is still not scaled\n\n\npd.DataFrame.pct_change which validates the % difference\nLogReturn which is the logarithmic return between each time step (x_1, x_2, ..).\nApply a Scaler which scales the data somehow\n\n\nCan be MinMaxScaler which scales Min and Max to (0,1) or (-1,1)\nCan be a MeanScaler which scales the data to have a mean of 0.\n\n… and more.\nWe’ll start of with LogReturn which is common in forecasting of stocks.\n\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)\n\n\ndf['LogReturn'] = log_return(df['Close'])\ndf['LogReturn'].head()\n\nDate\n2020-01-01         NaN\n2020-01-02   -0.030273\n2020-01-03    0.050172\n2020-01-04    0.008915\n2020-01-05    0.000089\nName: LogReturn, dtype: float64\n\n\nBecause .diff takes the diff with the next element you’ll end up with a NaN, as such we wish to remove the first element.\n\ndf = df[1:]\ndf['Close'].head()\n\nDate\n2020-01-02    6985.470215\n2020-01-03    7344.884277\n2020-01-04    7410.656738\n2020-01-05    7411.317383\n2020-01-06    7769.219238\nName: Close, dtype: float64\n\n\n\ndf['LogReturn'].plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\nLooks nice.\nFor now we’ll leave it here and move on to looking at Correlation."
  },
  {
    "objectID": "posts/2022-03-11-timeseries-pt-1/index.html#data-analysis-correlation",
    "href": "posts/2022-03-11-timeseries-pt-1/index.html#data-analysis-correlation",
    "title": "Decomposing & Working with Time Series (Time Series #1)",
    "section": "Data Analysis: Correlation",
    "text": "Data Analysis: Correlation\n\ndf = pdr.get_data_yahoo(['BTC-USD', 'ETH-USD'], start=datetime(2020, 1, 1), end=datetime(2022, 1, 1))\ndf.head()\n\n\n\n\n\n  \n    \n      Attributes\n      Adj Close\n      Close\n      High\n      Low\n      Open\n      Volume\n    \n    \n      Symbols\n      BTC-USD\n      ETH-USD\n      BTC-USD\n      ETH-USD\n      BTC-USD\n      ETH-USD\n      BTC-USD\n      ETH-USD\n      BTC-USD\n      ETH-USD\n      BTC-USD\n      ETH-USD\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-01\n      7200.174316\n      130.802002\n      7200.174316\n      130.802002\n      7254.330566\n      132.835358\n      7174.944336\n      129.198288\n      7194.892090\n      129.630661\n      18565664997\n      7935230330\n    \n    \n      2020-01-02\n      6985.470215\n      127.410179\n      6985.470215\n      127.410179\n      7212.155273\n      130.820038\n      6935.270020\n      126.954910\n      7202.551270\n      130.820038\n      20802083465\n      8032709256\n    \n    \n      2020-01-03\n      7344.884277\n      134.171707\n      7344.884277\n      134.171707\n      7413.715332\n      134.554016\n      6914.996094\n      126.490021\n      6984.428711\n      127.411263\n      28111481032\n      10476845358\n    \n    \n      2020-01-04\n      7410.656738\n      135.069366\n      7410.656738\n      135.069366\n      7427.385742\n      136.052719\n      7309.514160\n      133.040558\n      7345.375488\n      134.168518\n      18444271275\n      7430904515\n    \n    \n      2020-01-05\n      7411.317383\n      136.276779\n      7411.317383\n      136.276779\n      7544.497070\n      139.410202\n      7400.535645\n      135.045624\n      7410.451660\n      135.072098\n      19725074095\n      7526675353\n    \n  \n\n\n\n\nAnd retrieving only the Close to have something to compare.\n\ndf = df['Close']\ndf.head()\n\n\n\n\n\n  \n    \n      Symbols\n      BTC-USD\n      ETH-USD\n    \n    \n      Date\n      \n      \n    \n  \n  \n    \n      2020-01-01\n      7200.174316\n      130.802002\n    \n    \n      2020-01-02\n      6985.470215\n      127.410179\n    \n    \n      2020-01-03\n      7344.884277\n      134.171707\n    \n    \n      2020-01-04\n      7410.656738\n      135.069366\n    \n    \n      2020-01-05\n      7411.317383\n      136.276779\n    \n  \n\n\n\n\nLet’s validate the correlation, e.g. how our values correlate to each other!\n\ndf.corr().style.background_gradient(cmap=\"Blues\")\n\n\n\n\n  \n    \n      Symbols\n      BTC-USD\n      ETH-USD\n    \n    \n      Symbols\n       \n       \n    \n  \n  \n    \n      BTC-USD\n      1.000000\n      0.903202\n    \n    \n      ETH-USD\n      0.903202\n      1.000000\n    \n  \n\n\n\nThe correlation looks pretty high… We can use seaborn to show even better data by adding the plots.\n\nimport seaborn as sns\n\nsns.pairplot(df)\n\n<seaborn.axisgrid.PairGrid at 0x7f36c360d310>\n\n\n\n\n\nIndeed, looks very correlated! 🤩\nWhat conclusions can we take from the above chart? Cryptocurrencies using daily data are indeed correlated, that is ETH prices depends on BTC and v.v.\nThis blog is getting close to its end, and as such we won’t go further in depth of this.\nFor the reader an excercise would be to predict the BTC price depending on ETH, which should be possible based on this correlation. We’ll go further into this later."
  },
  {
    "objectID": "posts/2022-03-11-timeseries-pt-1/index.html#data-analysis-decomposition",
    "href": "posts/2022-03-11-timeseries-pt-1/index.html#data-analysis-decomposition",
    "title": "Decomposing & Working with Time Series (Time Series #1)",
    "section": "Data Analysis: Decomposition",
    "text": "Data Analysis: Decomposition\nDecomposing Time Series means that we try to find seasonality, trends and other things. This can be done using statsmodels which is a very impressive library that originally was done in R but now exists in Python.\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nres = seasonal_decompose(df['BTC-USD'])\nres.plot()\n\n\n\n\n\n\n\nLooks nice and dandy! But we can modify this further by adding model and freq parameter to validate how it looks by decomposing either through model=multiplicative or additive, and updating the freq (period in the newest version) to decompose it based on different periods.\n\nres = seasonal_decompose(df['BTC-USD'], model='additive', period=365) # Try weekly or monthly decomposition.\nres.plot()\n\n\n\n\n\n\n\nWe’re now looking at our final data analysis step, Fast Fourier Transform, A.K.A. FFT!"
  },
  {
    "objectID": "posts/2022-03-11-timeseries-pt-1/index.html#data-analysis-fft",
    "href": "posts/2022-03-11-timeseries-pt-1/index.html#data-analysis-fft",
    "title": "Decomposing & Working with Time Series (Time Series #1)",
    "section": "Data Analysis: FFT",
    "text": "Data Analysis: FFT\nWe’ll also add an Fast Fourier Transform which can show which frequencies the data “resets” at.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfft = np.fft.rfft(df['BTC-USD'])\n\nplt.step(range(len(fft)), abs(fft))\n\nplt.xscale('log')\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n_ = plt.xlabel('Frequency (log scale)')\n\n\n\n\nThat’s it for this time. Make sure to view part 2 if you want to start predicting some data, and in part 3 we’ll do the final prediction where we’ll look at different forecast horizons!\n\nTo learn more about Time Series and how one can analyze them please view the other parts,\n\nPart One - Decomposing & Working with Time Series (theoretical)\nPart Two - Predicting Stock Prices (Time Series) using classical Machine Learning\nPart Three -Forecasting Cryptocurrency Prices (Time Series) using Deep Learning (PyTorch, Tensorflow/Keras & darts)"
  },
  {
    "objectID": "posts/2022-07-07-snapsvisor-generated/2022-07-07-snapsvisor-generator.html",
    "href": "posts/2022-07-07-snapsvisor-generated/2022-07-07-snapsvisor-generator.html",
    "title": "GPT2-snapsvisor - Generating Swedish Drinking Songs",
    "section": "",
    "text": "This midsummer my friends gave me the idea that I should generate Swedish Drinking Songs, or Snapsvisor, using Machine Learning and I thought it could be a lot of fun! 🍻\nTo achieve the best results I’d need access to GPT-3, or equivalent model, alas I don’t and as such I needed  to do some extra work! Fun work though! 🤓\nFirst some examples:\nWhy GPT-3?\nDrawbacks with GPT-2: - The performance is noticable worse because of the lower parameters and less data, sometimes called tokens, used to train the model.\nAs such the result is not amazing, but it’s capable and really funny - based on the premise that you know Swedish!\nIt’s available on the HuggingFace Hub with a inference widget and as a pre-trained model, which can generate your own Snapsvisor - N.B. it removes newlines."
  },
  {
    "objectID": "posts/2022-07-07-snapsvisor-generated/2022-07-07-snapsvisor-generator.html#tldr-too-long-didnt-read",
    "href": "posts/2022-07-07-snapsvisor-generated/2022-07-07-snapsvisor-generator.html#tldr-too-long-didnt-read",
    "title": "GPT2-snapsvisor - Generating Swedish Drinking Songs",
    "section": "TL;DR (Too Long; Didn’t Read)",
    "text": "TL;DR (Too Long; Didn’t Read)\nFor those that are not interested in details or writeups I thought I’d leave a small TL;DR\n\nFind a pre-trained Language Model (e.g. GPT-2) (link)\nScrape the web for text data (in my case “Snapsvisor”) using requests  (link) and beautifulsoup4 (link)\nFine-Tune the model using HuggingFace Trainer-api (link)\nGenerate text using HuggingFace pipelines (link)\n\nAnd in the end we can generate some Snapsvisor, like the following examples\n\nSnälla skålar till en nubbe, buguperrens till en skål.\nSnapsen får ta en nubbe,\nnär nubben inte tagit visdomsträngar,\nsen får ta nubben hellre.\nHinka lilla magen, ta nubben,\ndär ska det gå till en nubbe.\n\n\nNu tar vi en nubbe, \nDen ska hellre krypa och bränna upp. \nVi bränna, men inte mjölka oss, \noch bränner till bords\n\n\nNu tar vi en nubbe – slåss här i ett svep.\nByssa luderorgon, snapsar!\nNär den evigt låga solen tar oss en ljus natt,\ningen blir så dragen vid näsan\n\nAnd it’s available on the HuggingFace Hub with a inference widget, which can generate your own Snapsvisor - N.B. it removes newlines.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"lunde/gpt2-snapsvisor\")\nmodel = AutoModelForCausalLM.from_pretrained(\"lunde/gpt2-snapsvisor\")\nUntil next time!\n~Hampus Londögård"
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "",
    "text": "Not interested in reading the whole article and just wanna play around with the model(s)? Head over to londogard.com/ner.  P.S. The Flair model is available for simple installation through huggingface.co’s model hub"
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#named-entity-recognition-and-how-it-can-do-your-bidding",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#named-entity-recognition-and-how-it-can-do-your-bidding",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Named Entity Recognition and how it can do your bidding",
    "text": "Named Entity Recognition and how it can do your bidding\nAs the name suggests NER is the task to recognize entities in text. Entities can be a lot of different things such as the obvious Person but also Location, Organisation & Time. More entities exists and they can really become whatever your data allows (Brand, Medicine or Dosage? You got it!)\nPractical use-cases of NER 1. Automatic anonymization of data 2. Medical prescription 3. Automatically tag data - e.g. News tagged by Organisations, Persons & Locations included\n… & much more\nIn my case I’m simply aiming for the traditional NER model which categorize things like Location, Person & Organisation."
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#flairing-the-way-to-success",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#flairing-the-way-to-success",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Flair(ing) the way to success",
    "text": "Flair(ing) the way to success\nFlair is a SotA NLP library developed by Humboldt University of Berlin and friends. As mentioned its core contributors are from Humboldt University of Berlin and the whole idea is to provide contextual embeddings. Some of the things provided through Flair:\n\nFlair Embeddings\n(Easily) Stacked Embeddings\n\ne.g. combine Transformer, Flair & GloVe for your end-model\n\nEasy access to multiple embeddings\n\nGloVe, Transformer, ELMo & many more\n\nSimple training of high-performant NER (Token Classifier) Model and a Text Classifier model\n\nAs mentioned Flair retain SotA in multiple languages for NER, but they do the same for POS.\nThe Language Model\nIf you’re curious the simplest Flair embeddings are essentially a Language Model built on Dropout, LSTM & a Linear Layer. Pretty simple.\nThe Token Classifier (NER/POS)\nIt’s based on a small LSTM-network with a CRF on top. The LSTM exists to create features for the CRF to learn and tag from. This is a very common approach which yields high accuracy. If you’re aware of what features you wish to use a pure CRF can be very strong, Stanford NLP library was actually for very long based on a CRF and had SotA, but the manual feature engineering can be expensive & hard.\nThe Text Classifier Simply a linear layer on top of the embeddings.\nMore Models\nFlair actually supports two other tasks, Text Regression & Similarity but I won’t go in to those.\nMore about how I trained my NER will come a bit further down. To read more about Flair and how they work please check out their GitHub which also links to the papers."
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#swedish-data",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#swedish-data",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Swedish data",
    "text": "Swedish data\nFirst of all I had to go find some data and I found crème de la crème in SUC 3.0, because we really do sentence by sentence training in NER it’s not the end of the world that the ‘free’ variant that doesn’t require a research licence is scrambled. Unscrambled data would lead to a better model but it’s still doable.\nBut as Jeremy Howard proposes, start with small and simple data then expand into your full task. SUC 3.0 is pretty large and slow to train. With some fast googling I found a saviour, klintan. Klintan has created a open Swedish NER dataset based on Webbnyheter 2020 from Språkbanken, it’s semi-manually annotated. This means that he first based it on Gazetters, essentially dataset(s) of entities, and then manually reviewed the data with two different native Swedish Speakers. More people have later added some improvements on top of that, find the full dataset here, but please note that it’s much smaller than SUC 3.0.\nAfter finding this dataset I read more into Flair and I found out that they actually provide this dataset through their API and in this dataset we have 4 categories PER, ORG, LOC and MISC.\nWith these two datasets in mind I went ahead to train."
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#training-the-flair",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#training-the-flair",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Training the flair",
    "text": "Training the flair\nFirst let me say the documentation is actually pretty good! First part is to set up the Corpus.\n\nSetting up the Corpus / Dataset\nThe built-in klintan/ner-swedish-corpus\n# 1. get the corpus\ncorpus: Corpus = NER_SWEDISH()\nprint(corpus)\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\nprint(tag_dictionary)\nCustom dataset (SUC 3.0, in my case scrambled)\nRemember to convert the SUC tags into a IOB format before training. Emil Stenström has kindly created a simple Python-script for this available through github.com/EmilStenstrom/suc_to_iob. First transform the data and later you can run the following code\ncolumns = {0: 'text', 1: 'ner'}\n\n# this is the folder in which train, test and dev files reside\ndata_folder = 'path/to/data/suc'\n\n# init a corpus using column format, data folder and the names of the train, dev and test files\ncorpus: Corpus = ColumnCorpus(data_folder, columns, train_file='train.txt', test_file='test.txt', dev_file='dev.txt')\n\n# 2. what tag do we want to predict?\ntag_type = 'ner'\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\nprint(tag_dictionary)\nWith this in mind we’re ready to set up our model for training.\n\n\nModel Setup\nOur model will build on FlairEmbeddings (e.g. contextual embeddings) and BytePairEmbeddings which are a bit like classic WordEmbeddings but done on BPE-tokenized text. This is a really interesting approach which achieves similar performance as fastText using ~ 0.2 % of the total size (11mb vs 6gb).\nThe model itself will use a LSTM with a hidden size of 256 and a CRF classifier on top.\n# 4. initialize embeddings\nembedding_types = [\n    # WordEmbeddings('sv'), # uncomment to add WordEmb\n    BytePairEmbeddings('sv'),\n    FlairEmbeddings(\"sv-forward\"),\n    FlairEmbeddings(\"sv-backward\")\n]\n\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# 5. initialize sequence tagger\ntagger: SequenceTagger = SequenceTagger(hidden_size=256,\n                                        embeddings=embeddings,\n                                        tag_dictionary=tag_dictionary,\n                                        tag_type=tag_type,\n                                        use_crf=True)\n\n\nTraining the model\nBecause I run through google colab and the machine can be terminated any second I run using checkpoint=True which means you can continue training where you left off. My models are saved to my Google Drive, real handy! >Pro-tip: use checkpoint=True in combination with Google Drive on your Google Colab.\n# 7. start training\ntrainer.train('drive/MyDrive/path/to/model/save/',\n                learning_rate=0.1,\n                # set chunk size to lower memory requirements\n                #mini_batch_chunk_size=16,\n                mini_batch_size=32,\n                checkpoint=True,\n                embeddings_storage_mode='none', # only required for SUC 3.0 which grows too large\n                #batch_growth_annealing=True,\n                #anneal_with_restarts=True,\n                max_epochs=150)\n\n\nLoading model from checkpoint\ntrainer = ModelTrainer.load_checkpoint('drive/MyDrive/path/to/model/save/checkpoint.pt', corpus)\nAnd that’s it!"
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#result",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#result",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Result",
    "text": "Result\nFor me the results looks really good and close to what I expected, I had hoped that Flair would achieve at least 0.88+ F1 but 0.855 isn’t too bad. The size, speed and simplicity of Flair makes it a great contender!\n\n\n\nDataset\nSize\nAvg F1\n\n\n\n\nklintan/swedish-ner-corpus\n320MB\n~0.89\n\n\nSUC 3.0 (PER, LOC & ORG)\n320MB\n~0.89\n\n\nSUC 3.0 (PER, LOC, ORG, TME, MSR, …)\n320MB\n0.855\n\n\nSUC 3.0 (PER, LOC, ORG, TME, MSR, …) Quantized\n80MB\n0.853\n\n\nSUC 3.0 (PER, LOC, ORG, TME, MSR, …) w/ ALBERT\n50MB\n0.85 (via KTH)\n\n\nSUC 3.0 (PER, LOC, ORG, TME, MSR, …) w/ BERT (KungBib)\n480MB\n0.928\n\n\nSUC 3.0 (PER, LOC, ORG, TME, MSR, …) w/ BERT Quantized\n120MB\n0.928\n\n\n\nI believe it’s important to note that Quantized models are also much faster running ~ 4 times faster (avg 360ms went to 80ms on a CPU for flair).\nQuantization updates the f32 into int8 which allows the model to more efficiently utilize CPU and the ONNX-runtime also makes the whole model better at using CPU-instructions."
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#deploying-on-streamlit.iosharing",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#deploying-on-streamlit.iosharing",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Deploying on streamlit.io/sharing",
    "text": "Deploying on streamlit.io/sharing\nAnd for the final part! :tada: First you need a new public repository on GitHub with the streamlit & model code. This requires to set up a requirements.txt with all necessary dependencies.\nThen you need to figure out how you’ll host your model if it’s too large. I found GitHub LFS to work out decently, but the cap was pretty small (1GB / Month) and I broke the limit on my 3rd model. I went ahead and registered on Backblaze which has great reviews, but I think the best solution in my shoes would be to host it through HuggingFace Model storage (free if public!). edit: I actually ended up storing the flair model on huggingface.co/londogard/flair-swe-ner 🤗.\nSetting up the script itself was quite easy for Flair.\n# load tagger for POS and\n@st.cache(allow_output_mutation=True)\ndef load_model():\n    tagger = SequenceTagger.load('best-model-large-data.pt')\n    return tagger\n\n@st.cache(allow_output_mutation=True, hash_funcs={SequenceTagger: lambda  _: None})\ndef predict(model, text):\n    manual_sentence = Sentence(manual_user_input)\n    model.predict(manual_sentence)\n    return render_ner_html(manual_sentence, wrap_page=False)\n\ntagger = load_model()\n\nst.title(\"Swedish Named Entity Recognition (NER) tagger\")\nst.subheader(\"Created by [Londogard](https://londogard.com) (Hampus Londögård)\")\nst.title(\"Please type something in the box below\")\nmanual_user_input = st.text_area(\"\")\n\nif len(manual_user_input) > 0:\n    sentence = predict(tagger, manual_user_input)\n    st.success(\"Below is your tagged string.\")\n    st.write(sentence, unsafe_allow_html=True)\nIt’s important to note how I’ve placed the caching solution. I both cache the model loading & predictions to keep it as speedy as possible.\nThe allow_output_mutation option skips hashing the output to validate that the cache is correct, we don’t care if output has been modified really.\nThe hash_funcs={SequenceTagger: lambda  _: None} is incredibly important.\nThe flair model are pretty slow to hash, especially if quantized. It’s possible to use id which is a unique ID for the python object that lasts the full lifetime, but because I know that the model wont change I simply use lambda _: None to not do any lookup at all.\nIf the model input would change in-between using id is the best approach. Note that neither of this approaches are any good if you wanna compare an object to another (e.g. two string inputs), there we should just keep standard hashing."
  },
  {
    "objectID": "posts/2021-03-29-swedish-named-entity-recognition.html#outro",
    "href": "posts/2021-03-29-swedish-named-entity-recognition.html#outro",
    "title": "Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)",
    "section": "Outro",
    "text": "Outro\nI trained Flair embeddings which is a much simpler approach than Transformers and achieved almost SotA while having a much smaller & simpler model (~2/3rd of the size). But in the end I was very impressed by how well quantization applies for CPU utilization so I also applied the same approach for BERT-ner by KB, where I even did a ONNX Quantization which has been shown to be even more effective than PyTorch own quantization, but then again it requires the ONNX runtime.\nBoth models are available on the same device / streamlit configuration, find them on londogard.com/ner.\nThe flair model is available through HuggingFace 🤗 through the following: londogard (huggingface.co).\nThanks for this time, Hampus Londögård @ Londogard"
  },
  {
    "objectID": "posts/2020-05-13-faq-search-covid-1.html",
    "href": "posts/2020-05-13-faq-search-covid-1.html",
    "title": "A simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency",
    "section": "",
    "text": "CoViD-19 Swedish QA\nI decided to scratch a small itch I’ve had for a while now - creating a search engine using an unsupervised approach. The final product, or the first iteration rather, ended up pretty good and I wanted to share what I’ve done so far. \n\nIntroduction to the problem and requirements\nAn unsupervised approach where we never edit the data nor supply any manually annotated data? Every Data Scientist dream I suppose. There’s a reason as of why supervised approaches generally result in better performance but there is some light at the end of the tunnel for unsupervised approaches too.\nLet’s begin with my own requirements, which are mainly created to only keep the fun problem-solving left.\n\nThe end-product must be unsupervised\n\nNo manually annotated data\nNo heuristic applied (at least in first iteration)\n\nIt should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end)\nMust be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!)\n\nWith this in mind I set out to build my own FAQ search engine.\nWhat is required to answer questions using a FAQ? We need to find the most relevant Q/A to the question posed.\nHow do we do this? There is numerous types of ways to do this unsupervised. I’ll account for a few here:\n\nLatent Dirichlet Allocation (LDA) which is a way to find topics through clever statistical analysis (basically soft clusters of documents)\nEmbedding and Cosine Similarity, find the distance between the two arrays of numbers in the embedded space. One can also apply Euclidean Distance which isn’t especially good because of Curse of Dimensionality. Other possible approaches includes Word Mover Distance.\nSimple word counting and Bag of Words\n\n\n\nTools Chosen\nAfter a little research I found a few tools which fit my need.\nfastText\nfastText that came out of Facebook AI Research (FAIR) and this paper. It’s a type of Word Embeddings where also subwords are embedded through ngrams of characters, this means that we are able to embedd words that are out of vocabulary, which can be the reason because of either misspelling or just a missing word. On their homepage they have a plethora of models including a Swedish one that has been derived from Wikipedia, pretty awesome!\nSmooth Inverse Frequency\nSmooth Inverse Frequency (SIF) is an algorithm to embed sentences which was proposed in \"A Simple but Tough-To-Beat Baseline for Sentence Embeddings\" in 2017. In its essence they propose to embed the sentence using a weighted average and thereafter modify them a bit using PCA/SVD.\nFolkhälsomyndigheten FAQ\nFinally I need the FAQ to use, in my case it’s Covid-19 FAQ from Folkhälsomyndigheten. It was parsed into pandas dataframes using requests & BeautifulSoup4 (bs4).\n\n\nFinal Result\nSo after all this was figured out I sat down an afternoon and cooked some code together, the result ended up more impressive than I had imagined. The questions posed are being responded with pretty good results. I’m especially impressed by question about astma, son and regler. Here’s a few of them:\n> Hur sjuk blir jag?\n\nHur sjuk blir man av covid-19? - 0.98\nHur länge är man sjuk av covid-19? - 0.97\nHur lång är inkubationstiden? - 0.81\n> Hur vet jag om det är astma?\n\nHur vet jag om mina symtom beror på pollenallergi eller på covid-19? - 0.63\nHur sjuk blir man av covid-19? - 0.53\nHur länge är man sjuk av covid-19? - 0.53\n> Hur förklarar jag corona för min son?\n\nHur pratar man med barn om det nya coronaviruset? - 0.58\nHur lång är inkubationstiden? - 0.53\nHur sjuk blir man av covid-19? - 0.49\n> Hur minskar vi spridningen i sverige?\n\nHur gör ni för att mäta förekomsten av covid-19 i samhället? - 0.65\nHur övervakar ni på Folkhälsomyndigheten spridningen av covid-19? - 0.57\nHur stor är dödligheten till följd av covid-19? - 0.56\n> Vad för regler finns?\n\nVad gäller för olika verksamheter? - 0.76\nVad gäller för handeln? - 0.75\nVad är covid-19? - 0.71\nOne can directly note the correlation of the beginning. It seems like the first word has a high correlation with the most similar question. Weird. Removing stop words could probably improve this, but that’d be for the second implementation.\n\n\nFurther improvements for iteration 2, 3 and beyond!\nPre-processing\nAs mentioned right above we can apply some basic pre-processing such as removing stop words. In reality this should be handled by SIF but looking at our similarity scores there’s a 1-1 relation between the first word of the sentence.\nOther improvements worth trying out is lemmatizing or stemming the words (\"cutting them to the root\" in simple terms) and further using a better tokenization is worth trying out (currently splitting on whitespace). spaCy offers a strong tokenizer, but I haven’t tried it out for Swedish yet. Once again fastText should handle this but it’s worth trying out if it improves or keep the result at the same level.\nDifferent Embedding Techniques\nThere exist a certain Sentence Embedding that’s basically made for this task - MULE (Multimodal Universal Language Embeddings). MULE is even multilingual but unfortunately they’re not able to embed Swedish so we’d require a translation from Swedish to one of the 16 languages supported by MULE. This means that it is out of the question because of my requirements, but could still be fun to check out.\nOther embeddings such as FLAIR (by Zalando), BERT (using BERT-as-a-service) or even training my own embeddings (perhaps using StarSpace) could prove interesting also.\nCompletely other technique\nI mentioned first of all LDA, and I think LDA could be interesting. Most often LDA is applied to larger documents but as with everything it is never wrong to try out and verify the results.\nSupervised approaches would certainly be able to show us some good performance but that requires annotating data in one way or another which is a boring task - but very important. Perhaps I’ll revisit and label some data, with todays Transfer Learning we can achieve higher accuracy with less data using other pre-trained Language Models such as BERT or Multifit (from Ulmfit).\n\n\nEnding words\nThis was a really fun task and I’m happy that I tried it out. I’m sure I’ll revisit and improve it further by applying some of the possible improvements. Further I think I might actually try to do this for all FAQs available by our authorities to create a \"Multi FAQ\" which could prove pretty cool. With more data the results should also be better.\nAnd as an ending note my model ended up using 2.5-3 GB of memory during run-time which means it’s possible to run on my Raspberry Pi 4! Further reduction of size can be done by removing the most uncommon words in the vocabulary (vocab is 2M words, which is very large). I applied a dimension reduction using the built in version of fastText (ending up using d=100 and still achieving good search results).\nThe implementation is available at my GitHub (Londogard) or directly launched in Google Colaboratory.\nThanks for this time, I’ll be back with more!\nHampus Londögård"
  },
  {
    "objectID": "posts/2020-02-10-gradle-github-packages.html",
    "href": "posts/2020-02-10-gradle-github-packages.html",
    "title": "Gradle, JVM and GitHub Packages",
    "section": "",
    "text": "Initial comment this is mainly done as a reminder to myself.\nSo about 6 months ago GitHub launched a new exciting service; GitHub Package Registry. This service lets you as a GitHub-user upload your Open Source code for free on GitHubs registry supporting a wide array of languagues and build systems - JavaScript (npm), Java/JVM-languages (Maven/Gradle), Ruby (RubyGems), .NET (NuGet), and Docker images. Perhaps more have been added since I last verified. \nIn this post I’ll try to keep to the point and give clear easy instruction that you’ll be able to bookmark and go back to whenever you need to set this up.\nI’ll give the instruction directly beneath with some comments afterwards for the one who’d like to read some extra.\n\n\n\nbuild.gradle.kts\nplugins {\n    `maven-publish` // Add this\n    kotlin(\\\"jvm\\\") version \\\"1.3.60\\\"\n}\nbuild.gradle\nplugins {\n    id(\\\"maven-publish\\\")\n}\nbuild.gradle (using the old apply way of things)\napply plugin: 'maven-publish'\n\n\n\nbuild.gradle.kts\npublishing {\n    repositories {\n        maven {\n            name = \\\"GitHubPackages\\\"\n            url = uri(\\\"https://maven.pkg.github.com/OWNER/REPO\\\")\n            credentials {\n                username = project.findProperty(\\\"gpr.user\\\") as String? ?: System.getenv(\\\"GITHUB_ACTOR\\\")\n                password = project.findProperty(\\\"gpr.key\\\") as String? ?: System.getenv(\\\"GITHUB_TOKEN\\\")\n            }\n        }\n    }\n    publications {\n        register<MavenPublication>(\\\"gpr\\\"){\n            from(components[\\\"java\\\"])\n        }\n    }\n}\nbuild.gradle\npublishing {\n    repositories {\n        maven {\n            name = \\\"GitHubPackages\\\"\n            url = uri(\\\"https://maven.pkg.github.com/OWNER/REPOSITORY\\\")\n            credentials {\n                username = project.findProperty(\\\"gpr.user\\\") ?: System.getenv(\\\"GITHUB_ACTOR\\\")\n                password = project.findProperty(\\\"gpr.key\\\") ?: System.getenv(\\\"GITHUB_TOKEN\\\")\n            }\n        }\n    }\n    publications {\n        gpr(MavenPublication) {\n            from(components.java)\n        }\n    }\n}\n\n\n\nTo simplify our lifes further;\n\nJitPack is already automated and tracking your repository automatically adding the new releases ones a release is created.\nGitHub is not automated and we need to upload our assets\n\nAutomating GitHub packages upload through release & GitHub Actions\nWe’ll use GitHub Actions to create a workflow where once a release passes stage ‘published’ the assets will be uploaded to the repository/artifactory of GitHub Packages. This integration is really awesome as once we’ve set it up we only need to press \"Create Release\" on the GitHub page to deploy our library to both GitHub Packages & JitPack!\nCreate the directory .github/workflows in your root-folder of the project if it doesn’t exist yet. Add the following file:\nname: Release & Publish Build\n\non:\n  release:\n    types: [published]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v1\n\n      - name: Set up JDK 1.8\n        uses: actions/setup-java@v1\n        with:\n          java-version: 1.8\n\n      - name: Clean Build\n        run: ./gradlew clean build\n\n      - name: Publish Build\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        run: ./gradlew publish\nThe secrets.GITHUB_TOKEN is automatically supplied by GitHub itself during the run of the GitHub Actions-script.\nPretty awesome right? Go build your libraries and deploy!"
  },
  {
    "objectID": "posts/2020-09-03-til-fastutil-primitive-structures.html",
    "href": "posts/2020-09-03-til-fastutil-primitive-structures.html",
    "title": "TIL: fastutil - fast & compact type-speciic collections for JVM (no autobox!)",
    "section": "",
    "text": "fastutil - how to optimize your collections by primitives\n\nfastutil extends the Java™ Collections Framework by providing type-specific maps, sets, lists and queues with a small memory footprint and fast access and insertion \n\nHomepage of fastutil\n\nWhat\nSo what does the quote above actually mean?\nFirst we need to dive into, what is a Java Collection, and why are they \"bad\" for performance and memory requirements?\nJava Collections (Collection<E>) only works with objects, meaning that if we have a List<E> which we populate with int it’ll actually \"autobox\" the int into a Integer, i.e. the class, rather than the primitive type.\nWhat does this mean for you as a user?\n\nIt’s done through \"autoboxing\" which means automatic casting to the Integer-type, so nothing required for you\nIt allocates more memory than the primitive int.\n\nSo how do you create an effective List that contains primitives, such as int, boolean and float? You can’t.\nWhat you can do is to create an array, int[], which will contain the actual primitives, no autoboxing applied.\nBut what if you want to have the methods from List, such as find and \"auto-resizing\"? Then you’ll have to research and find a library, fastutil to the rescue!\n\n\nHow\nfastutil implements their own versions of List, HashMap and so on which actually use the raw primitives, thereby increasing throughput while lowering memory used (as we’re not allocating as many objects anymore, when using primitives).\nThese types of libraries are only required once you hit an enormous amount of data or very strict requirement.\n\nInstallation\nUsing gradle\nimplementation: 'it.unimi.dsi:fastutil:8.4.1' (latest version as of Aug 2020, mvnrepository)\n\n\nUsage\nDoubleToDoubleMap\nval d2dMap: Double2DoubleMap = Double2DoubleOpenHashMap().apply {\n    put(2.0, 5.5)\n    put(3.0, 6.6)\n}\nassertEquals(5.5, d2dMap.get(2.0))\nThis map is not only less memory-hungry (because using double rather than Double) but is also faster with insertions & get, than the Java Collections counterpart.\n\n\n\nWhy\nLess space used & faster - it is as simple as that!\n\nNo \"AutoBoxing\"\nNo Object allocations for primitives\n\nSide-note Something I noticed while working on my Language Model in Kotlin, with some strict requirements and a lot of data, was that even when using fastutil I wasn’t gaining that much as I was mainly using views of my Lists, further optimizing memory. Views are what the name implies, a view of the List. It never creates a copy but just the indices and make use of the original structure. Using immutable data this is very effective, but if you’d been using mutable data it could prove dangerous as someone can change the structure and data of your view (even if your view is immutable the underlying List might not be).\n\n\nAlternatives\nGoldman Sachs Collection - now Eclipse Collections - Probably the best alternative, in my opinion. HPPC - Carrot Search Labs Trove4j - Not as active as other alternatives, but who cares when it’s performant and \"done\"?\nFind a 2015 benchmark of the libraries here At least both fastutil& Eclipse Collections are updated for Java 8 streams!\n-Hampus Londögård"
  },
  {
    "objectID": "posts/2020-03-31-email-generator-kotlin-tipsrundan.html",
    "href": "posts/2020-03-31-email-generator-kotlin-tipsrundan.html",
    "title": "How I created a email generator in Kotlin (for Afry Tipsrundan)",
    "section": "",
    "text": "At AFRY IT South I’m co-responsible with Hassan Ftouni at driving the competence. One of my initiatives that we both now drive is to have a biweekly, every second week for all you picky readers out there, newsletter called \"Tipsrundan\". \nTipsrundan has lately gathered some fuss around and Afry IT West now wants to join in. This means new challenges to somehow build an email together with more people, collaborating on what to have and what to keep regional. Let me say that this is a fun challenge!\nIn this post I’ll go through how I built our completely new \"Email Generator\" in Kotlin that I built a Sunday afternoon. This includes a few things such as\n\nLearning how CSS works in emails (in comparison to browsers)\nKotlin\n\nLet me start at how Tipsrundan has evolved since we initiated it in October (crazy how time flies).\n\n\nWhen we sent the first Tipsrundan, called TL;DR back then, I used a \"email templating language\" called MJML and a pre-built template found on their homepage.\nWith this we got a responsive email using their \"homemade\" templating language. I enjoyed it at the same time as I hated it, there was way to much manual labour copying the sections and inserting my own code, the indentation in the web-editor wasn’t great and so on. I bet it’s a great tool but it didn’t cut it for me, after two or three issues and a ton of research I found a new tool I liked, with a good free variant, called Stripo. Stripo is a really good tool which has excellent support with its drag n’ drop editor where you can save modules and much more. We got a good looking & responsive email that worked out great, everything good right?\nIt was really good until I realized we had to share the template with new people and Stripo requires premium for this (can’t blame them, they need their cut) which honestly I was to lazy to fix through management.\nWith this knowledge I set out to create a tool which we could use internally that is simple and keeps simple. Forward comes a solution I built over a Sunday afternoon where we generate emails from JSON.\n\n\n\nI had some pretty simple requirement: 1. JSON or yaml as the filetype which we’d generate Tipsrundan-email from. 2. Have different sections and easily extendable 3. Decent looking & responsive (i.e. work on a phone and desktop)\nBy having these requirements I knew that it’ll be easy to sync over git or whatever tool we need and that we can potentially create themed Tipsrundan editions in the future.\nI also knew that I wanted to do this in Kotlin, mainly because I really enjoy coding in Kotlin.\n\n\nThe first step was to decide what format to use, or at least begin with. Both yaml and JSON was considered and in the end JSON felt like the best fit.\n{\n  \\\"title\\\": string,\n  \\\"issue\\\": number,\n  \\\"regional\\\": [item],\n  .. more categories\n}\nwhere item is\n{\n  \\\"title\\\": string,\n  \\\"description\\\": string,\n  \\\"url\\\": string\n}\nPretty straight-forward, as I said I prefer to keep it simple.\n\n\n\nNow that we have a definition of the data we need to read it, this is really a solved problem in most languages through some kind of library. In my case I choose the serialization library provided by Kotlin in the kotlinx-library. As a FYI this library can serialize using CBOR (Concise Binary Object Representation) and other formats. The name of the library is kotlinx-serialization and can be found here. It’s easiest installed through gradle (using the Kotlin DSL):\nplugins {\n  kotlin(\\\"plugin.serialization\\\") version \\\"1.3.70\\\" // same version as kotlin\n}\n\ndependencies {\n  .. other dependencies\n  implementation(\\\"org.jetbrains.kotlinx:kotlinx-serialization-runtime:0.20.0\\\") // Requires jcenter() as a repository\n}\nkotlinx-serialization is actually cross-platform compatible meaning that it exists for Kotlin targeting JVM, Native & JS (yes we can target all these platforms through Kotlin!).\nOnce installed it’s pretty easy to serialize & deserialize, like any other library really. Create the data classes, which is the equivalent of a case class in Scala.\ndata class Item(val title: String, val description: String, val url: String)\nCurrently kotlinx-serialization can do the serialization through two different methods, either add an annotation to the class that we’ll use Reflection - this does not work for native. Or we mark the data class as Serializable, the latter being preferred as it’s truly cross-platform and is more performant. If anyone is wondering a data class is basically a class that provides setters, getters, equality, toString and more! It’s really awesome. Adding the annotation we end up with the following:\n@Serializable\ndata class Item(val title: String, val description: String, val url: String)\n\n\n\nWe all probably know about html-templating that’s available in most languages, I decided against that and went for a DSL. Kotlin is the language for DSL (Domain Specific Language), for good and bad. Through yet another kotlinx library we got kotlinx-html which provides this DSL.\nIt looks something like this\nfun BODY.createFooter() = footer {\n    hr { }\n    section {\n        p {\n            b { +\\\"Thank you for this time see you in two weeks\\\" }\n            br { }\n            +\\\"Hampus & Hassan\\\"\n        }\n    }\n}\nBy using a DSL we get types (as you can see on the BODY) and other bonuses. Although this DSL is pretty verbose it works pretty good. In the end using a DSL or html-template engine does not matter that much in my opinion. By the way, the way this function is typed is called a extension function in Kotlin and is one of my favorite tools. It means that we extend the class, BODY with a new method which is usable on a object of the class. Cool right?\nLet’s move on to the styling and how CSS can be annoying.\n\n\n\nThere was some important parts going into this, we want the email to look at least decent and also be responsive so that it’s viewable on both a phone and computer.\nCSS and emails are not as simple as with a webpage I learned rather fast. I had great issues actually getting the HTML to look good in gmail/outlook. In the end I found this awesome post from Litmus which is one of the leading Email Marketing providers. I learned that\n\nExternal CSS is a no-go for emails (a lot of the providers turned it off because of security concerns)\nEmbedded CSS (using style-tag in the header) works on most places today (not true a few years ago)\nInline CSS is the best\n\nBecause I want to keep it simple I went with the second approach, this mean that I can keep the code a bit cleaner and not write as many wrappers for the styled elements.\nSo knowing how I should implement my styling I needed to find a good style, in the end I remembered an old Reddit-post where I found \"MVP.css\" which is a small CSS that gives cards, buttons and more. Really brilliant in my opinion, made by Andy Brewer and can be found here. I’ve personally tweaked it a bit to keep the email a bit more compact and informative as this is really made for webpages, but the essentials are the same.\n\n\n\nCombining all this into a few files in a git repo we can now generate emails from a JSON easily and have multiple categories.\n\nThe JSON is used as a data structure\nKotlin used as language\nkotlinx-serialization used as a JSON deserializer\nkotlinx-html used to build the HTML directly in Kotlin with types\nEmbedded CSS used as it’s widely usable by today in email clients\n\nThe repository can be found here.\nI hope this was somewhat interesting & something learned. If you’ve any comments please reach out to me through any of the available channels!\nHampus Londögård"
  },
  {
    "objectID": "posts/2021-02-18-transformers-explained/transformers-explained.html",
    "href": "posts/2021-02-18-transformers-explained/transformers-explained.html",
    "title": "Transformers From Scratch",
    "section": "",
    "text": "The video from the workshop (in Swedish) can be found on YouTube.\nToday I’m taking huge inspiration from Peter Bloem (Vrije Universiteit Amsterdam) which is a great blog post & video lecture. It’s essentially a 80-90 % copy of the blog, with minor modifications and reworked code into TODOs to make it more like a workshop using the Jupyter format (fitting nicely with Google Colab).\nAs demonstrations in the end I integrate PyTorch-code with fast.ai framework.\nThese demos are done to have a foundation for you to play around with as the models are not deep enough and does not train enough to really show-case the power of Transformers.\nI want to note that the original blog by Peter Bloems is a work of art and does a better job at visualizing graphics in combination with text.\nFinally this blog was done to support a ‘Competence Night’ in Machine Learning that I did @ AFRY.\n\n\n\nTransformers are a very exciting family of machine learning architectures. Many good tutorials exist but in the last few years, transformers have mostly become simpler, so that it is now much more straightforward to explain how modern architectures work. This post (read: Peter Bloems blog) is an attempt to explain directly how modern transformers work, and why, without some of the historical baggage.\n\nIt’s assumed a basic understanding of neural networks and backpropagation, for those that are not sure you can either brush up your knowledge in this video and learn how they’re used today here.\nFurther we’ll use PyTorch, with some fast.ai for the demos in appendix, to implement the full self attention & transformer module(s).\n\n\n\nThe fundamental operation of any transformer architecture is the self-attention operation.\nSelf-attention is a sequence-to-sequence operation, that is we input a sequence and we have a sequence returned.\nLet’s say we’ve got x1..xn as input and y1..yn as output where each vector is dimensioned k.\nTo produce output vector 𝐲i, the self attention operation simply takes a weighted average over all the input vectors\n\\(y_i = \\sum_{j=0}^{j=k}{w_{ij}x_j}\\)\nWhere the weights summed over \\(j=0..k\\) is equal to \\(1\\). \\(w_{ij}\\) is not a parameter but is derived from a function over \\(x_i\\) and \\(x_j\\).\nThe simplest being the dot-product.\n\\(w_{ij}^{'} = x_i^Tx_j\\)\nWhere \\(x_i\\) is at the same position as \\(y_i\\) and \\(x_j\\) traverses through all k values. For \\(x_{i+1}\\) we get a completely different output!\nBecause the dot product has no bounds we apply softmax to the result, so that the whole sequence sum to \\(1\\).\n\\(w_{ij} = \\frac{\\text{exp } w_{ij}^{'}}{\\sum_{j} \\text{exp } w_{ij}^{'}}\\)\nAnd that’s really it.\n\n\n\nattention\n\n\n\nA visual illustration of basic self-attention. Note that the softmax operation over the weights is not illustrated.\n\nObviously we need to apply a few more things to create a Transformer, but we’ll get to that.\n\n\n\nAs shown previously we’ll use Movie Recommender system to show why the dot-product works.\nSay you created your manually annotated data\n\n\n\nimage\n\n\nAs you see, negative + negative = positive, positive + positive = positive.\nThe magnitude of the score increases the final outcome.\nSo combining these two vectors together will work out real nice!\nThis might not be so practical in reality and as such we make the movie and user features parameters of the model. Then we ask the user for for a small number of movies they like and optimize the features so that their dot product matches the known likes.\nEven when not manually giving any features meaningful data is extracted.\nEven though we don’t tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content.\n\n\n\nimage\n\n\nThis is in essence the same as self-attention.\nUsing word embeddings on a sentence as\n\\(v_{the}, v_{cat}, v_{walk}\\)\nand feed it into self attention we get the output-vectors y,\n\\(y_{the}, y_{cat}, y_{walk}\\)\nwhere y-vectors are the weighted sum over all embedding vectors in the first sequence, weighted by their (normalized) dot-product with \\(v_{cat}\\).\nE.g. \\(y_{the}\\) = \\(v_{the} * v_{the} + v_{the} * v_{cat} + v_{the} * v_{walk}\\).\nBecause v is learned this will continually be updated to work better. While updating how v is created most likely walk and cat will have a high dot-product as those are correlated.\nThis is the basic intuition. Please note:\n\nNo parameters (yet) so the upstream mechanism creating the embeddings fully drives the self-attention by learning representations with particular dot-products\nSelf-attention see the input as a bag, i.e. not a continuous input as it is, e.g. it is permutation equivariant.\n\n\n\n\n\nWhat I cannot create, I do not understand - Feynman\n\nThe first thing we need to implement is matrix multiplications, which will be done through torch as native python loops are too slow.\nInput: \\(t\\) vectors, dimension \\(k\\) and \\(b\\) mini-batches (tensor: \\(b, t, k\\))\nWe’ll represent the input, a sequence of t vectors of dimension k as a t by k matrix 𝐗. Including a minibatch dimension b, gives us an input tensor of size (b,t,k).\nThe set of all raw dot products w′ij forms a matrix, which we can compute simply by multiplying 𝐗 by its transpose:\nLet’s code!\n\nclass TODO(Exception):\n    \"\"\"Raised when there is something TODO\"\"\"\n    pass\n\n\nimport torch\nimport torch.nn.functional as F\n\n# assume we have some tensor x with size (b=1, t=3, k=3)\nx = torch.array([[1.,2.,3.,\n                 1.,2.,3.,\n                 1.,2.,3.]])\n\n\n# batch matrix multiplication\nraw_weights = raise TODO(\"Use torch batch-matrix-multiply (bmm) to do x*x^T. Remember there's 3 dimensions!\")\n\n# Apply row-wise Softmax\nweights = raise TODO(\"Use functional softmax to apply softmax row-wise (along which dim?)\")\n\n# creating y\ny = torch.bmm(weights, x)\n\nAnd that’s it. We have created basic self-attention which is the basis of Transformers (state-of-the-art).\n\n\n\nTo implement Transformers fully we need to add a few tricks.\n\n\nEach input-vector \\(x_i\\) is used 3 times\n\nCreating its own weight for \\(y_i\\)\nCreating others weight for \\(y\\) (\\(y_j\\))\nUsed as a part of the weighted sum for each output \\(y\\)\n\nThis is often called the query, the key, and the value (explained later).\nWe update the network to instead use three weight-matrices, one for each task, making it more controllable. Adding \\(W_k, W_q, W_v\\) of size \\(k*k\\), we’ve got\n\\(q_i = W_qx_i, k_i = W_kx_i, v_i = W_vx_i\\)\n\\(w_{ij}^{'} = q_i^Tk_j\\) \\(w_ {ij} = softmax(w_{ij}^{'})\\)\n\\(y_i = \\sum_j w_{ij}v_j\\)\nWe’ve now given the Self Attention some controllable parameters & allows modification to the input vector to fit the task at hands.\n\n\n\nimage\n\n\n\nIllustration of the self-attention with key (red), query (query) and value (green) transformations. Remember old image and compare!\n\n\n\n\nSoftmax can be very sensitive to large values, exploding gradient or making it slow to learn.\nI don’t recall where but ~ 8 years ago someone figured out that scaling the value by \\(\\frac{1}{\\sqrt{k}}\\) where \\(k\\) is the embedding dimension.\n\\(w_{ij}^{'} = \\frac{q_i^Tk_j}{\\sqrt{k}}\\)\nWhy \\(\\sqrt{k}\\)? Imagine a vector in \\(ℝk\\) with values all \\(c\\). Its Euclidean length is \\(\\sqrt{k}c\\). Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors.\n\n\n\nThe final improvement is to allow word to have different meanings with different neighbours (basically what ngram achieves).\nBy adding multiple, indexed r, self attention mechanisms with different matrices, \\(W_q^r\\) etc. These are called attention heads.\nFor input \\(x_i\\) each attention head produces a different output vector \\(y_i^r\\). We concatenate these, and pass them through a linear transformation to reduce the dimension back to \\(k\\). Remember what is a linear transformation?\nNarrow and wide self-attention. There’s two ways to apply Multi-Head Self-Attention.\n1. (narrow) Cut embedding vector into chunks - 8 heads & \\(k=256\\) –> 8 chunks of size 32 - Each chunk gets Q, K, V matrices (\\(W_q^r\\),…) (\\(32\\times32\\)) 2. (wide) Make matrices \\(256\\times256\\) and apply each head to the whole 256-vector - First (narrow) = faster & less memory - Second (wider) = better result\nOnly second (wider) variant described.\n\n\n\n\nLet’s make the implementation with bells & whistles.\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass SelfAttention():\n  def __init__(self, k, heads=8):\n    raise TODO(\"Make SelfAttention a torch module (nn.Module)\")\n    super().__init__()\n    self.k, self.heads = k, heads\n\n\n\n\nimage\n\n\n\nCombining three attention heads into one matrix multiplication (for the queries).\n\nh attention-heads considered as h separate sets of \\(W_q^r,W_k^r,W_v^r\\), but as shown in image above a more efficient approach is possible.\nCombine all heads into three single \\(k\\times hk\\) matrices.\nThis means that we can compute the concatenated queries, keys & values in a single matrix multiplication.\n\nclass SelfAttention(nn.Module):\n  def __init__(self, k, heads=8):\n    super().__init__()\n    self.k, self.heads = k, heads\n\n    # These compute the queries, keys and values for all \n    # heads (as a single concatenated vector)\n    self.tokeys    = raise TODO(\"Create a linear layer of k * hk size, no bias\")\n    self.toqueries = raise TODO(\"Create a linear layer of k * hk size, no bias\")\n    self.tovalues  = raise TODO(\"Create a linear layer of k * hk size, no bias\")\n\n      # This unifies the outputs of the different heads into a single k-vector\n      self.unifyheads = raise TODO(\"Create a linear layer of k * hk size, WITH bias\")\n\nFrom here we can implement the computation of the self-attention (forward function). Where we first calculate queries, keys & values.\n\nclass SelfAttention(nn.Module):\n  def __init__(self, k, heads=8):\n    super().__init__()\n    self.k, self.heads = k, heads\n\n    # These compute the queries, keys and values for all \n    # heads (as a single concatenated vector)\n    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n    self.toqueries = nn.Linear(k, k * heads, bias=False)\n    self.tovalues  = nn.Linear(k, k * heads, bias=False)\n\n      # This unifies the outputs of the different heads into \n      # a single k-vector\n      self.unifyheads = nn.Linear(heads * k, k)\n\n  def forward(self, x):\n    b, t, k = x.size()\n    h = self.heads\n\n    queries = raise TODO(\"Create queries and then reshape into h separate matrices, using `view`\")\n    keys    = raise TODO(\"Create keys and then reshape into h separate matrices, using `view`\")\n    values  = raise TODO(\"Create values and then reshape into h separate matrices, using `view`\")\n\nHaving reshaped queries etc from \\((b,t, h*k)\\) into \\((b,t,h,k)\\) each head has its own dimension.\nThe step now is to compute dot-product for each head.\nWe can batch this if we reshape the matrices into something that’s possible to batch (transposing; as head/batch is not next to each-other). costly\n\nclass SelfAttention(nn.Module):\n  def __init__(self, k, heads=8):\n    super().__init__()\n    self.k, self.heads = k, heads\n\n    # These compute the queries, keys and values for all \n    # heads (as a single concatenated vector)\n    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n    self.toqueries = nn.Linear(k, k * heads, bias=False)\n    self.tovalues  = nn.Linear(k, k * heads, bias=False)\n\n      # This unifies the outputs of the different heads into \n      # a single k-vector\n      self.unifyheads = nn.Linear(heads * k, k)\n\n  def forward(self, x):\n    b, t, k = x.size()\n    h = self.heads\n\n    queries = self.toqueries(x).view(b, t, h, k)\n    keys    = self.tokeys(x)   .view(b, t, h, k)\n    values  = self.tovalues(x) .view(b, t, h, k)\n\n    # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory\n    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n\n    queries = raise TODO(\"Scale queries by k^(1/4)\")\n    keys    = raise TODO(\"Scale keys by k^(1/4)\")\n\n    # - get dot product of queries and keys, and scale\n    dot = torch.bmm(queries, keys.transpose(1, 2))\n    # - dot has size (b*h, t, k) containing raw weights\n\n    dot = raise TODO(\"Normalize row-wise using F.softmax\")\n    # - dot now contains row-wise normalized weights\n\n    # apply the self attention to the values\n    out = torch.bmm(dot, values).view(b, h, t, k)\n\n    # swap h, t back, unify heads\n    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n    return raise TODO(\"Unify the heads into the classes again using unifyheads\")\n\nAnd there you have it: multi-head, scaled dot-product self attention.\n\n\n\nA transformer is not just a self-attention layer, it is an architecture.\n\nAny architecture designed to process a connected set of units—such as the tokens in a sequence or the pixels in an image—where the only interaction between units is through self-attention.\n\nLike most mechanism, e.g. convolutions, a standard approach as emerged. The first step is to wrap the self-attention into a block that we can repeat.\n\n\nThere are some variations on how to build a basic transformer block, but most of them are structured roughly like this:\n\n\n\nimage\n\n\n\nMultiLayer Perceptron = MLP = Basic Feed-forward Neural Network\nBlue lines = Residual Connection (allow the gradient to flow through the network on a kind of “highway”, making the training faster and reducing “blown up gradients”)\n\n\nclass TransformerBlock(nn.Module):\n  def __init__(self, k, heads):\n    super().__init__()\n\n    self.attention = raise TODO(\"make a SelfAttention\")\n\n    self.norm1 = raise TODO(\"make a nn.LayerNorm of size k\")\n    self.norm2 = raise TODO(\"make a nn.LayerNorm of size k\")\n\n    # k * 4 is arbitrary, but needs to be larger than input/output layer (k)\n    self.ff = raise TODO(\"create a MLP: Sequential(Linear(k, 4*k), ReLU, Linear(4*k, k))\")\n\n  def forward(self, x):\n    attended = raise TODO(\"call your created attention on x\")\n    x = raise TODO(\"Use the first normalizer to normalizer attented+x\")\n    \n    fedforward = raise TODO(\"Call feedforward (ff) on new x\")\n    return raise TODO(\"Finally normalize with 2nd on the addition of feedforward & x\")\n\nAnd that is really it! We have now built a Transformer Block & Self Attention.\nNow we want to use it :)\n\n\n\n\nThe simplest classification task is a Sequence Classifier.\nThe IMDB Review dataset is a great contender to try things out with, where each review is positive or negative.\nEssentially we’ll create a chain of Transformers Block and input our embedded vectors of words, and in the end transforming the output into a single value (true/false).\n\n\nMost common way: Global Average Pooling on final output sequence and map result to a softmaxed class vector.\n\n\n\nimage\n\n\n\nOverview of a simple sequence classification transformer. The output sequence is averaged to produce a single vector representing the whole sequence. This vector is projected down to a vector with one element per class and softmaxed to produce probabilities.\n\n\n\n\nWe’ve already discussed that the current model uses embedding layer but has no sense of sequence time slotting (~ ngram).\nWe want our State-of-the-Art model to have a sense of order so we need to fix it.\nAdd a second vector of the same length as word embedding, that represent the position of the sentence+word, and add it to the word embedding. There’s two ways to do this.\n\nPosition Embedding We simply embed the positions like we did the words.\n\nEasy to implement\nWorks pretty good\nDrawback is that we have to see sequences of every length during training, otherwise the position is not trained!\n\nPosition Encoding Position encodings work in the same way as embeddings, except that we don’t learn the position vectors, we just choose some function \\(f:ℕ→ℝk\\) to map the positions to real valued vectors, and let the network figure out how to interpret these encodings.\n\nE.g. sin/cos\nWorks with longer sequences than seen (might not work well, but it works!)\nDrawback: choice of encoding function is a complicated hyperparameter, and more complicated implementation.\n\n\nFor this tutorial the Position Embedding is used.\n\n\n\nLet’s implement this!\n\nclass Transformer(nn.Module):\n    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n        super().__init__()\n\n        self.num_tokens = num_tokens\n        self.token_emb = raise TODO(\"Create a Embedding layer (nn.X) with num_tokens & k\")\n        self.pos_emb = raise TODO(\"Create Embedding Layer with seq_length & k\")\n\n        # The sequence of transformer blocks that does all the \n        # heavy lifting\n        blocks = []\n        for i in range(depth):\n            raise TODO(\"Append a TransformerBlock we recently created for each loop; and why not use list-comprehension?\")\n        self.t_blocks = raise TODO(\"Now make them a Sequential layer (*list = spread)\")\n\n            # Maps the final output sequence to class logits\n        self.to_probs = raise TODO(\"To get class logits we simply use a Linear layer of k * num_classes\")\n\n    def forward(self, x):\n        \"\"\"\n        :param x: A (b, t) tensor of integer values representing \n                  words (in some predetermined vocabulary).\n        :return: A (b, c) tensor of log-probabilities over the \n                 classes (where c is the nr. of classes).\n        \"\"\"\n            # generate token embeddings\n        tokens = raise TODO(\"Embedd the tokens\")\n        b, t, k = tokens.size()\n\n            # generate position embeddings\n            positions = torch.arange(t)\n        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n        \n        x = tokens + positions\n        x = raise TODO(\"Run the network through the tranformer blocks\")\n        \n        # Average-pool over the t dimension and project to class \n        # probabilities\n        x = raise TODO(\"Use the probability function, but first take the mean over dim=1 to average\")\n        return raise TODO(\"Use the F.log_softmax on dim=1 to normalize output!\")\n\nAt depth 6, with a maximum sequence length of 512, this transformer achieves an accuracy of about 85%, competitive with results from RNN models, and much faster to train. To see the real near-human performance of transformers, we’d need to train a much deeper model on much more data. More about how to do that later.\n\n\n\n\nLet’s move on!\nIn Text Generation we are not allowed to know the future during training, how else are we going to predict it? This means that we need to use an autoregressive model.\nFor Text Generation we’ll train a character-to-character prediction, the input is a sequence of characters and output is the input shifted one charafter to the left.\n\n\n\nimage\n\n\nFor the usual RNN this is all that is needed, but as mentioned now we need to make our model autoregressive, meaning that it can’t look ahead.\nThis is done by applying a mask which disables all elements that are ahead of current index, as in image below.\n\n\n\nimage\n\n\n\nMasking the self attention, to ensure that elements can only attend to input elements that precede them in the sequence. Note that the multiplication symbol is slightly misleading: we actually set the masked out elements (the white squares) to −∞\n\n\n\n\nImplementing this in PyTorch\n\nclass SelfAttentionAutoRegressive(nn.Module):\n  def __init__(self, k, heads=8):\n    super().__init__()\n    self.k, self.heads = k, heads\n\n    # These compute the queries, keys and values for all \n    # heads (as a single concatenated vector)\n    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n    self.toqueries = nn.Linear(k, k * heads, bias=False)\n    self.tovalues  = nn.Linear(k, k * heads, bias=False)\n\n      # This unifies the outputs of the different heads into \n      # a single k-vector\n      self.unifyheads = nn.Linear(heads * k, k)\n\n  def forward(self, x):\n    b, t, k = x.size()\n    h = self.heads\n\n    queries = self.toqueries(x).view(b, t, h, k)\n    keys    = self.tokeys(x)   .view(b, t, h, k)\n    values  = self.tovalues(x) .view(b, t, h, k)\n\n    # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory\n    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n\n    queries = queries / (k ** (1/4))\n    keys    = keys / (k ** (1/4))\n\n    # - get dot product of queries and keys, and scale\n    dot = torch.bmm(queries, keys.transpose(1, 2))\n\n    # === START OF CHANGES ===\n    indices = torch.triu_indices(t, t, offset=1)\n    dot[:, indices[0], indices[1]] = raise TODO(\"-inf; and think off what we are doing. triu_indices is shown below\")\n    # === END OF CHANGES ===\n\n    # - dot has size (b*h, t, t) containing raw weights\n\n    dot = F.softmax(dot, dim=2) \n    # - dot now contains row-wise normalized weights\n\n    # apply the self attention to the values\n    out = torch.bmm(dot, values).view(b, h, t, k)\n\n    # swap h, t back, unify heads\n    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n    return self.unifyheads(out)\n\n\n??torch.triu_indices\n\nOnce the model has been ‘handicapped’ as this we’re ready to go!\nIf you’d like to learn the historical aspect of Transformers, some design consideration & more details please visit the original blog by Peter Bloem and finish that one!\nI hope you found this helpful!\nPlease take a good look in Appendix for demos with Text Classification & Text Generation.\nDo note that the depth should probably be increased as should the amount of data. Transformers improve a lot with time, depth & data (especially data!).\nThanks\n~Hampus"
  },
  {
    "objectID": "posts/2021-02-18-transformers-explained/transformers-explained.html#dependencies",
    "href": "posts/2021-02-18-transformers-explained/transformers-explained.html#dependencies",
    "title": "Transformers From Scratch",
    "section": "Dependencies",
    "text": "Dependencies\nSimply run this block to - Upgrade fastai (2.0+) - Import fastai\n\n%%capture\n!pip install -U fastai\n\nfrom fastai.text.all import *\nfrom functools import partial # Can use partial to preload a transformer block"
  },
  {
    "objectID": "posts/2021-02-18-transformers-explained/transformers-explained.html#text-classification",
    "href": "posts/2021-02-18-transformers-explained/transformers-explained.html#text-classification",
    "title": "Transformers From Scratch",
    "section": "Text Classification",
    "text": "Text Classification\nThe Text Classification is done on the IMDB challenge. There’s code included for both IMDB_SAMPLE and IMDB where the sample is a little too small to actually get good stats on.\nI think adding some more depth (perhaps 6 blocks?) will increase the accuracy even further.\n\nModel (Classifier)\nCode we already wrote for the classifier\n\nclass SelfAttention(Module):\n  def __init__(self, k, heads=8):\n    self.k, self.heads = k, heads\n\n    # These compute the queries, keys and values for all \n    # heads (as a single concatenated vector)\n    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n    self.toqueries = nn.Linear(k, k * heads, bias=False)\n    self.tovalues  = nn.Linear(k, k * heads, bias=False)\n\n    # This unifies the outputs of the different heads into \n    # a single k-vector\n    self.unifyheads = nn.Linear(heads * k, k)\n\n  def forward(self, x):\n    b, t, k = x.size()\n    h = self.heads\n    \n    assert self.k == k, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n\n    queries = self.toqueries(x).view(b, t, h, k)\n    keys    = self.tokeys(x)   .view(b, t, h, k)\n    values  = self.tovalues(x) .view(b, t, h, k)\n\n    # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory\n    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n\n    queries = queries / (k ** (1/4))\n    keys    = keys / (k ** (1/4))\n\n    # - get dot product of queries and keys, and scale\n    dot = torch.bmm(queries, keys.transpose(1, 2))\n\n    assert dot.size() == (b*h, t, t)\n\n    # - dot has size (b*h, t, t) containing raw weights\n    dot = F.softmax(dot, dim=2) \n    # - dot now contains row-wise normalized weights\n\n    # apply the self attention to the values\n    out = torch.bmm(dot, values).view(b, h, t, k)\n\n    # swap h, t back, unify heads\n    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n    \n    return self.unifyheads(out)\n\n\nclass TransformerBlock(Module):\n  def __init__(self, k, heads):\n    super().__init__()\n\n    self.attention = SelfAttention(k, heads=heads)\n\n    self.norm1 = nn.LayerNorm(k)\n    self.norm2 = nn.LayerNorm(k)\n\n    self.ff = nn.Sequential(\n      nn.Linear(k, 4 * k),\n      nn.ReLU(),\n      nn.Linear(4 * k, k)\n    )\n\n  def forward(self, x):\n    attended = self.attention(x)\n    x = self.norm1(attended + x)\n    \n    fedforward = self.ff(x)\n    return self.norm2(fedforward + x)\n\n\nclass Transformer(Module):\n    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes, device):\n        super().__init__()\n\n        self.device = device\n        self.seq_length = seq_length\n        self.num_tokens = num_tokens\n        self.token_emb = nn.Embedding(num_tokens, k)\n        self.pos_emb = nn.Embedding(seq_length, k)\n\n            # The sequence of transformer blocks that does all the \n        # heavy lifting\n        tblocks = [TransformerBlock(k=k, heads=heads) for x in range(depth)]\n        self.tblocks = nn.Sequential(*tblocks)\n\n            # Maps the final output sequence to class logits\n        self.toprobs = nn.Linear(k, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: A (b, t) tensor of integer values representing \n                  words (in some predetermined vocabulary).\n        :return: A (b, c) tensor of log-probabilities over the \n                 classes (where c is the nr. of classes).\n        \"\"\"\n            # generate token embeddings\n        # x = x.split(' ')\n        x = x[:,:self.seq_length,]\n        \n        tokens = self.token_emb(x)\n        b, t, k = tokens.size()\n        # print(b, t, k)\n\n        # generate position embeddings\n        positions = torch.arange(t, device=self.device)\n        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n        \n        x = tokens + positions\n        x = self.tblocks(x)\n        \n        # Average-pool over the t dimension and project to class \n        # probabilities\n        x = self.toprobs(x.mean(dim=1))\n        return F.log_softmax(x, dim=1)\n\n\n\nThe Text Classification\n\nData Collection\nTraining Loop\nValidation\n\n\npath = untar_data(URLs.IMDB)\npath.ls()\n\n\n\n\n(#7) [Path('/root/.fastai/data/imdb/README'),Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/tmp_clas'),Path('/root/.fastai/data/imdb/imdb.vocab')]\n\n\n\n# === Use this if you want to use IMDB_SAMPLE: But IMDB_SAMPLE is too small for a transformer ===\n# df = pd.read_csv(path/'texts.csv');df.head(2)\n# dls = TextDataLoaders.from_df(df, path=path, text_col='text', label_col='label', valid_col='is_valid')\n\n\n\n\n/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order)\n\n\n\n# === Enable this to use much more data (better for transformer) === \ndb = DataBlock(\n    blocks = (TextBlock.from_folder(path, max_vocab=10_000, seq_len=256), CategoryBlock),\n    get_items=get_text_files,\n    splitter=GrandparentSplitter(valid_name='test'),\n    get_y=parent_label\n)\n\ndls = db.dataloaders(path, path = path)\n\n\ndls.show_batch(max_n=2)\n\n\n\n  \n    \n      \n      text\n      category\n    \n  \n  \n    \n      0\n      xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n\\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n't quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director 's part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n't fool me . xxmaj raising xxmaj victor xxmaj vargas is\n      negative\n    \n    \n      1\n      xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot … which is highly believable to xxunk . xxmaj it 's easy to think that such a love story , as beautiful as any other ever told , * could * happen to you … a feeling you do n't often get from other romantic comedies\n      positive\n    \n  \n\n\n\n\nlen(dls.vocab[0]),dls.device,dls.one_batch()[0].size()\n\n(10008, device(type='cuda', index=0), (64, 3345))\n\n\n\n# (self, k, heads, depth, seq_length, num_tokens, num_classes)\nlearn = Learner(dls, Transformer(k=256, heads=8, depth=4, seq_length=256, num_tokens=len(dls.vocab[0]), num_classes=2, device=dls.device), metrics=[accuracy])\n\n\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=5.754399462603033e-05, lr_steep=0.6309573650360107)\n\n\n\n\n\n\nlearn.fit_one_cycle(6, 5.7e-5) # We can increase depth & more to improve the result\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.652948\n      0.627165\n      0.656120\n      04:00\n    \n    \n      1\n      0.580878\n      0.548993\n      0.722400\n      04:08\n    \n    \n      2\n      0.481650\n      0.492818\n      0.760320\n      04:09\n    \n    \n      3\n      0.458933\n      0.472389\n      0.770200\n      04:09\n    \n    \n      4\n      0.432385\n      0.461327\n      0.778800\n      04:10\n    \n    \n      5\n      0.409607\n      0.460612\n      0.780320\n      04:08\n    \n  \n\n\n\nFast.AI approach\n\nlearn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\nlearn.fine_tune(4, 1e-2)\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.624490\n      0.643156\n      0.625000\n      00:12\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy\n      time\n    \n  \n  \n    \n      0\n      0.441161\n      0.550412\n      0.755000\n      00:27\n    \n    \n      1\n      0.380404\n      0.602693\n      0.650000\n      00:27\n    \n    \n      2\n      0.316619\n      0.483822\n      0.750000\n      00:27\n    \n    \n      3\n      0.256229\n      0.535840\n      0.750000\n      00:27\n    \n  \n\n\n\n\nlearn.show_results()"
  },
  {
    "objectID": "posts/2021-02-18-transformers-explained/transformers-explained.html#text-generation",
    "href": "posts/2021-02-18-transformers-explained/transformers-explained.html#text-generation",
    "title": "Transformers From Scratch",
    "section": "Text Generation",
    "text": "Text Generation\nFirst of we generate text based on the IMDB dataset, but then we’ve also got the shakespeare txt file afterwards for personal testing :)\n\nThe Model\nCode we’ve done, the essential model\n\nclass SelfAttentionAutoRegressive(Module):\n  def __init__(self, k, heads=8):\n    self.k, self.heads = k, heads\n\n    # These compute the queries, keys and values for all \n    # heads (as a single concatenated vector)\n    self.tokeys    = nn.Linear(k, k * heads, bias=False)\n    self.toqueries = nn.Linear(k, k * heads, bias=False)\n    self.tovalues  = nn.Linear(k, k * heads, bias=False)\n\n      # This unifies the outputs of the different heads into \n    # a single k-vector\n    self.unifyheads = nn.Linear(heads * k, k)\n\n  def forward(self, x):\n    b, t, k = x.size()\n\n    h = self.heads\n\n    queries = self.toqueries(x).view(b, t, h, k)\n    keys    = self.tokeys(x)   .view(b, t, h, k)\n    values  = self.tovalues(x) .view(b, t, h, k)\n\n    # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory\n    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n\n    queries = queries / (k ** (1/4))\n    keys    = keys / (k ** (1/4))\n\n    # - get dot product of queries and keys, and scale\n    dot = torch.bmm(queries, keys.transpose(1, 2))\n\n    indices = torch.triu_indices(t, t, offset=1, device='cuda') # ---OBS--- this also changed\n    dot[:, indices[0], indices[1]] = float('-inf')\n\n    # - dot has size (b*h, t, t) containing raw weights\n\n    dot = F.softmax(dot, dim=2) \n    # - dot now contains row-wise normalized weights\n\n    # apply the self attention to the values\n    out = torch.bmm(dot, values).view(b, h, t, k)\n\n    # swap h, t back, unify heads\n    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n    return self.unifyheads(out)\n\n\nclass TransformerBlock(Module):\n  def __init__(self, k, heads):\n    super().__init__()\n\n    self.attention = SelfAttentionAutoRegressive(k, heads=heads)\n\n    self.norm1 = nn.LayerNorm(k)\n    self.norm2 = nn.LayerNorm(k)\n\n    self.ff = nn.Sequential(\n      nn.Linear(k, 4 * k),\n      nn.ReLU(),\n      nn.Linear(4 * k, k)\n    )\n\n  def forward(self, x):\n    attended = self.attention(x)\n    x = self.norm1(attended + x)\n    \n    fedforward = self.ff(x)\n    return self.norm2(fedforward + x)\n\n\nclass Transformer(Module):\n    def __init__(self, k, heads, depth, seq_length, num_tokens, device):\n        super().__init__()\n\n        self.device = device\n        self.seq_length = seq_length\n        self.num_tokens = num_tokens\n        self.token_emb = nn.Embedding(num_tokens, k)\n        self.pos_emb = nn.Embedding(seq_length, k)\n\n            # The sequence of transformer blocks that does all the \n        # heavy lifting\n        tblocks = [TransformerBlock(k=k, heads=heads) for x in range(depth)]\n        self.tblocks = nn.Sequential(*tblocks)\n\n            # Maps the final output sequence to class logits\n        self.toprobs = nn.Linear(k, num_tokens)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: A (b, t) tensor of integer values representing \n                  words (in some predetermined vocabulary).\n        :return: A (b, c) tensor of log-probabilities over the \n                 classes (where c is the nr. of classes).\n        \"\"\"\n            \n        # generate token embeddings\n        # print(x.size())\n        \n        tokens = self.token_emb(x)\n        b, t, k = tokens.size()\n        # print(b, t, k)\n\n        # generate position embeddings\n        positions = torch.arange(t, device=self.device)\n        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n        \n        x = tokens + positions\n        x = self.tblocks(x)\n        \n        # Average-pool over the t dimension and project to class \n        # probabilities\n        x = self.toprobs(x.view(b*t, k)).view(b, t, self.num_tokens)\n        # print(x.size())\n        return F.log_softmax(x, dim=2)\n\n\n\nThe Text Generation\n\nData Collection\nTraining Loop\nValidation\n\n\npath = untar_data(URLs.IMDB_SAMPLE)\npath.ls()\n\n(#2) [Path('/root/.fastai/data/imdb_sample/texts.csv'),Path('/root/.fastai/data/imdb_sample/models')]\n\n\n\ndf = pd.read_csv(path/'texts.csv')\n\n\ndf.head(1)\n\n\n\n\n\n  \n    \n      \n      label\n      text\n      is_valid\n    \n  \n  \n    \n      0\n      negative\n      Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!\n      False\n    \n  \n\n\n\n\n\ndls = TextDataLoaders.from_df(df, path=path, text_col='text', is_lm=True, valid_col='is_valid', seq_len=256)\n\n\n\n\n/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  return array(a, dtype, copy=False, order=order)\n\n\n\ndls.show_batch(max_n=1)\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos i enjoyed this movie . xxmaj unlike like some of the xxunk up , xxunk trash that is passed off as action movies , xxmaj playing xxmaj god is simple and realistic , with characters that are believable , action that is not over the top and enough twists and turns to keep you interested until the end . \\n\\n xxmaj well directed , well acted and a good story . xxbos xxmaj for those fans of xxmaj laurel and xxmaj hardy , the 1940s and beyond were a very sad time for the team . xxmaj their xxunk with xxmaj xxunk xxmaj xxunk xxmaj studios had xxunk and now they were \" free xxunk to work for any studio who xxunk them a job . xxmaj unfortunately , xxmaj fox , xxup xxunk , xxup xxunk ( without xxmaj xxunk ) and even a xxmaj french film company\n      i enjoyed this movie . xxmaj unlike like some of the xxunk up , xxunk trash that is passed off as action movies , xxmaj playing xxmaj god is simple and realistic , with characters that are believable , action that is not over the top and enough twists and turns to keep you interested until the end . \\n\\n xxmaj well directed , well acted and a good story . xxbos xxmaj for those fans of xxmaj laurel and xxmaj hardy , the 1940s and beyond were a very sad time for the team . xxmaj their xxunk with xxmaj xxunk xxmaj xxunk xxmaj studios had xxunk and now they were \" free xxunk to work for any studio who xxunk them a job . xxmaj unfortunately , xxmaj fox , xxup xxunk , xxup xxunk ( without xxmaj xxunk ) and even a xxmaj french film company who\n    \n  \n\n\n\n\nlen(dls.vocab),dls.one_batch()[0].size()\n\n(7080, (64, 256))\n\n\n\nlearn = Learner(dls, Transformer(k=256, heads=8, depth=3, seq_length=256, num_tokens=len(dls.vocab), device=dls.device), loss_func=CrossEntropyLossFlat())\n\n\nlearn.freeze()\nlearn.lr_find()\n\n\n\n\nSuggestedLRs(lr_min=0.02089296132326126, lr_steep=0.0014454397605732083)\n\n\n\n\n\n\nlearn.unfreeze()\n\n\nlearn.fit_one_cycle(4)  # Add suggested LR if you'd like\nlearn.fit_one_cycle(4)\nlearn.fit_one_cycle(4)\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      5.952718\n      6.005916\n      00:09\n    \n    \n      1\n      5.943147\n      5.950939\n      00:09\n    \n    \n      2\n      5.907956\n      5.901570\n      00:09\n    \n    \n      3\n      5.869049\n      5.870370\n      00:09\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      5.782402\n      5.828668\n      00:09\n    \n    \n      1\n      5.687460\n      5.575956\n      00:09\n    \n    \n      2\n      5.571561\n      5.469945\n      00:09\n    \n    \n      3\n      5.484975\n      5.450408\n      00:09\n    \n  \n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      5.322245\n      5.439701\n      00:09\n    \n    \n      1\n      5.280037\n      5.311460\n      00:09\n    \n    \n      2\n      5.215474\n      5.259928\n      00:09\n    \n    \n      3\n      5.167902\n      5.247015\n      00:09\n    \n  \n\n\n\n\ndef predict(self, text, n_words=1, no_unk=True, temperature=1., min_p=None, no_bar=False,\n            decoder=decode_spec_tokens, only_last_word=False):\n    \"Return `text` and the `n_words` that come after\"\n    \n    idxs = idxs_all = self.dls.test_dl([text]).items[0].to(self.dls.device)\n    if no_unk: unk_idx = self.dls.vocab.index(UNK)\n    for _ in (range(n_words) if no_bar else progress_bar(range(n_words), leave=False)):\n        with self.no_bar(): preds,_ = self.get_preds(dl=[(idxs[None],)])\n        # print(preds.size())\n        res = preds[0][-1]\n        if no_unk: res[unk_idx] = 0.\n        if min_p is not None:\n            if (res >= min_p).float().sum() == 0:\n                warn(f\"There is no item with probability >= {min_p}, try a lower value.\")\n            else: res[res < min_p] = 0.\n        if temperature != 1.: res.pow_(1 / temperature)\n        idx = torch.multinomial(res, 1).item()\n        idxs = idxs_all = torch.cat([idxs_all, idxs.new([idx])])\n        if only_last_word: idxs = idxs[-1][None]\n\n    num = self.dls.train_ds.numericalize\n    tokens = [num.vocab[i] for i in idxs_all if num.vocab[i] not in [BOS, PAD]]\n    sep = self.dls.train_ds.tokenizer.sep\n    return sep.join(decoder(tokens))\n\n@delegates(Learner.get_preds)\ndef get_preds(self, concat_dim=1, **kwargs):\n  return super().get_preds(concat_dim=concat_dim, **kwargs)\n\n\npredict(learn, \"I think its a very\", n_words=20, temperature=1.)\n\n\n\n\n'i think its a very Who spoiler to sort with a flat in clumsy and my world to sit to american to were even'\n\n\n\nlearn.save('gen.pkl')\n\n\n# Shakespeare for those that'd want it!\n!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt\n\nFastAI way\n\npredict(language_model_learner(dls, AWD_LSTM), \"I think its\", n_words=2, temperature=1.)"
  },
  {
    "objectID": "posts/2020-02-23-competence-meeting-imdb-text-classification.html",
    "href": "posts/2020-02-23-competence-meeting-imdb-text-classification.html",
    "title": "AFRY NLP Competence Meeting: Text Classification IMDB",
    "section": "",
    "text": "I’ve set a goal to create one blog post per Competence Meeting I’ve held at AFRY to spread the knowledge further. This goal will also grab all the older meetings, my hope is that I’ll be finished before summer 2020, but we’ll see. \n\n\n\nMost of my Competence Meetings take place in the form of Jupyter Notebooks (.ipynb). Notebooks are awesome as they allow us to:\n\nMix and match markdown & code-blocks\nKeep the state of the program, i.e. very explorative\n\nThis is really good in combination with the workshop-format that we usually have. Using services such as Google Colab one can take the file and open it in the browser and run it there. This means that we don’t need any downloads and pretty often we also have a speed gain because the node used is faster than a laptop with its GPU.\nLet’s get on to the competence evening.\n\n\n\n\nToday we’ll go through text classification, what it is, how it is used and how to make it yourself while trying to keep have a great mix of both theory and practical use. Text classification is just what the name suggest, a way to classify texts. Let it be spam or reviews, you train it and it’ll predict what class the text belongs to.\n\n\n\nTo have a good baseline is incredibly important in Machine Learning. In summary you want the following\n\nSimple model to predict outcome\nUse this model to compare your new, more complex model to\n\nThis is to be able to know what progress you’re making. You don’t want to do anything more complex without any gains.\nOne pretty common simple baseline is just to pick a random class as prediction.\n\n\n\nWhat is a class and feature?\nFeatures are the input to the model, you can see a machine learning system as a \"consumer\" of features. You can view this as a cookie monster consuming cookies and then he says if they taste good or bad. He has the input, cookie, that can be a feature. He then has a output, class, that is good/bad. Repeat this a lot of times and you can retrieve statistics if Cookie Y is good or bad.\nTo generalize this system we would divide the feature into multiple feature, like what ingredients the cookie contains. So instead of saying this is a \"Chocolate Chip Cookie\" we know tell the system the features are:\nchocolate: yes\nsugar:yes\nhoney:no\noat:no\ncinnamon: no\nsweet: yes\nsour: no\\\"\n. In numerical input it would translate to something as [1,1,0,0,0,1,0].\n\n\nAs shown in the translation to numerical vectors we don’t represent words as actual words. We always use numbers, often we even use something called One-Hot-Encoding.\nOne-Hot-Encoding means that we have an array of one 1 and the rest is 0s. This is to optimize math performed by the GPU (or CPU).\nUsing the example of Good & Bad cookies with the extension of Decent we will One-Hot-Encode these as the following\nGood   = [1,0,0]\nBad    = [0,1,0]\nDecent = [0,0,1]\nThe same is applied to our features. If you’re using a framework (such as Keras) it is pretty common that they include an method to do this, or even that it is done automatically for you.\n\n\n\n\nTo classify a text we do what is called an sentiment analysis meaning that we try to estimate the sentiment polarity of a text body. In the first part of this workshop we’ll be assuming that there’s only two sentiments, Negative and Positive. Then we can express this as the following classification problem:\nFeature: String body\nClass:   Bad|Good\nThe output, Classes, are easy to One-Hot-Encode but how do we succesfully One-Hot-Encode a string? A character can be seen as a class but is that really something we can learn from? To solve this we need to preprocess our input somehow.\n\n\n\nPreprocessing is an incredibly important part of Machine Learning. Combining preprocessing with Data Mining is actually around 70% of the workload (IBM) when developing models through the CRISP-DM. From my experience this is true.\nHaving good data and finding the most important features is incredibly important to have a competent system. In this task we need to preprocess the text to simplify the learning process for our system. We will do the following:\n\nClean the text\nVectorize the texts into numerical vectors\n\n\n\nWhy do we need to clean the text? It is to remove weird stuff & outliers. If we have the text I'm a cat.we want to simplify this into [i'm, a, cat] or even [im, a, cat].\nRemoving data such as non-alphabetical characters and the letter case makes more data look a like and reduces the dimension of our input – this simplifies the learning of the system. But removing features can be bad also, if someone writes in all CAPS we can guess that they’re angry. But let’s take that later.\nimport regex as re\n\n\ndef clean_text(text):\n    \\\"\\\"\\\"\n    Applies some pre-processing on the given text.\n\n    Steps :\n    - Removing punctuation\n    - Lowering text\n    \\\"\\\"\\\"\n    \n    # remove the characters [\\\\], ['] and [\\\"]\n    text = re.sub(r\\\"\\\\\\\\\\\", \\\"\\\", text)    \n    text = re.sub(r\\\"\\\\'\\\", \\\"\\\", text)    # Extra: Is regex needed? Other ways to accomplish this.\n    text = re.sub(r\\\"\\\\\\\"\\\", \\\"\\\", text)\n    # replace all non alphanumeric with space \n    text = re.sub(r\\\"\\\\W+\\\", \\\" \\\", text)\n    # text = re.sub(r\\\"<.+?>\\\", \\\" \\\", text) # <br></br>hej<br></br>\n    \n    # Extra: How would we go ahead and remove HTML? Time to learn some Regex!\n    \n    return text.strip().lower()\nclean_text(\\\"Wow, we can clean text now. Isn't that amazing!?\\\").split()\n\n\n\nNow that we can extract text we need to be able to input it to the system. We have to vectorize it. In this part we’ll vectorize each word as a number. The simplest approach to this is using Bag of Words (BOW).\nBag of Words creates a list of words which is called the Dictionary. The Dictionary is just a list of the words from the training data.\nTraining data: [\\\"ÅF is a big company\\\", \\\"ÅF making future\\\"]\n--> Dictionary: [ÅF, is, a, big, company, making, future]\n\nNew text: \\\"ÅF company is a future company\\\" --> [1,1,1,0,2,0,1]\nOur new text is vectorized on top of the dictionary. You take the dictionary and replace the words position with the count of it that is found in the new text.\n\n\n\nWe can actually do some more things to improve the system which I won’t go into detail about (read the code). We remove stop-words and so on.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ntraining_texts = [\n    \\\"ÅF is a big company\\\", \n    \\\"ÅF making future\\\"\n]\n\ntest_texts = [\n    \\\"ÅF company is a future company\\\"\n]\n\n# this is the vectorizer\nvectorizer = CountVectorizer(\n    stop_words=\\\"english\\\",    # Removes english stop words (such as 'a', 'is' and so on.)\n    preprocessor=clean_text  # Customized preprocessor\n)\n\n# fit the vectorizer on the training text\nvectorizer.fit(training_texts)\n\n# get the vectorizer's vocabulary\ninv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}\nvocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]\n\n# vectorization example\npd.DataFrame(\n    data=vectorizer.transform(test_texts).toarray(),\n    index=[\\\"Test sentence\\\"],\n    columns=vocabulary\n)\n\n\n\n\nTo begin with we need data. Luckily I know a perfect dataset for this – the IMDB movie reviews from stanford. This is a widely used dataset throughout Sentiment Analysis. The data contains 50 000 reviews where 50 % is positive and the rest negative. First we fetch a dataset. Download this file and unpack it (into aclImdb) if the first code-snippet was unsuccessful.\nimport os\nimport numpy as np\n\ndef load_train_test_imdb_data(data_dir):\n    \\\"\\\"\\\"\n    Loads the IMDB train/test datasets from a folder path.\n    Input:\n    data_dir: path to the \\\"aclImdb\\\" folder.\n    \n    Returns:\n    train/test datasets as pandas dataframes.\n    \\\"\\\"\\\"\n\n    data = {}\n    for split in [\\\"train\\\", \\\"test\\\"]:\n        data[split] = []\n        for sentiment in [\\\"neg\\\", \\\"pos\\\"]:\n            score = 1 if sentiment == \\\"pos\\\" else 0\n\n            path = os.path.join(data_dir, split, sentiment)\n            file_names = os.listdir(path)\n            for f_name in file_names:\n                with open(os.path.join(path, f_name), \\\"r\\\") as f:\n                    review = f.read()\n                    data[split].append([review, score])\n  \n    # We shuffle the data to make sure we don't train on sorted data. This results in some bad training.\n    np.random.shuffle(data[\\\"train\\\"])        \n    data[\\\"train\\\"] = pd.DataFrame(data[\\\"train\\\"],\n                                 columns=['text', 'sentiment'])\n\n    np.random.shuffle(data[\\\"test\\\"])\n    data[\\\"test\\\"] = pd.DataFrame(data[\\\"test\\\"],\n                                columns=['text', 'sentiment'])\n\n    return data[\\\"train\\\"], data[\\\"test\\\"]\ntrain_data, test_data = load_train_test_imdb_data(\n    data_dir=\\\"aclImdb/\\\")\n\n\n\nWe now have a dataset that we have successfully partitioned into a dictionary so that we can use it for our classifier.\nDo you see an issue with our baseline right now?\n…As mentioned we want to only have important features to simplify training. Right now we have an enormous amount of features, our BOW-approach result in an 80 000-dimensional vector. Because of this we must use simple algorithms that learn fast & easy, e.g. Linear SVM, Naive Bayes or Logistic Regression.\nLet’s create some code that actually let’s us train a Linear SVM!\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\n\n\n# Transform each text into a vector of word counts\nvectorizer = CountVectorizer(stop_words=\\\"english\\\",\n                             preprocessor=clean_text)\n\ntraining_features = vectorizer.fit_transform(train_data[\\\"text\\\"])    \ntest_features = vectorizer.transform(test_data[\\\"text\\\"])\n\n# Training\nmodel = LinearSVC()\nmodel.fit(training_features, train_data[\\\"sentiment\\\"])\ny_pred = model.predict(test_features)\n\n# Evaluation\nacc = accuracy_score(test_data[\\\"sentiment\\\"], y_pred)\n\nprint(\\\"Accuracy on the IMDB dataset: {:.2f}\\\".format(acc*100))\n\n\n\nOur accuracy is somewhere around 83.5-84 % which is really good! With this simple model and incredibly simplistic feature extraction we achieve a really high amount of correct answer! Comparing this to state-of-the-art we’re around 11 percent units beneat (~95% accuracy achieved here).\nIncredible right? Exciting!? For me it is at least!\nHow do we improve from here?\n\n\n\nWe have some huge improvements to make outside of fine-tuning, so we’ll skip the fine-tuning from now.\nThe first step is to improve our vectorization.\n\n\nIf you were at first friday (@ÅF) you have heard about TF-IDF earlier. TF-IDF stands for Term Frequence-Inverse Document Frequency and is a measurement that aims to fight imbalances in texts.\nIn our vectorization step we look at the word-count meaning that we’ll have some biases to how much a word is present, the longer the text the more the bias. To reduce this we can take the word-count divided by the total amount of words in the text (TF). We also want to downscale the words that are incredibly frequent such as stop words and topic-related words, and upscale unusual words somewhat, e.g.glamorous might not be frequent but it is important to the text most likely. We use IDF for this. We then take these two and combine.\n\n\n\nalt text\n\n\n\n\n\n\nThis is actually really easy to do as sklearn already has a finished TfIdfVectorizer so all we have to do is to replace the CountVectorizer. Let’s see how it goes!\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n# Transform each text into a vector of word counts\nvectorizer = TfidfVectorizer(stop_words=\\\"english\\\",\n                             preprocessor=clean_text)\n\ntraining_features = vectorizer.fit_transform(train_data[\\\"text\\\"])    \ntest_features = vectorizer.transform(test_data[\\\"text\\\"])\n\n# Training\nmodel = LinearSVC()\nmodel.fit(training_features, train_data[\\\"sentiment\\\"])\ny_pred = model.predict(test_features)\n\n# Evaluation\nacc = accuracy_score(test_data[\\\"sentiment\\\"], y_pred)\n\nprint(\\\"Accuracy on the IMDB dataset: {:.2f}\\\".format(acc*100))\n\n# Extra: Implement our own TfIdfVectorizer.\n\n\n\nThe TfIdVectorizer improved our scoring with 2 percent units, that’s incredible for such an easy improvement!\nThis for me shows how important it is to understand the data and what is important. You really need to grasp how to extract the important and what tools are available.\nBut let’s not stop here, lets reiterate and improve further.\nWhat is the next natural step? Context I believe. During my master-thesis on spell correction of Street Names it was very obvious how important context is to increase the models understanding. Unfortunately we couldn’t use the context of a sentence in the thesis (as of the nature of street names) but here we can!\n\n\n\nWords by themself prove some meaning but sometimes they’re used in a negated sense, e.g. not good. Good in itself would most likely be positive but if we can get the context around the word we can be more sure about in what manner it is applied.\nWe call this N-grams where N is equal to the amount of words taken into consideration for each word. Using bigrams (N=2) we get the following:\ncompanies often use corporate bs => [companies, often, use, slogans, (companies, often), (often,use), (use,slogans)]\nSometimes you include a start & ending word so that it would be (\\\\t, companies) and (slogans, \\\\r) or such. In this case as we are not finetuning we won’t go into that. We’ll keep it simple.\nThe all-mighty sklearn TfIdfVectorizer actually already have included N-gram support using the parameter ngram_range=(1, N). So let’s make it simple for us and make use of that!\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n# Transform each text into a vector of word counts\nvectorizer = TfidfVectorizer(ngram_range=(1, 2),\n                            strip_accents='ascii',\n                            max_df=0.98)\n\ntraining_features = vectorizer.fit_transform(train_data[\\\"text\\\"])    \ntest_features = vectorizer.transform(test_data[\\\"text\\\"])\n\n# Training\nmodel = LinearSVC()\nmodel.fit(training_features, train_data[\\\"sentiment\\\"])\ny_pred = model.predict(test_features)\n\n# Evaluation\nacc = accuracy_score(test_data[\\\"sentiment\\\"], y_pred)\n\nprint(\\\"Accuracy on the IMDB dataset: {:.2f}\\\".format(acc*100))\n\n\n\nOnce again we see a massive improvement. We’re almost touching 89 % now! That’s just a mere 6 percent units below state-of-the-art. What can we do to improve now?\nSome possible improvements for you to try!\n\nUse a custom threshold to reduce the dimensions\nPlay around with the ngram_range (don’t forget a threshold if you do this)\nImprove the preprocessing\n\n# Try some fun things here if you want too :)\n\n\n\n\nWe have created a strong baseline for text classification with great accuracy for its simplicity. The following steps has been done\n\nFirst a simple preprocessing step which is of great importance. We have to remember to not make it to complex, the complexity of preprocessing is like an evil circle in the end. In our case we remove punctuations, stopwords and lower the case.\nSecondly we vectorize the data to make it readable by the system. A classifier requires numerical features. For this we had a TfIdfVectorizer that computes frequency of words while downsampling words that are to common & upsampling unusual words.\nFinally we added N-gram to the model to increase the understanding of the sentence by supplying context.\n\n\n\n\nHow do we improve from here? TF-IDF has its cons and pros. Some of the cons are that they:\n\nDon’t account for any kind of positioning at all\nThe dimensions are ridiculous large\nThey can’t capture semantics.\n\nImprovements upon this is made by using neural networks and word embeddings.\n\n\n\nWord Embeddings & Neural Networks are where we left off. By change our model to instead utilize these two concepts we can improve the accuracy once again.\n\n\nWord Embeddings (WE) are actually a type of Neural Network. It uses embedding to create the model. I quickly explained WE during my presentation on Summarization and how to build a great summarizer. Today we’ll go a little more into depth.\nTo begin with I’ll take the most common example, WE lets us do the following arithmetiric with words:\nKing - Man + Woman = Queen\nThis is, in my opinion, completely amazing and fascinating. How does this work? Where do I learn more? Those are my first thoughts. In fact the theory is pretty basic until you get to the nittygritty details, as with most things.\nWE is built on the concept ot learn how words are related to eachother. What company do a word have? To make the example more complex we can redefine this too the following: A is to B what C is to D.\nCurrently there is three \"big\" models that are widely used. The first one Word2Vec (Mikolov et al 2013), the second is GloVe (MIT MIT, Pennington et al 2014) and the final one is fastText (facebook).\nWe will look into how you can achieve this without Deep Learning / Neural Networks unlike the models mentioned.\n\n\nThe first thing we have to do to actually understand/achieve word embeddings is to represent words in a numerical vector. In relation to this a quick explanation of sparse & dense representations would be great. Read more in detail at Wikipedia: Sparse Matrix\nSparse representation is when we represent something very sparsely. It tells us that the points in the space is very few in regards to the dimensions and that most elements are empty. Think one-hot-encoding.\nA Dense representation in comparison has few dimensions in comparison to possible values and most elements are filled. Think of something continuous.\nThe most simple way to represent words in a numerical vector is something we touched earlier, by one-hot-encoding them, i.e. a sparse representation.\n(Source: Marco Bonzanini, 2017)\nBecause of how languages are structured having one-hot-encoding means that we will have an incredibly sparse matrix (can be good) but it will have an enormous amount of dimensions (bad).\nOn top of this how would we go ahead and measure the distance between words? Normally one would use the cosine similarity but if we have a one-hot-encoding all the words would be orthogonal against eachother meaning that the dot-product will be zero.\nCreating a dense representation however would indeed capture similarity as we could make use of cosine-similarity and more. Introducing Word2Vec.\n\n\n\nThe goal of Word2Vec, at least to my understanding, is to actually predict the context of a word. Or in other words we learn embeddings by prediciting the context of the word. The context here being the same definition as in N-grams. Word2Vec uses shallow neural network to learn word vectors so that each word is good at predicting its own contexts (more about his in Skip-Grams) and how to predict a word given a context (more about this in CBOW).\n\n\n\nSkip-gram very simplified is when you train on the N-grams but without the real word. \nAs of now we have empirical results showing how this technique is very successful at learning the meaning of the words. On top of this the embedding that we get has both direction of semantic and syntatic meaning that are exposed in example such as King - Man....\nAnother example would be: Vector(Madrid) - Vector(Spain) + Vector(Sweden) ~ Vector(Stockholm)\n\n\n\nI won’t go into details (some complicated math, see Gittens et al) but if we assume the following to be true:\n\nAll words are distributed uniformly\nThe embedding model is linear\nThe conditional distributions of words are indepedent\n\nThen we can prove that the embedding of the paraphrase of a set of words is obtained by taking the sum over the embeddings of all of the individual words.\nUsing this result it’s easy to show how the famous man-woman, king-queen relationship works.\nExtra note: You can show this then by havingn King and Queen having the same Male-Femalerelationship as the King then is the paraphrase of the set of words {Queen, X}\nI want to note that these assumptions are not 100 percent accurate. In reality word distributions are thought to follow Zipf’s law.\n\n\n\nA year after Word2Vec was a fact to the world the scientist decided to reiterate again. This time we got GloVe. GloVe tried to improve upon Word2Vec by that given a word its relationship(s) can be recovered from co-occurence statistics of a large corpus. GloVe is expensive and memory hungry, but it’s only one load so the issue isn’t that big. Nitty bitty details\n\n\n\nWith fastText one of the biggest problems is solved, both GloVe and Word2Vec only learn embeddings of word of the vocabulary. Because of this we can’t find an embedding for a word that isn’t in the dictionary.\nBojanowski et al solved this by learning the word embeddings using subword information. To summarize fastText learns embeddings of character n-grams instead.\n\n\n\nA simple approach to create your own word embeddings without a neural network is by factorizing a co-occurence matrix using SVD (singular-value-decomposition). As mentioned Word2Vec is barely a neural network as it has no hidden layers nor an y non-linearities. GloVe factorizes a co-occurense matrix while gaining even better results.\nI highly recommend you to go check this blog out: https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ by Stitch Fix. An awesome read and we can go implement this too!"
  },
  {
    "objectID": "posts/2021-05-17-seam-carving.html",
    "href": "posts/2021-05-17-seam-carving.html",
    "title": "Seam Carving (Presentation & Workshop)",
    "section": "",
    "text": "This is from a presentation I did last week (12th of May 2021). Notebook just under the slides!\nPlease note that this requires the Kotlin kernel to run as it’s Kotlin and not Python."
  },
  {
    "objectID": "posts/2021-05-17-seam-carving.html#loading-an-image-with-boofcv",
    "href": "posts/2021-05-17-seam-carving.html#loading-an-image-with-boofcv",
    "title": "Seam Carving (Presentation & Workshop)",
    "section": "Loading an Image with BoofCV",
    "text": "Loading an Image with BoofCV\nLet’s load our first image using BoofCV. It’s simply done using UtilImageIO.loadImage(\"path/to/file\").\nThe conversion to show an image in a notebook is a little awkward.\nBut I talked to Jetbrains through Slack - had a response & patch up within 20 minutes (waiting for the new release right now…) which simplifies this!\nval img = UtilImageIO.loadImage(\"dali.jpg\")\nImage(img.toByteArray(\"jpg\"), \"jpg\").withWidth(\"45%\")\n\n\n\nSalvador Dali Painting"
  },
  {
    "objectID": "posts/2021-05-17-seam-carving.html#sobel-filters",
    "href": "posts/2021-05-17-seam-carving.html#sobel-filters",
    "title": "Seam Carving (Presentation & Workshop)",
    "section": "Sobel Filters",
    "text": "Sobel Filters\nSobel is a very simple Edge Detector that runs the gradient in two directions\n\n  (18.S191 MIT Fall 2020 | Grant Sanderson) By applying this filter\nval grayImg = img.asGrayF32()\nImage(grayImg.asBufferedImage().toByteArray(\"jpg\"), \"jpg\").withWidth(\"45%\")\n\n\n\nS. Dali Painting Gray\n\n\nval grayDY = GrayF32(1,1)\nval grayDX = GrayF32(1,1)\nsobel.process(grayImg, grayDX, grayDY)\nDISPLAY(Image(grayDY.asBufferedImage().toByteArray(\"jpg\"), \"jpg\").withWidth(500))\nImage(grayDX.asBufferedImage().toByteArray(\"jpg\"), \"jpg\").withWidth(500)\n \nNow we need to combine these two into one image, that’s done by taking the intensity, e.g.\n\\(\\sqrt{D_x^2 + D_y^2}\\)\nAlso called l2-norm, euclidean-norm or square-norm.\nWhere \\(D_x\\) is the gradient in X direction (applying sobel-filter).\nLuckily this exists in BoofCV (and most libraries), in the way of GGradientToEdgeFeatures.intensityE(grayDX, grayDY, intensity)\nval intensity = GrayF32(1,1)\nGGradientToEdgeFeatures.intensityE(grayDX, grayDY, intensity)\nImage(intensity.asBufferedImage().toByteArray(\"jpg\"), \"jpg\").withWidth(\"75%\")\n\n\n\nPainting Edges by Filter\n\n\nWith the edges at hands (white) we can go ahead and try to create a graph of total intensity.\nIt is this graph we’ll use to traverse later, to find the cheapest path.\nSo how would we do this?\nTrying all paths would prove expensive, but if we traverse the reverse we can speed things up by being clever.\nLet’s take a look at how a subimage of the image looks.\nintensity.subimage(10,10,20,20).printInt()\n  5   9   2   7   6   6   4   4   5   6 \n  3   7   4   7   4   3   3   7   3   5 \n  9   7   5   6   6   1   5   5   3   6 \n 10   6   7   4   4   4   5   4   6   3 \n  7   7   6   4   5   2   5   8   5   2 \n  4   3   2   2   3   1   3  11   3   1 \n  3   1   7   6   3   3   4   9   4   2 \n  3   5   4   4   7   7   1   6   6   5 \n 10   6   1   4   3   8   4   6   7  10 \n  4   7   3   3   4   5   6   7   7  10 \nAnd now we’ll take a look at how this traversal works. These screens are taken from (18.S191 MIT Fall 2020 | Grant Sanderson) which is really good by the way.\n    \nval tmp = intensity.clone()\nval width = tmp.width\nval height = tmp.height\n\n(height - 2 downTo 0).forEach { y ->\n    (0 until width).forEach { x -> \n        val range = (max(0, x-1)..min(x+1, width - 1))\n        val minimum = range.minOf { x2 -> tmp[x2, y + 1] }\n        tmp[x,y] += minimum\n    }\n}\n\nval max = (tmp.data.maxOrNull() ?: 0f) / 255f\ntmp.data.forEachIndexed { i, _ -> tmp.data[i] /= max }\nImage(tmp.asBufferedImage().toByteArray(\"jpg\"), \"jpg\").withWidth(\"50%\")\n\n\n\nEdges Delta\n\n\nThrough this image we can find the cheapest path by traversing the darkest path, where white is “expensive”.\nThis is simply done by taking the cheapest value at each row, forming a line. We’ve precalculated the whole matrix making this a walk in the park.\nvar previousX = 0 //int[]\nval cheapestPath = IntArray(height) { y ->\n    val range = when(y) {\n        0 -> 0 until width\n        else -> (max(previousX - 1, 0)..min(previousX + 1, width - 1))\n    }\n    previousX = range.minByOrNull { x -> tmp[x,y] } ?: previousX\n    \n    previousX + (y * width)\n}\ncheapestPath.take(10)\n[1022, 2222, 3421, 4621, 5821, 7022, 8223, 9422, 10622, 11821]\nval WHITE = 255f\ncheapestPath.forEach { i -> grayImg.data[i] = WHITE }\nImage(grayImg.asBufferedImage().toByteArray(\"jpg\"), \"jpg\").withWidth(\"75%\")\n\n\n\nThe Best Seam to Remove\n\n\nfun cheapestPath(image: GrayF32): Set<Int> {\n        val widthRange = 0 until image.width\n        for (y in image.height - 2 downTo 0) {\n            for (x in widthRange) {\n                val range = when (x) {\n                    0 -> 0..1\n                    widthRange.last -> x-1..x\n                    else -> x-1..x+1\n                }\n                val cheapestPath = range.minOf { i -> image[i, y + 1] }\n\n                image[x, y] = image[x, y] + cheapestPath\n            }\n        }\n\n        var previousBest = 0\n        return IntArray(image.height) { i ->\n            val range =\n                if (i == 0) 0 until image.width\n                else max(previousBest - 1, 0)..min(previousBest + 1, image.width - 1)\n            previousBest = range.minByOrNull { j -> image.get(j, i) } ?: 0\n\n            previousBest + (i * image.width) // Raw Index\n        }.toSet()\n}\n\nfun seamCarve(img: GrayF32): Set<Int> {\n        sobel.process(img, grayDX, grayDY)\n        GGradientToEdgeFeatures.intensityE(grayDX, grayDY, intensity)\n        return cheapestPath(intensity)\n}\nval grayImg = img.asGrayF32()\n(0..200).forEach { i ->\n    val indices = seamCarve(grayImg)\n    \n    if (i % 25 == 0) {\n        indices.forEach { j -> grayImg.data[j] = WHITE }\n        UtilImageIO.saveImage(grayImg.asBufferedImage(), \"step_$i.jpg\")\n    }\n    grayImg.apply {\n        setData(grayImg.data.removeIndices(indices))\n        reshape(width - 1, height)\n    }\n}\nDISPLAY(Image(\"step_0.jpg\"))\nImage(\"step_200.jpg\")"
  },
  {
    "objectID": "posts/2021-03-17-replace-in-string.html",
    "href": "posts/2021-03-17-replace-in-string.html",
    "title": "When to use what - RegExp, String Replace & Character Replace (JVM/Kotlin)",
    "section": "",
    "text": "Sometimes it’s hard to know what to use, and why to use it even. \n\n\nIn most, or dare I say all, popular programming languages there exists a multitude of string replacements methods, most common is to have one String-based and one RegExp-based. In some languages such as Java there’s also a special method to replace Characters in a String.\n\n\n\nPerformance sometimes matter, sometimes it doesn’t. But if it does it’s really good knowing which method to use as the speed-up can be substantial!\n\n\nReplace “a”, “b” & “c” to “d”. It’s simple, but good. As for data I’m using a few of shakespeares works which in total is 4.5 million characters, I’ve also added variants of these as shown in the table.\n\n\n\n\n\n\n\n\n\n\nType\nLength (characters)\nIterations\nAverage (msg)\nNormalized to RegExp\n\n\n\n\nRegExp\n1k\n1 million\n0.0049ms\n1x\n\n\nChar\n1k\n1 million\n0.0027ms\n0.55x\n\n\nString\n1k\n1 million\n0.0087ms\n1.63x\n\n\n—\n—\n—\n—\n—\n\n\nRegExp\n4.5 million\n1k\n29.67ms\n1x\n\n\nChar\n4.5 million\n1k\n11.84\n0.39x\n\n\nString\n4.5 million\n1k\n57.20\n1.92x\n\n\n—\n—\n—\n—\n—\n\n\nRegExp\n45 million\n10\n361.8ms\n1x\n\n\nChar\n45 million\n10\n117.0ms\n0.32x\n\n\nString\n45 million\n10\n588.1ms\n1.54x\n\n\n\nAs shown the Character-based replace is much faster! It’s only getting faster in comparison to the RegExp the bigger the file is.\nI think a interesting test would be to do character swaps, using these methods and see if it’s retained.\n\n\n\nI’d say that I see a few clear results.\n\nUse Character Based Replace if you only need to replace characters. It’s much faster!\nUse String Based Replace if you only swap one string to another (it’s faster than RegExp), doing multiple swaps grows fast in time consumed.\nUse RegExp Based Replace if you want to swap multiple strings\nUse RegExp Based Replace if you wanna do anything complex really! It’s pretty performant if you remember to compile the pattern :)\n\n\n\n\n\nSome extra comments that are good to know in cases as these\n\n\nI’ve said this before but… Please remember to compile your patterns once, and not in each loop. Compiling patterns is incredibly expensive! Running (1..1_000_000).forEach { str.find(regexStr) } is a multitude slower than\n// pseudo-code\nval regexCompiled = regexStr.toRegex()\n(1..1_000_000).forEach { regexCompiled.find(str) }\nbecause in the first example pattern is compiled each time…\n\n\n\nNote that in Python as an example there exists C-implementations for some methods, it’s very important to actually use these if you care about performance. As an example str.find(keyword) is a multitude slower than keyword in str, because the in keyword is actually a C-implementation when str.find is a python one.\n\n\n\n\nGitHub gist\nimport java.io.File\nimport kotlin.system.measureTimeMillis\n\nobject RegexTester {\n    val text = File(\"/home/londet/git/text-gen-kt/files/shakespeare.txt\").readText()\n    val textSmall = text.take(1000)\n    val textLarge = text.repeat(10)\n\n    val regex = \"[abc]\".toRegex()\n    val charReplace = listOf('a', 'b', 'c')\n    val stringReplace = listOf(\"a\", \"b\", \"c\")\n\n    @JvmStatic\n    fun main(args: Array<String>) {\n        println(\"Warming up JVM by running 10,000 iterations of each replacer on normal size.\")\n        (1..10_000)\n            .forEach { regex.replace(text, \"d\") }\n        (1..10_000)\n            .forEach { charReplace.fold(text) { acc, ch -> acc.replace(ch, 'd') } }\n        (1..10_000)\n            .forEach { stringReplace.fold(text) { acc, ch -> acc.replace(ch, \"d\") } }\n        println(\"Warmup done!\")\n\n\n        val regexSmall = measureTimeMillis { (1..1_000_000).forEach { regex.replace(textSmall, \"d\") } } / 1_000_000.0\n        val regexNormal = measureTimeMillis { (1..1_000).forEach { regex.replace(text, \"d\") } } / 1000.0\n        val regexLarge = measureTimeMillis { (1..10).forEach { regex.replace(textLarge, \"d\") } } / 10.0\n        // val regexLargeCompile = measureTimeMillis { (1..10).forEach { textLarge.replace(\"[abc]\", \"d\") } } / 10.0\n        println(\"Regex Small (1000 characters, 1,000,000 avg): $regexSmall\")\n        println(\"Regex Normal (4.5 million characters, 1000 avg): $regexNormal\")\n        println(\"Regex Large (45 million characters, 10 avg): $regexLarge\")\n\n\n        val charSmall = measureTimeMillis { (1..1_000_000).forEach { charReplace.fold(textSmall) { acc, ch -> acc.replace(ch, 'd') } } } / 1_000_000.0\n        val charNormal = measureTimeMillis { (1..1_000).forEach { charReplace.fold(text) { acc, ch -> acc.replace(ch, 'd') } } } / 1000.0\n        val charLarge = measureTimeMillis { (1..10).forEach { charReplace.fold(textLarge) { acc, ch -> acc.replace(ch, 'd') } } } / 10.0\n        println(\"CharReplace Small (1000 characters, 1,000,000 avg): $charSmall\")\n        println(\"CharReplace Normal (4.5 million characters, 1000 avg): $charNormal\")\n        println(\"CharReplace Large (45 million characters, 10 avg): $charLarge\")\n\n        val stringSmall = measureTimeMillis { (1..1_000_000).forEach { stringReplace.fold(textSmall) { acc, ch -> acc.replace(ch, \"d\") } } } / 1_000_000.0\n        val stringNormal = measureTimeMillis { (1..1_000).forEach { stringReplace.fold(text) { acc, ch -> acc.replace(ch, \"d\") } } } / 1000.0\n        val stringLarge = measureTimeMillis { (1..10).forEach { stringReplace.fold(textLarge) { acc, ch -> acc.replace(ch, \"d\") } } } / 10.0\n        println(\"StringReplace Small (1000 characters, 1,000,000 avg): $stringSmall\")\n        println(\"StringReplace Normal (4.5 million characters, 1000 avg): $stringNormal\")\n        println(\"StringReplace Large (45 million characters, 10 avg): $stringLarge\")\n        \n        /**\n        Regex Small (1000 characters, 1,000,00 avg): 0.004949\n        Regex Normal (4.5 million characters, 1000 avg): 29.671\n        Regex Large (45 million characters, 10 avg): 361.8\n        CharReplace Small (1000 characters, 1,000,00 avg): 0.002752\n        CharReplace Normal (4.5 million characters, 1000 avg): 11.835\n        CharReplace Large (45 million characters, 10 avg): 117.0\n        StringReplace Small (1000 characters, 1,000,00 avg): 0.008692\n        StringReplace Normal (4.5 million characters, 1000 avg): 57.204\n        StringReplace Large (45 million characters, 10 avg): 588.1\n        */\n    }\n}"
  },
  {
    "objectID": "posts/2022-03-13-timeseries-pt-3/index.html#predicting-time-series",
    "href": "posts/2022-03-13-timeseries-pt-3/index.html#predicting-time-series",
    "title": "Forecasting Crypto Prices using Deep Learning (Time Series #3)",
    "section": "Predicting Time Series 📈",
    "text": "Predicting Time Series 📈\nToday we’ll move on from analyzing and using simple models to predict time series to using advanced models and using libraries that simplifies some of the work.\nTo be able to predict the data we must understand it and we’ll make a minor analysis.\n\nInstallation & Imports\n\nfrom IPython.display import clear_output\n!pip install -U pandas_datareader\n!pip install plotly\n!pip install pytorch-lightning\n!pip install -U darts\n!pip install matplotlib==3.1.3\n!pip install pyyaml==5.4.1\n\nclear_output()\n\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np  # linear algebra\nimport pandas_datareader as pdr\nimport seaborn as sns\nfrom darts import TimeSeries\n\nfrom datetime import datetime\n\n/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n  defaults = yaml.load(f)\n\n\n\ndef get_btc_close() -> pd.Series:\n  return pdr.get_data_yahoo('BTC-USD')['Close']\n\ndf = get_btc_close()\nprint(df.head())\n\ndf.plot(y='Close', backend='plotly')\n\nDate\n2017-03-12    1221.380005\n2017-03-13    1231.920044\n2017-03-14    1240.000000\n2017-03-15    1249.609985\n2017-03-16    1187.810059\nName: Close, dtype: float64\n\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nShow Plotly Chart (code cell only visible in active notebook)\n\n\n\n\n\nData Wrangling & Transformation\nLast time we built \\(t_0 .. t_x\\) time steps. This is bad because it makes our memory consumption explode.\nHow can we solve this?\n\n\n\ngenerators meme\n\n\n\nGenerators\nWe can solve it by batching the data and building the batch on-the-fly. This is achieved through use of generators and the yield keyword in Python.\nA lot like a lazy sequence really.\nSee image 👇\n\nBy using this kind of batching we can generate a subset of the dataset at a time which in turn does not blow our memory through the roof and to the moon.\nHow would we implement this in practise?\nTurns out it’s not that hard. You can do it by hand with usual np.ndarray, list or anything, but I choose to use torch.utils.data.Dataset which is the PyTorch dataset. This means that we’ll have data in the same format that we’d feed into our PyTorch-model. 🥳\nFirst we need to implement torch.utils.data.Dataset which is simple in Python;\n\nimport torch\n\nclass TimeseriesDataset(torch.utils.data.Dataset):\n  def __init__(self):\n    pass\n\nThen we need to instantiate it by saving X and y, and a seq_len which is our window-size.\nUsing the self keyword we’ll save the value as a class value.\nInstead of typing our input we could’ve wrapped X and y with torch.tensor to make sure they’re the correct type. But as a fan of types I really prefer this approach, rather than band-aiding it inside the __init__.\n\nclass TimeseriesDataset(torch.utils.data.Dataset):\n    def __init__(self, X: torch.tensor, y: torch.tensor, seq_len: int=1):\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n\nWe’re still missing some crucial methods to make this work in the end, even if Python don’t complain (hey, it’s Python - what did I expect? ¯_ (ツ)_/¯).\n__len__ needs to be implemented to let downstream task consume the dataset. Without a length you won’t know how much data there is.\n\nclass TimeseriesDataset(torch.utils.data.Dataset):\n    def __init__(self, X: torch.tensor, y: torch.tensor, seq_len: int=1):\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n    \n    def __len__(self) -> int:\n        return self.X.__len__() - (self.seq_len - 1)\n\nself.X.__len__() - (self.seq_len - 1) <– What is this sorcery?\nRemember from part #2 where we built our history we had to use pd.DataFrame.dropna, the same has to be done here which means our final dataset is a little bit less than len(X).\nNow there’s a single piece left, __getitem__(self, index) which fetches the element(s).\nFor our use-case we wish to window/slide the data, so we’ll fetch a slice, [a:b], as X and the future element as y.\n\nclass TimeseriesDataset(torch.utils.data.Dataset):\n    def __init__(self, X: torch.tensor, y: torch.tensor, seq_len: int=1):\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n    \n    def __len__(self) -> int:\n        return self.X.__len__() - (self.seq_len - 1)\n    \n    def __getitem__(self, index):\n        return (self.X[index:index + self.seq_len], self.y[index + self.seq_len - 1])\n\nThat’s it, simple right? 🥳\nLet’s test it and validate that this works.\n\nℹ️ torch.roll is the equivalent of pd.DataFrame.shift.\nℹ️ torch.utils.data.DataLoader is PyTorch loader that provides simple batching, multiprocessing and much more automatically!\n\n\ntensor_close = torch.tensor(df)\ntrain_dataset = TimeseriesDataset(tensor_close[:-1], tensor_close.roll(-1)[:-1], seq_len=7)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntrain_loader\n\n<torch.utils.data.dataloader.DataLoader at 0x7f98210aeb90>\n\n\nAnd validating the input\n\nfor batch in train_loader:\n    print(f\"X: {batch[0][:2]}\")\n    print(f\"y: {batch[1][:2]}\")\n    break\n\nX: tensor([[1221.3800, 1231.9200, 1240.0000, 1249.6100, 1187.8101, 1100.2300,\n          973.8180],\n        [1231.9200, 1240.0000, 1249.6100, 1187.8101, 1100.2300,  973.8180,\n         1036.7400]], dtype=torch.float64)\ny: tensor([1036.7400, 1054.2300], dtype=torch.float64)\n\n\nSeems like the math is on point the first element in y is the same as the final element in the second X-tensor. And the second y is nowhere to be found (as that’d be final in the third X-tensor).\n\n\nUsing a library made for Time Series - darts\nDarts allows us to use State-of-the-Art models very easily, just like scikit-learn has a interface for most Machine Learning models.\n\ndf.head()\n\nDate\n2017-03-12    1221.380005\n2017-03-13    1231.920044\n2017-03-14    1240.000000\n2017-03-15    1249.609985\n2017-03-16    1187.810059\nName: Close, dtype: float64\n\n\nThen using TimeSeries.from_* we can load the data into TimeSeries.\n\nts = TimeSeries.from_series(df)\n\ntrain, val = ts.split_before(0.8)\ntrain.plot(label=\"Train\")\nval.plot(label=\"Validation\")\n\n\n\n\nIn darts there’s a plethora of utility functions such as fill_missing_values & add_holidays.\ndarts also make it really simple to do - Multivariate Forecasting.\n- Forecasting with Covariates\n\n💡 Multivariate Forecasting is when you include multiple variables with their history. Predicting a single signal is called Univariate Forecasting.\n💡 Covariates are other things that are known like holiday, I think the image below is very telling.\n\n\n\n\ncovariates-darts\n\n\nUsing SHAP (A game theoretic approach to explain the output of any machine learning model.) you can identify which covariates that affects the result the most. But I’ll leave that for another time.\n\nfrom darts.dataprocessing.transformers import Scaler\nfrom darts.models import NBEATSModel, RNNModel, RandomForest, TCNModel, Prophet\nfrom darts.utils.statistics import check_seasonality, plot_acf\nfrom darts.metrics import mape\n\nFirst we need to scale the data, most models expect the data to be in a good format and having increasingly overly large numbers can be hard to work with.\ndarts provide a Scaler which is like a Transform from scikit-learn.\n\nscaler = Scaler()\n\ntrain_scaled = scaler.fit_transform(train)\ntrain_scaled.plot()\n\n\n\n\nLet’s train a model on this data.\nNBEATS is a really good model and as such let’s use it.\nWhat does the parameters do?\n\n\n\n\n\n\n\nparam\naction\n\n\n\n\ninput_chunk_length\nThis is the “lookback window” of the model- i.e., how many time steps of history the neural network takes as input to produce its output in a forward pass.\n\n\noutput_chunk_length\nThis is the “forward window” of the model - i.e., how many time steps of future values the neural network outputs in a forward pass.\n\n\nrandom_state\nJust as in scikit-learn and other toolkits we wish to have reproducible results, hence we set random_state\n\n\n\n\nfrom darts.models import NBEATSModel, RNNModel, Prophet, RandomForest, TCNModel, TFTModel\n\nmodel = NBEATSModel(input_chunk_length=7, output_chunk_length=1, random_state=42,)\nmodel.fit(train_scaled, epochs=10)\n\n[2022-03-11 14:31:44,193] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 1453 samples.\n[2022-03-11 14:31:44,193] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 1453 samples.\n[2022-03-11 14:31:44,663] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 64-bits; casting model to float64.\n[2022-03-11 14:31:44,663] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 64-bits; casting model to float64.\nGPU available: True, used: False\nTPU available: False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\n/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1585: UserWarning:\n\nGPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n\n\n  | Name      | Type       | Params\n-----------------------------------------\n0 | criterion | MSELoss    | 0     \n1 | stacks    | ModuleList | 6.1 M \n-----------------------------------------\n6.1 M     Trainable params\n1.3 K     Non-trainable params\n6.1 M     Total params\n48.490    Total estimated model params size (MB)\n\n\n\n\n\n<darts.models.forecasting.nbeats.NBEATSModel at 0x7f980cde79d0>\n\n\nNow that the model is trained we wish to do a historical_forecasts to validate how it would’ve done on the validation data.\nLet’s go ahead!\n\nval_scaled = scaler.transform(val)\n\n\n%%capture\npreds = model.historical_forecasts(\n    val_scaled, start=0.1, forecast_horizon=1, retrain=False\n)\n\n# scale back:\npreds = scaler.inverse_transform(preds)\n\n\nval.plot(label=\"actual\")\npreds.plot(label=\"predicted\")\n\n\n\n\nTry using different forecasting, like forecast_horizon=7.\nTo make it even more interesting you should reshape the model to use output_chunk_length=7, which should mean it’s better at predicting further into the future as that target has been “developed” during training.\nTry new models like RNNModel, Prophet (by Facebook), TCNModel (Temporal Convolutional Neural Network), TCTModel (Temporal Fusion Transformer) or our old buddy RandomForest.\nFind more models in the docs."
  },
  {
    "objectID": "posts/2022-03-13-timeseries-pt-3/index.html#pytorch",
    "href": "posts/2022-03-13-timeseries-pt-3/index.html#pytorch",
    "title": "Forecasting Crypto Prices using Deep Learning (Time Series #3)",
    "section": "PyTorch",
    "text": "PyTorch\nWe should not only have fun with pre-built libraries but it’d be nice to try building this by hand using PyTorch.\nI’ll dump the code, but walk it through right below on what and why.\nFirst we’ll define our class\nclass RNNModel(pl.LightningModule):\nWhich in our case is a pytorch-lightning (pl) one, pl is a very thin wrapper on top of PyTorch that automate some mundane tasks, but still makes it easy to configure them by hand as I’ll show.\nThen we’ll define our __init__:\nclass RNNModel(pl.LightningModule):\n  def __init__(self, \n                 n_features, \n                 hidden_size, \n                 seq_len, \n                 batch_size,\n                 num_layers, \n                 dropout, \n                 learning_rate,\n                 criterion):\n        super(RNNModel, self).__init__()\n        self.n_features = n_features\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.criterion = criterion\n        self.learning_rate = learning_rate\n\n        self.lstm = nn.LSTM(input_size=n_features, \n                            hidden_size=hidden_size,\n                            num_layers=num_layers, \n                            dropout=dropout, \n                            batch_first=True)\n        self.linear = nn.Linear(hidden_size, 1)\nThat’s a lot to chew! 😅\nLet’s walk it through,\n\n\n\n\n\n\n\nargument\nwhat it does\n\n\n\n\nhidden_size\nwidth of the RNN (e.g. cells)\n\n\nnum_layers\nthe number of layers of RNNs\n\n\ndropout\nthe dropout probability between the layers in the RNN, requires >= 2 layers\n\n\nseq_len\nthe window/history size\n\n\nlearning_rate\nthe learning rate\n\n\ncriterion\nthe loss function\n\n\n\nSeems OK right?\nIn the __init__ we defined all our parts required to run the neural network, but we need to define how to run it. That’s what we define forward to do, and the backward-pass is automatically done for us.\ndef forward(self, x):\n  # lstm_out = (batch_size, seq_len, hidden_size)\n  lstm_out, _ = self.lstm(x)\n  y_pred = self.linear(lstm_out[:,-1])\n  return y_pred\nFirst we run our data through the LSTM, then our linear/dense layer to retrieve a single output. Sounds good?\nAnd that’s really all that’s needed for a PyTorch-model. But because I chose to use pytorch-lightning to simplify our training loop we need a little more:\ndef configure_optimizers(self):\n  return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n\ndef predict_step(self, batch, batch_idx, dataloader_idx):\n  x,y = batch\n  return self(x)\n\ndef training_step(self, batch, batch_idx):\n  x, y = batch\n  y_hat = self(x)\n  loss = self.criterion(y_hat, y)\n  self.log('train_loss', loss)\n  \n  return loss\nFirst we define our optimizer to be Adam in configure_optimizers.\nThen we define how to predict, e.g. only splitting our batch. predict_step is defined by default to simply run forward which does not fit our dataloaders.\nFinally we define training_step which explains how to run training. On top of this I define testing_step and validation_step to do the exact same except for the logging.\n\n💡the self.log will automatically allow us to log everything with Tensorboard – cool right?\n\n\nRNNModel PyTorch\nRun the two cells below that contains the pl.LightningModule and our PyTorch Dataset.\n\nimport pytorch_lightning as pl\nfrom torch import nn\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass RNNModel(pl.LightningModule):\n    def __init__(self, \n                 hidden_size, \n                 seq_len, \n                 batch_size,\n                 num_layers, \n                 dropout, \n                 learning_rate,\n                 criterion):\n        super(RNNModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.criterion = criterion\n        self.learning_rate = learning_rate\n\n        self.lstm = nn.LSTM(input_size=1, \n                            hidden_size=hidden_size,\n                            num_layers=num_layers, \n                            dropout=dropout, \n                            batch_first=True)\n        self.linear = nn.Linear(hidden_size, 1)\n        \n    def forward(self, x):\n        # lstm_out = (batch_size, seq_len, hidden_size)\n        lstm_out, _ = self.lstm(x)\n        y_pred = self.linear(lstm_out[:,-1])\n        return y_pred\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n      x,y = batch\n      return self(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('train_loss', loss)\n        \n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('val_loss', loss)\n        \n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        self.log('test_loss', loss)\n\n        return loss\n\n\nclass TimeseriesDataset(torch.utils.data.Dataset):   \n    '''\n    Custom Dataset subclass. \n    Serves as input to DataLoader to transform X \n      into sequence data using rolling window. \n    DataLoader using this dataset will output batches \n      of `(batch_size, seq_len, n_features)` shape.\n    Suitable as an input to RNNs. \n    '''\n    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 7):\n        self.X = torch.tensor(X).float()\n        self.y = torch.tensor(y).float()\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return self.X.__len__() - (self.seq_len - 1)\n\n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])"
  },
  {
    "objectID": "posts/2022-03-13-timeseries-pt-3/index.html#the-datamodule",
    "href": "posts/2022-03-13-timeseries-pt-3/index.html#the-datamodule",
    "title": "Forecasting Crypto Prices using Deep Learning (Time Series #3)",
    "section": "The DataModule",
    "text": "The DataModule\nThis step is not really a requirement but rather a show-case of how to create a pl.LightningDataModule which contains all your code to validate different models simpler as you only need to supply your datamodule to do everything.\nLet me walk us through it.\nclass BitcoinDataModule(pl.LightningDataModule):\n    \n    def __init__(self, seq_len = 7, batch_size = 128, num_workers=0):\n      # add arguments\nDefining our class and __init__.\nWe then need to define our setup which loads the data and our dataloaders, which is done in the following sense:\ndef setup(self, stage=None):\n  X = df[:-1]\n  y = df.shift(-1)[:-1]\n\n  X_cv, X_test, y_cv, y_test = train_test_split(\n      X, y, test_size=0.2, shuffle=False\n  )\n\n  X_train, X_val, y_train, y_val = train_test_split(\n      X_cv, y_cv, test_size=0.25, shuffle=False\n  )\n\n  preprocessing = StandardScaler()\n  preprocessing.fit(X_train)\n\n  self.X_train = preprocessing.transform(X_train)\n  self.y_train =  preprocessing.transform(y_train).reshape((-1, 1))\n  self.X_val = preprocessing.transform(X_val)\n  self.y_val = preprocessing.transform(y_val).reshape((-1, 1))\n\ndef train_dataloader(self):\n  train_dataset = TimeseriesDataset(self.X_train, \n                                    self.y_train, \n                                    seq_len=self.seq_len)\n  train_loader = DataLoader(train_dataset, \n                            batch_size = self.batch_size, \n                            shuffle = False, \n                            num_workers = self.num_workers)\n  \n  return train_loader\n\ndef val_dataloader(self):\n  # repeat train_dataloader\nThis is rather simple, even if it’s a lot of code.\n\nDataModule definition\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader\n\nclass BitcoinDataModule(pl.LightningDataModule):\n    '''\n    PyTorch Lighting DataModule subclass:\n    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n\n    Serves the purpose of aggregating all data loading and processing work in one place.\n    '''\n    \n    def __init__(self, seq_len = 7, batch_size = 128, num_workers=0):\n        super().__init__()\n        self.seq_len = seq_len\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.X_train = None\n        self.y_train = None\n        self.X_val = None\n        self.y_val = None\n        self.X_test = None\n        self.X_test = None\n        self.preprocessing = None\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None):\n        if stage == 'fit' and self.X_train is not None:\n            return \n        if stage == 'test' and self.X_test is not None:\n            return\n        if stage is None and self.X_train is not None and self.X_test is not None:  \n            return\n        \n        X = df[:-1].to_numpy().reshape(-1, 1)\n        y = df.shift(-1)[:-1].to_numpy().reshape(-1, 1)\n\n        X_cv, X_test, y_cv, y_test = train_test_split(\n            X, y, test_size=0.2, shuffle=False\n        )\n    \n        X_train, X_val, y_train, y_val = train_test_split(\n            X_cv, y_cv, test_size=0.25, shuffle=False\n        )\n\n        preprocessing = StandardScaler()\n        preprocessing.fit(X_cv)\n\n        if stage == 'fit' or stage is None:\n            self.X_train = preprocessing.transform(X_train)\n            self.y_train =  preprocessing.transform(y_train).reshape((-1, 1))\n            self.X_val = preprocessing.transform(X_val)\n            self.y_val = preprocessing.transform(y_val).reshape((-1, 1))\n\n        if stage == 'test' or stage is None:\n            self.X_test = preprocessing.transform(X_test)\n            self.y_test = preprocessing.transform(y_test).reshape((-1, 1))\n        \n\n    def train_dataloader(self):\n        train_dataset = TimeseriesDataset(self.X_train, \n                                          self.y_train, \n                                          seq_len=self.seq_len)\n        train_loader = DataLoader(train_dataset, \n                                  batch_size = self.batch_size, \n                                  shuffle = False, \n                                  num_workers = self.num_workers)\n        \n        return train_loader\n\n    def val_dataloader(self):\n        val_dataset = TimeseriesDataset(self.X_val, \n                                        self.y_val, \n                                        seq_len=self.seq_len)\n        val_loader = DataLoader(val_dataset, \n                                batch_size = self.batch_size, \n                                shuffle = False, \n                                num_workers = self.num_workers)\n\n        return val_loader\n\n    def test_dataloader(self):\n        test_dataset = TimeseriesDataset(self.X_test, \n                                         self.y_test, \n                                         seq_len=self.seq_len)\n        test_loader = DataLoader(test_dataset, \n                                 batch_size = self.batch_size, \n                                 shuffle = False, \n                                 num_workers = self.num_workers)\n\n        return test_loader"
  },
  {
    "objectID": "posts/2022-03-13-timeseries-pt-3/index.html#training-our-model",
    "href": "posts/2022-03-13-timeseries-pt-3/index.html#training-our-model",
    "title": "Forecasting Crypto Prices using Deep Learning (Time Series #3)",
    "section": "Training our Model",
    "text": "Training our Model\nLet’s move on to the fun part! First we define our input values such as dropout, criterion and more.\n\nseq_len = 7\nbatch_size = 256\ncriterion = nn.MSELoss()\nmax_epochs = 300\nhidden_size = 56\nnum_layers = 2\ndropout = 0.2\nlearning_rate = 1e-3\n\nThen we define our trainer, model & dm and in the end do a fit.\n\ntrainer = pl.Trainer(max_epochs=max_epochs, gpus=1, log_every_n_steps=4)\n\nmodel = RNNModel(\n    hidden_size = hidden_size,\n    seq_len = seq_len,\n    batch_size = batch_size,\n    criterion = criterion,\n    num_layers = num_layers,\n    dropout = dropout,\n    learning_rate = learning_rate\n)\n\ndm = BitcoinDataModule(\n    seq_len = seq_len,\n    batch_size = batch_size\n)\n\ntrainer.fit(model, dm)\nclear_output()\ntrainer.test(model, dm)\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_loss': 3.8363118171691895}\n--------------------------------------------------------------------------------\n\n\n[{'test_loss': 3.8363118171691895}]\n\n\nHow does this look in the TensorBoard?\n\n%load_ext tensorboard\n%tensorboard --logdir=lightning_logs/\n\n\n\n\nAnd let’s validate how good our predictions are. Please note that we trained for 300 epochs with not a lot of data, running perhaps 500 should yield bettwer results.\nBut I’ll leave that for you to play around with 😉\n\npredictions_all_batches = trainer.predict(model, dataloaders=dm.val_dataloader())\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n\n\n\n\n\n\npreds = torch.cat(predictions_all_batches)\n\ntrue = []\nfor _, y_true in dm.val_dataloader():\n  true += y_true\n\ntrue = torch.cat(true)\n\npd.DataFrame({'Preds': preds.flatten(), 'True': true.flatten()}).plot(backend=\"plotly\")\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nShow Plotly Chart (code cell only visible in active notebook)"
  },
  {
    "objectID": "posts/2022-03-13-timeseries-pt-3/index.html#extra-material",
    "href": "posts/2022-03-13-timeseries-pt-3/index.html#extra-material",
    "title": "Forecasting Crypto Prices using Deep Learning (Time Series #3)",
    "section": "Extra Material",
    "text": "Extra Material\nDo you like the fast.ai-approach? Then make sure to check out the awesome tsai! It contains a lot of the SotA-models.\nDo you wish for another PyTorch approach? Then check out pytorch-forecasting which is also available in lightning-flash.\nDo you wish for a third (and really awesome ❗) approach with PyTorch? Then make sure to research neuralforecast which actually includes the latest models such as Informer.\nThat’s all for these three posts, have a great time exploring!\n~Hampus Londögård"
  },
  {
    "objectID": "posts/2022-11-30-why-polars.html",
    "href": "posts/2022-11-30-why-polars.html",
    "title": "Polars - A Refreshingly Great DataFrame Library",
    "section": "",
    "text": "While working at AFRY we’ve noted that in performance intensive application that isn’t really Big Data ends up being slow when using pandas.\nComing from languages such as Scala, Kotlin & golang we knew there had to be more to it. There was a lot of performance to be squeezed! 🏎️\nCherry on the top? The pandas API is a constant source of confusion and thereby not very satisfying. I end up having to read/search the documentation more times than I care to admit. All in all a cleaner and more efficient tool was needed to handle our data & model training pipelines.\nOne day I stumbled upon polars - an blazing fast DataFrame-library written in Rust. Plenty of buzzwords, documentation and user-guide later I was ready to trial it in a personal project. 🤠\nIt was a smooth addition because of the pandas integration, pl.from_pandas & df.to_pandas(), which in turn made it a gradual adoption. The trial was an instant success, moving DataFrame’s to and from polars was diminished by the fact that polars sped up my pipeline so much. And the code was clean, the API more natural, only downgrade was a bit less reading options - otherwise only upgrades! 🤯\nI was ready to trial it work, and boy was I in for a wonderful journey!\nAfter gradually adopting it in one of our client project we saw huge speedups (some parts being >3 magnitudes (!) faster) and our code became a lot simpler. Additionally something I didn’t expect: we decoupled our code in a more efficient way producing leaner code that’s more testable! 🦸‍♂️\nThen… what the actual fudge is polars?"
  },
  {
    "objectID": "posts/2022-11-30-why-polars.html#in-production",
    "href": "posts/2022-11-30-why-polars.html#in-production",
    "title": "Polars - A Refreshingly Great DataFrame Library",
    "section": "In Production",
    "text": "In Production\nWe use polars extensively in production and after evaluating we found:\n\nPipelines to be 2x-20x faster, averaging about 7x\nSimpler pipelines\nEasier testing of pipelines\n\nWhich is some pretty fantastic gains!"
  },
  {
    "objectID": "posts/2022-11-30-why-polars.html#future",
    "href": "posts/2022-11-30-why-polars.html#future",
    "title": "Polars - A Refreshingly Great DataFrame Library",
    "section": "Future",
    "text": "Future\nI see a bright future with polars as it enables workloads which previously required to run in the cloud to be able to run locally, because the efficiency is so high."
  },
  {
    "objectID": "posts/2022-11-30-why-polars.html#bonus",
    "href": "posts/2022-11-30-why-polars.html#bonus",
    "title": "Polars - A Refreshingly Great DataFrame Library",
    "section": "Bonus",
    "text": "Bonus\npolars is more than a “simpler API” and “faster pandas” with its additional functionality. Ever heard of over? Not? Let me tell you a cool story!\n\npl.Over\npl.col(age).mean().over(gender) is like pd.groupBy(gender).transform({age: \"mean\"}) but way more expressive and powerful!\nIt can be used to build columns, filter DataFrame and anything really. We can combine multiple of them in the same select:\ndf.select([\n  pl.col(age).mean().over(gender),\n  pl.col(height).mean().over(gender),\n  pl.col([age, height]).over([gender, species])\n])\nThe first and second row of the select uses the same grouper, query optimizer yay!\nThe third line does it over multiple columns, combining all this into one single select is some pretty powerful stuff! 🦸‍♂️\n\n\npl.Fold\nYet another incredibly powerful piece of operation is the fold which most Scala- or FP-programmers will know and love. fold is a more powerful reduce as it allows us to define what type we’d like to accumulate.\nThe simplest example is using fold as a reduce to calculate the sum, e.g.\nout = df.select(\n    pl.fold(acc=pl.lit(0), f=lambda acc, x: acc + x, exprs=pl.col(\"*\")).alias(\"sum\"),\n)\nWhich is an obvious overkill solution, but allowing to aggregate expressions with conditionals is an inredibly powerful concept which can yield the best types of expressions.\nout = df.filter(\n    pl.fold(\n        acc=pl.lit(True),\n        f=lambda acc, x: acc & x,\n        exprs=pl.all() > 1,\n    )\n)\nIn this expression we filter that every coluumn is larger than 1.\nThat’s it for this small article. ~Hampus Londögård"
  },
  {
    "objectID": "posts/2020-06-01-sqldelight-kotlin.html",
    "href": "posts/2020-06-01-sqldelight-kotlin.html",
    "title": "SQL - Different Abstraction Levels (& how I came to love SQLDelight)",
    "section": "",
    "text": "SQL - different abstraction levels and how I came to love SQLDelight\nIn this blog I’ll cover a few different abstraction levels of database access, focusing purely on SQL and not NoSQL / Reddis or anything like that. The purpose is to share the knowledge that there exist these types of abstractions and they do exist in all or at least most of the popular languages.  I’ll try to move from \"raw SQL\" to the modern \"Object-Relational Mapping\"-style, a.k.a ORM.\nIn the end I wish to make a short piece leaving out a lot of details but maintaining a feel of each style and some pros/cons. I bet you already guessed my preferred approach straight from the title :wink:.\n\nHow to interact with a SQL Database from a programming language\nStructured Query Language (SQL) is as the name, once spelled out, a Domain Specific Language (DSL) just like regex. It’s basically a programming language written to facilitate and simplify the experience with the underlying engine. By using a DSL you gain capabilities that would be natural to integrate with most languages, and it also makes the engine do the same with the same code across languages.\nI think that Regex and SQL are the most famous DSLs and for good reason, having regex work (almost) the same across languages simplifies the guides and the same applies to SQL.\nGoing forward let’s see how we communicate with a SQL-db from a programming language like Java using their famous jdbc (Java Database Connectivity) which is the driver that communicates with the db.\ntry {\n      System.out.println(\\\"Connecting to database...\\\");\n      conn = DriverManager.getConnection(DB_URL,USER,PASS);\n\n      //STEP 4: Execute a query\n      System.out.println(\\\"Creating statement...\\\");\n      stmt = conn.createStatement();\n      String sql;\n      sql = \\\"SELECT id, first, last, age FROM Employees\\\";\n      ResultSet rs = stmt.executeQuery(sql);\n\n      //STEP 5: Extract data from result set\n      while(rs.next()){\n         //Retrieve by column name\n         int id  = rs.getInt(\\\"id\\\");\n         int age = rs.getInt(\\\"age\\\");\n         String first = rs.getString(\\\"first\\\");\n         String last = rs.getString(\\\"last\\\");\n\n         //Display values\n         System.out.print(\\\"ID: \\\" + id);\n         System.out.print(\\\", Age: \\\" + age);\n         System.out.print(\\\", First: \\\" + first);\n         System.out.println(\\\", Last: \\\" + last);\n      }\n      //STEP 6: Clean-up environment\n      rs.close();\n      stmt.close();\n      conn.close();\n   } catch (SQLException se) {\n    ....\nNot very convenient right? Personally I think this looks horrible, it’s filled with horrible getters & setters like we’re stuck in the Middle Ages or something. Personally my mind directly flows to serialization and how that must work somehow with databases, and that’s right - we can move into the future today!\n\n\nMoving one abstraction level up\nWelcome Room & slick (two libraries I’ve experience with) to the room! Both of these libraries provide a type of serialization to classes and more convenient syntax to write the code. The first one heavily leans on annotation to make it work while the other one uses a more slick approach of \"copying\" the way you work with the standard Scala Collections (filter, map, flatMap, reduce etc).\nI’d say that both do count as ORMs but they’re still not as abstract as other solutions such as peewee which we’ll discuss later. Let’s get into Room and how it works. First you define entities like a class with the added annotation @Entity and then you define a Data Access Object (DAO) to interact with the table / object. The DAO is where you define your queries, let’s take a look.\n@Dao\ninterface UserDao {\n    @Query(\\\"SELECT * FROM user\\\")\n    fun getAll(): List<User>\n\n    @Query(\\\"SELECT * FROM user WHERE uid IN (:userIds)\\\")\n    fun loadAllByIds(userIds: IntArray): List<User>\n...\n}\nIn my opinion this approach strikes a really good balance between simple-to-use but still powerful and very configurable because you still use SQL, a bonus here is that it’s safe from SQL-injection as you’re making use of so-called prepared-statements (wikipedia). The biggest drawback is that it’s hard to write easy-to-read SQL in the annotation and for the annotation-haters we’ve a lot of annotations (which often slows down the compile-time noticeably among other things).\nMoving on we’ve slick which is also a really cool approach! slick allows you to this but instead you write your queries in something that feels like using the normal Scala Collection library. This allows you to use map, filter, reduce etc to create queries, and even for-comprehension. Let’s see!\n// Read all coffees and print them to the console\nprintln(\\\"Coffees:\\\")\ndb.run(coffees.result).map(_.foreach {\n  case (name, supID, price, sales, total) =>\n    println(\\\"  \\\" + name + \\\"\\\\t\\\" + supID + \\\"\\\\t\\\" + price + \\\"\\\\t\\\" + sales + \\\"\\\\t\\\" + total)\n})\n\n// Read coffee with price lower than 9 and join with matching supplier using for-comprehension\nval q2 = for {\n  c <- coffees if c.price < 9.0\n  s <- suppliers if s.id === c.supID\n} yield (c.name, s.name)\n\n// A find using filter\ndef find(id: Int) = db.run(\n  users\n    .filter(_.id === id)\n    .result.headOption\n  )\nPretty slick right?\n\n\nMoving another level up (Python + Peewee)\nOk, maybe it’s not actually moving one level up from slick but I’d say it’s still a little bit further away from raw SQL as we make more use of objects, in the case of slick you can more easily see the generated SQL-code. Let’s take a look at peewee which supports most databases (sqlite, mysql, postgresql and cockroachdb).\nSo where do we begin? Create the database and tables! It’s done by initiating a database and then creating different classes which each maps to their own tables automatically.\ndb = SqliteDatabase('people.db') # create the db\n\nclass Person(Model):\n    name = CharField()\n    birthday = DateField()\n\n    class Meta:\n        database = db # This model uses the \\\"people.db\\\" database.\n\nclass Pet(Model):\n    owner = ForeignKeyField(Person, backref='pets')\n    name = CharField()\n    animal_type = CharField()\n\n    class Meta:\n        database = db # this model uses the \\\"people.db\\\" database\nAnd how would one create entries and then query them? It’s simply done through object creation as in the following examples.\nuncle_bob = Person(name='Bob', birthday=date(1960, 1, 15))\nuncle_bob.save()\n# Sometimes the class already has a \\\"create method\\\" as in\nPerson.create(name='Sarah', birthday=date(1980, 10, 20))\n\n# And create a pet which belongs to uncle_bob\nbob_dog = Pet.create(owner=uncle_bob, name='Doggy', animal_type='dog')\nAnd to query the tables we also make use of the object fully, as in the following small example.\nbobby = Person.select().where(Person.name == 'Bob').get()\n# or all persons!\nfor person in Person.select():\n    print(person.name)\nNow we’ve gone through the different abstraction layers that you usually see available in most languages. Going forward I’d like to show SQLDelight which turns the abstraction a little bit upside down.\n\n\nSQLDelight: Abstraction level left to the right\nIn SQLDelight I’d say we get the ideal balance of abstraction and configurability. We deal with raw SQL which is both a pro & con, people will need to know SQL unlike in a abstracted ORM but you also get the full potential and it’s really simple to do complex joins (which is really messy in ORMs).\nI was delighted at how simple it was to use from my Kotlin code while also providing a simple way to write my DB-interactions. No confusion and there’s a million guides out there showing how you write SQL code for complex joins if you ever need a hand.\nLet’s begin with how you define a table and queries, through a so-called .sq-file.\n-- .sq-file\n\nCREATE TABLE person (\n  name TEXT NOT NULL,\n  birthday DATE NOT NULL\n);\n-- You can actually also insert a Person directly in this file if you'd like using the normal SQL insert statement.\n\nselectAll:\nSELECT *\nFROM person;\n\ninsert:\nINSERT INTO person(name, birthday)\nVALUES (?, ?);\n\ninsertPerson:\nINSERT INTO person(name, birthday)\nVALUES ?;\nFor those that don’t know SQL this does the following\n\nDefine the table\nCreate queries on the table\n\nThese queries makes use of the custom format methodName: and then define the method using the SQL code beneath until it hits end ;.\n\n\nNow we have some SQL code defined in a .sq-file, how do we actually use this from our Kotlin-code? We build the project, while building the project the code is generated to our build project with the Kotlin-code. It’ll provide\n\nData Classes (like structs / objects / case classes)\nQueries for each table\n\nAnd on top of this you’ll have full typing, which is pretty damn awesome! Let’s take a look at how we’d use this from Kotlin.\n// Not optimal code, should use injection or something in reality for the db.\nval database = Database(driver)\n\nval personQueries: PersonQueries = database.personQueries\n\nprintln(personQueries.selectAll().executeAsList())\n// Prints []\n\npersonQueries.insert(name = \\\"Bob\\\", birthday = Date(2010, 1, 10))\nprintln(personQueries.selectAll().executeAsList())\n// Prints [Person.Impl(\\\"Bob\\\", Date(2010, 1, 10))]\n\nval person = Person(\\\"Ronald McDonald\\\", Date(2020, 1, 5))\npersonQueries.insertPerson(person)\nprintln(personQueries.selectAll().executeAsList())\n// Prints [Person.Impl(\\\"Bob\\\", Date(2010, 1, 10)), Person.Impl(\\\"Ronald McDonald\\\", Date(2020, 1, 5))]\nLet me just say, I’m amazed about this kind of reverse thinking of generating code from SQL. It gives us the convenience of a ORM but the flexibility of raw SQL :happy:.\n\n\nComparison Table\n\n\n\n\n\n\n\n\n\n\n\nDatabase\nSimplicity\nRequires SQL knowledge\nConfigurability (complex queries etc)\nScore (5)\nComment\n\n\n\n\nJDBC\nI\nIII\nIII\n2\nTo much overhead\n\n\nRoom / Slick\nII\nII\nII\n4\nStrikes a good balance between natural in normal code while configurable*\n\n\nPeewee\nIII\nI\nI\n3\nReally easy and fits into code great, but the complex queries becomes really hard and feels forced\n\n\nSQLDelight\nII\nIII\nIII\n5\nNatural to use in the code, great customability & little overhead*\n\n\n\nBoth Room & SQLDelight are enforcing SQLite right now which is a major con for those that needs postgresql etc. Personally I only use SQLite as was discussed in expensify’s blog SQLite can be squeezed to the extreme - expensify managed to handle up to 4 million queries per second!\n\n\nOutro\nIn its essence today there’s a great variety of different kinds of wrappers for databases in almost all languages and it is all about finding one that strikes your balance of perfect. For a really simple database perhaps an ORM such as peewee where no SQL knowledge is really required could be enough. But be sure to know the trade-offs, once your database grows complex so does peewee grow complex fast, same applies to slick and others. Raw SQL as a fall-back is always good to have and a lot of the libraries are starting to add it (e.g. slick), but it never feels natural and always is a bit like a bandaid, ugly right?\nAnyhow, I hope this was interesting and perhaps someone learned about a new abstraction-level for databases or was inspired to pick up their own.\n~Hampus"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "",
    "text": "A three part blog (all included in this one) that goes through\nAll the code is available here.\nIt’s highly recommended using IntelliJ, a free (community) edition can be downloaded from jetbrains.com.\nIt’s also required that your Kotlin version is above 1.4 (some huge Multiplatform changes was added in release 1.4).\nAll the finished code is available here (open master if you wish to have the unfinished code)."
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#llvm",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#llvm",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "LLVM",
    "text": "LLVM\nLLVM is probably the biggest project (compiler) that exists to build native binaries. Languages such as C, C++, Haskell, Rust & Swift compile into native binaries through LLVM.\nFrom the info-box previously,\n\nIt is an LLVM based backend for the Kotlin compiler and native implementation of the Kotlin standard library.\n\nSo… What is a backend? More specifically, what is a backend for a compiler?"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#how-the-kotlin-compiler-works-frontend-to-backend",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#how-the-kotlin-compiler-works-frontend-to-backend",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "How the Kotlin Compiler works, Frontend to Backend",
    "text": "How the Kotlin Compiler works, Frontend to Backend\nA compiler is like a translator, just as you’d translate Swedish into English a compiler instead translates computer code written in one programming language into another one of lower level, e.g. assembly.\nIn general all compilers follow the same pattern, and Kotlin is no different. Even though it’s a similar path it’s interesting to learn about, even more if you don’t know how it usually works!\nThe Kotlin Compiler first compiles Kotlin code into a Intermediate Representation, or IR, which it later turns into Java Bytecode, when targeting the Java Virtual Machine (JVM).\n\n\n\nCompiler Chain\n\n\nThe first part is called the Compiler Frontend\n\nThe Kotlin Compiler Frontend\n\n\n\nCompiler Frontend\n\n\nThe Compiler Frontend turns the Kotlin code into a Intermediate Representation of the code which is represented by a abstract syntax tree. The abstract syntax tree in turn is built from concrete syntax, e.g. strings.\nThe process involves lexical analysis which creates tokens and pass it forward to the parser that finally builds the abstract syntax tree.\nFor those interested this could be a really fun challenge and learning lesson to implement yourself!\nMoving on, the second and final part is called the Compiler Backend.\n\n\nThe Kotlin Compiler Backend\n\n\n\nCompiler Backend\n\n\nThe Compiler Backend turns this abstract syntax tree, or IR, into computer output language.\nIn the image that is Java Bytecode which is understood by the JVM. The backend is the part that actually optimize code to remove for-loops where applicable, exchange variables into constants and so on.\nJust as with the frontend it’s a really good challenge to either implement a backend without optimizations, or focus on a existing one and adding a optimization yourself!\nWhat is interesting about Kotlin is that it has different compiler backends, which means that the IR compile not only into Java Bytecode but also JS/Browser & Native binaries.\n\n\n\nDifferent Kotlin Compiler Backends\n\n\n\nSide-note: For Native Backend there’s in fact two Intermediate Representations, first Kotlin IR which then compiles into LLVM IR. LLVM finally turns this into a native binary through its own Compiler Backend.\nDuring the final step in LLVM all the optimizations applied to C, C++, Swift & many more is also applied to Kotlin Native code!"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#how-kotlin-keeps-multiplatform-clean",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#how-kotlin-keeps-multiplatform-clean",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "How Kotlin keeps multiplatform clean",
    "text": "How Kotlin keeps multiplatform clean\nIt might sound messy to target multiple platforms like this, and how could it possibly end up clean?\nBy using the standard libraries that are included with Kotlin, which includes almost everything you need, and multiplatform-developed community libraries, e.g. SQLDelight, you get code that looks the same and works the same irregardless of target (JS/Browser, Native or the JVM).\nTo give an example of how Kotlin std-lib works, let’s take one of the most common types - String.\n\n\n\nHow Kotlin types are compiled\n\n\nBy using Kotlin.String rather than the usual Java.lang.String you do when programming Java you get a type that works on multiple platforms, including some awesome convenience functions. Imagine, you can write native code using .substring, .take(n) & .replace - amazing compared to c right? :happy:\nTo put this in context of the compiler, this means that the Compiler Backend automatically maps the IR of a Kotlin.String into the correct type.\nYou can take this concept and apply to anything such as IO , network & more - all which are included in the std-lib!"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#summarizing-how-multiplatform-works",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#summarizing-how-multiplatform-works",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "Summarizing how Multiplatform works",
    "text": "Summarizing how Multiplatform works\nLet’s recollect what we’ve gone through\n\nKotlin Compiler compiles Kotlin code into a Intermediate Representation (IR) through the Compiler Frontend.\n\nIR is a abstract syntax tree.\n\nKotlin Compiler then goes the Compiler Backend which turns the code into the lower level language, e.g. Java Bytecode, and applies multiple optimisations.\nKotlin has a std-lib which has functionalities as Kotlin.String, Kotlin.List, networking and much more.\n\nKotlin.String turns into KString in the case of targeting Native. KString is Kotlins own native strings with some cool helper methods.\n\n\nNative is usually considered “dangerous” and “hard” because of all the quirks like pointers, address space and other.\nKotlin Native deals with memory allocation in the same way as Swift, namely reference counting which deallocates objects once they’ve got no references. There’s some advantages such as being really fast, but also downsides such as reference cycles which it handles poorly when compared to the JVM Garbace Collector (GC).\nKotlin also has some really nice convenience syntax such as the memScope-block."
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#outro-kotlin-multiplatform-and-why-it-matters",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#outro-kotlin-multiplatform-and-why-it-matters",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "Outro: Kotlin Multiplatform and why it matters",
    "text": "Outro: Kotlin Multiplatform and why it matters\n✔️ One code-base for common logic\n- Serialization logic, e.g. parsing JSON into a data class - Networking - Database\n✔️ Development speed\n✔️ Required Knowledge\n❌ Still requires some code in said language\n- Especially for UI\nSo all in all we can share our code between platforms which improves development speed & quality in multiple ways.\nThe biggest “downside” is that even though we share the code we most likely will need some kind of specific code for the platform, for the GUI on iOS as an example. Perhaps compose can help us get closer to that reality soon - who knows.\nThe final, and perhaps obvious, one I’d like to mention straight away is that platform specific libraries of course are not usable on multiplatform. This includes libraries such as React (JS) & ncurses (native).\nPersonally I see Kotlin Multiplatform as a great way to share core logic between different targets, but one must use it with care and not try to force it into being used everywhere in every way."
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#building-a-jvm-app-snake",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#building-a-jvm-app-snake",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "Building a JVM App (Snake)",
    "text": "Building a JVM App (Snake)\nLet’s start simple, Keep It Simple Stupid (KISS) principle applied, and create a JVM app. JVM has multiple advantages while developing such as\n\nEasy to run on all OS:es\nGreat debugging!\nA ton of resources\n\nFirst off, we need to draw something. This is easiest done through the Swing library which is included in the default jdk, some might call it old but hey - it does the job.Create a file called main.kt in src/jvmMain/kotlin.\nSwing has a built-in threading solution (almost too bad, because Coroutines are awesome in Kotlin!) and the best way to start the GUI is by using the existing EventQueue class and its invokeLater function. invokeLater makes sure the code runs last in the EventQueue if you add more methods, which makes sense - you want to draw the UI as the final thing.\nfun main() {\n    EventQueue.invokeLater {\n        JFrame().apply {\n            title = \"Snake\"\n            isVisible = true\n        }\n    }\n}\nWhere the apply is a context wrapper that takes the object and uses it as context (this) inside of the block/scope ({}). See its signature:\ninline fun <T> T.apply(block: T.() -> Unit): T\nThis would equate to\nval jframe = JFrame()\njframe.title = \"Snake\"\njframe.isVisible = true\nin more Java-like syntax. Why use apply? It allows us to achieve some interesting chaining concepts which I really enjoy.\nNow run the main-function, there should be a green “run”-button at the left, press that. Hopefully it compiles and a window will appear, with the title set to “Snake”.\nAwesome! We need to render something inside of the box, a game soon enough, let’s see how we can achieve that.\nAdding some minor refactoring and some new classes we can draw something\nclass Board: JPanel() {\n    init {\n        TODO(\"\"\"\n        - Set background to black\n        - Allow focus\n        - Set preferredSize to some 200x300\n        \"\"\")\n    }\n}\n\nclass GUI: JFrame() {\n    init {\n        title = \"Snake\"\n        isVisible = true\n        isResizable = false\n        setLocationRelativeTo(null)\n        defaultCloseOperation = EXIT_ON_CLOSE\n\n        TODO(\"Add the Board to the JFrame, through add()\")\n    }\n}\n\nfun main() {\n    EventQueue.invokeLater {\n        GUI()\n    }\n}\nWhat are we doing?\nJFrame was refactored GUI which then is a subclass of JFrame, with a few extra attributes added such as defaultCloseOperation = EXIT_ON_CLOSE that makes sure the program exits if we close the window, feel free to test it out!\nFurther a Board was added which extends JPanel, it’s in the Board the game will be rendered.\nFinally, add(Board()) allows us to add our Board to the JFrame.\nRun!\nSomething is not right.. The background seems black enough, but the size is most likely not correct. We can’t even resize as isResizable=false was set.\nMake sure to add pack() at the end, as in\nclass GUI: JFrame() {\n    init {\n        { /** same code as before */ }\n        add(Board())\n        pack()\n    }\n}\nWhat pack() does is that it packs and resizes the JFrame to include all its component(s) and their current size(s).\nSuper! We’re now able to render our Board and see the whole board.\n\nDrawing the snake & apple\nWe’ve got the canvas (Board), now we just need to get artsy and add a Snake and some Apples!\nI’ll keep it simple and will make the Board exist of a few cells, all pretty large. On each cell you either have nothing, Snake or Apple - pretty simple right?\nJPanel has some nice-to-have methods built-in, such as repaint() which simply repaints the component, which in turns calls paintComponent(g: Graphics?) to paint/render it.\n\nDisclaimer: the code might not be the most idiomatic, but I try to introduce a few concepts.\n\nclass Board: JPanel() {\n    init { /** same code as before */ }\n\n    override fun paintComponent(g: Graphics?) {\n        super.paintComponent(g)\n        \n        val g2d: Graphics2D = g as? Graphics2D ?: return\n        \n        g2d.scale(20.0, 20.0)\n        g2d.color = Color.GREEN\n        g2d.fill(Rectangle(5, 6, 1, 1))\n        g2d.fill(Rectangle(5, 7, 1, 1))\n        g2d.fill(Rectangle(5, 8, 1, 1))\n    }\n}\nOnce again, let’s dive into what’s happening ‘under the hood’.\nFirst, we override the function paintComponent which renders Board layout. The input is a nullable Graphics, which is shown by the type having a ? at the end. This is a cool property of Kotlin, if something can be null it actually is a type. No Option/Maybe, just pure type.\nThen Graphics? is cast to non-null Graphics2D through a safe approach using as?, without ? the cast can crash, with ? the cast would return null if failing.\nFinally we use a elvis-expression ?: which is basically a wrapper for if (null) doThis else doThat, so if the left-hand-side is null it’ll give the right-hand-side. The right-hand-side in our case is a empty return statement, meaning that we just make a early-exit. If the value is not null it’ll give the non-null variant of the type!\n\nExample use-case of elvis-operator ?:\nval a: Int = 1 ?: 0 // a = 1\nval b: Int = null ?: 0 // b = 0\n\nDetailing the code further we now have g2d: Graphics2D where Graphics2D which gives us a few nice functions to draw components on the Board.\n\nWe set the scale to 20\n\nThis simplifies the behaviour, we can now use 20x30 grid where each cell is size 1, but it’s scaled into the 200x300 grid.\n\nWe use fill to draw Rectangle’s with set Color.\n\n\nSide-note: For those wondering how you safely execute on nulls by chaining, like you do with Monads (Option)\nval nullableGraphics: Graphics? = null\nnullableGraphics?.scale(20.0, 20.0)   // This is safe! No operation executed if null\n\nSumming up, we now know how to render stuff on the Board and it’s all very static.\nThe next step is to make the rendering less static and I believe the natural step from now is to create the data structures that’ll contain the game & its state. Then we can make sure the data structures are able to update, so we can render new states.\n\n\nCreating the data structures\nData structures are required to have a game state, that is the score and position of everything.\nThe natural state is Game which contains everything, let’s begin by creating a Game structure which contains the size of the Board.\ndata class Game(val width: Int, val height: Int)\n\nSide-note: A data class is essentially the same as a case class from Scala. And for those who don’t know what a case class is it’s basically a class that simplifies a lot of stuff, mainly used as a data structure.\nYou get equals, getters & setters, and much more for free.\nAnyone from Java knows how awesome this is.\n\nMoving on we need to define the cells mentioned, something like\ndata class Cell(val x: Int, val y: Int)\nWrapping up our current state we got most of what we need, Game which contains our game state & Cell which is our co-ordinates.\nThe next step is to actually draw the Cell’s and wrap the Cell in other classes such as Apple and Snake.\nLet’s add all the required code.\ndata class Apples(val cells: Set<Cell> = emptySet())\n\ndata class Snake(val cells: List<Cell>) {\n    val head: Cell = TODO(\"Take the first cell.\")\n    val tail: List<Cell> = TODO(\"Drop one cell and return the rest.\")\n}\n\ndata class Game(\n    val width: Int,\n    val height: Int,\n    val snake: Snake,   // Adding snake and apples\n    val apples: Apples\n)\n\nclass Board: JPanel() {\n    private val game: Game = Game(\n        20,\n        30,\n        Snake(listOf(Cell(2,3),Cell(2,4),Cell(2,5),)),\n        Apples(setOf(Cell(4,5)))\n    )\n\n    init {\n        background = Color.black\n        isFocusable = true\n        preferredSize = Dimension(200, 300)\n    }\n\n    override fun paintComponent(g: Graphics?) {\n        super.paintComponent(g)\n        val g2d = g as? Graphics2D ?: return\n        g2d.scale(20.0, 20.0)\n\n        g2d.color = Color.GREEN\n        game.snake.tail.forEach { cell -> TODO(\"Render the cells using the previously used technique\") }\n\n        TODO(\"Render the head using the color YELLOW\")\n\n        TODO(\"Render the apples using the color RED\")\n    }\n}\nFixing the added TODOs and keeping the same GUI & fun main we can now run the code. You should be seeing something like\n\n\n\nSnake Game\n\n\nPretty cool right!? We’ve got\n✔️ Rendering\n✔️ Data Structures\nWhat’s left?\n\nA game loop\nAbility to actually move the data structures ( Snake)\n\n\nAdding a Direction\nTo be able to move we need to know what directions to move in. In my humble opinion this is simplest done through a Enum.\nenum class Direction {\n    UP,\n    DOWN,\n    LEFT,\n    RIGHT\n}\nSimple enough. But let’s make it better, even though pre-optimization is the root of all evil it is sometimes fun :grinning:.\nEnums in Kotlin are pretty awesome, they can both keep values and have methods! Let’s add dx and dy.\nenum class Direction(val dx: Int, val dy: Int) {\n    // -->\n    // |\n    // v\n    UP(0, -1),\n    DOWN(0, 1),\n    LEFT(-1, 0),\n    RIGHT(1, 0);\n}\nThrough dx and dy we can add it to the current cell to move in the direction which Direction is!\nUpdating Snake.kt & Cell.kt to have Cell with Direction and some turn.\ndata class Cell(val x: Int, val y: Int) {\n    fun move(direction: Direction) = TODO(\"Create new cell which moves in direction. OBS: Remember Direction now has dx, dy!\")\n}\n\ndata class Snake(\n    val cells: List<Cell>,\n    val direction: Direction, // new attribute\n    val eatenApples: Int = 0 // new attribute\n) {\n    fun move(): Snake {\n        val newHead = TODO(\"Move head\")\n        val newTail = TODO(\"Move tail!\")\n        return TODO(\"Create a new Snake with the updated position!\")\n    }\n    fun turn(newDirection: Direction?) = TODO(\"Make sure to turn correctly\")\n}\nThis is all fine & dandy, but there is some improvements to be made that’ll clean up the code.\nI mentioned that Kotlin Enums can have methods, which is awesome. We can simplify the turn-logic by adding a method to Direction, namely isOppositeTo. See the code below.\nenum class Direction(val dx: Int, val dy: Int) {\n    /** Same code as previously */\n\n    fun isOppositeTo(that: Direction) =\n        dx + that.dx == 0 && dy + that.dy == 0\n}\nRight, we can now turn the snake and render the game. We need the Game-state to update to actually re-render the updated Snake, let’s add a update-function that does this.\nfun update(direction: Direction?): Game {\n    return TODO(\"\"\"\n    Make sure to\n        1. Turn snake in direction\n        2. Move\n        3. Update the game state by returning Game\n    \"\"\")\n}\nAnd our GUI\nclass Board: JPanel() {\n    var dir: Direction = Direction.RIGHT\n    var game: Game = Game(\n        20,\n        30,\n        Snake(listOf(Cell(2,3),Cell(2,4),Cell(2,5),), dir),\n        Apples(setOf(Cell(4,5)))\n    )\n    \n    init {\n        addKeyListener(object : KeyAdapter() {\n            override fun keyPressed(e: KeyEvent?) {\n                dir = when (e?.keyCode) {\n                    VK_I, VK_UP, VK_W -> Direction.UP\n                    else -> TODO(\"Add the other key bindings. Reflect of how the object works and what is happening.\")\n                }\n                game = game.update(dir)\n                repaint()\n            }\n        })\n    }\n}\nIn our init (equal to a constructor) we add a keyListener which will listen on whenever we move. We moved game to be a var which allows us to change the reference.\n\nSide-note: The difference between a val and var is not about immutability of the value, but rather that you cannot change the pointer to the object. By using val the compiler don’t allow you to change the reference.\nval a = 1\na = 3 // CRASH -- This is not allowed\nvar b = 1\nb = 3 // b = 3, this is allowed.\nPlease note that this means that if your object is mutable, you can mutate the state of the object even though it’s a val.\n\nWhy put game on a var you might ask? Otherwise how would we update our Game as the data structure itself is “immutable”, i.e. cannot be changed, which would mean that we’d need to add a new Game object each time and save it on the stack (never cleaning it up) and that’d pretty fast make the application crash because of out of memory.\nFinally, we update the game by calling our created update-method and then we use repaint() which draws the components!\n\nRemember: paintComponent draws the canvas (game), so whenever repaint is called paintComponent draws the game again based on the game and cell’s in the game.\n\nIn conclusion this gives us an incredibly simple game, the snake moves whenever we press a key as we still don’t have a game loop based on time. So how do we add a game loop based on time?\nThe JVM got you covered! In the keyListener remove the update & repaint, then add a timer\nfixedRateTimer(TODO(\"Explore options to use for the timer and how they work\")) {\n    TODO(\"Insert a game loop here, essentially the same as done in the keyListener previously!\")\n}\nRun the game! …amazing right?\nWe now move our snake, and it moves by itself if we don’t. But the game is still pretty boring… We never die, no apples can be eaten and finally no new apples appear. We have a few additions to make to make the game a bit challenging..\nLet’s start by fixing the apples. Update the Apples.kt to randomly add apples to the board when calling grow().\n\nTo simplify the logic we use a set which means that all apples added are unique.\n\ndata class Apples(\n    val width: Int,\n    val height: Int,\n    val cells: Set<Cell> = emptySet(),\n    val growthSpeed: Int = 3, // this could actually be to only spawn apple when there is no other apple. Up to user\n    val random: Random = Random // Once again, Kotlin provides a superb class, in this case a Random wrapper that works on JVM, JS & Native - cool right?\n) {\n    fun grow(): Apples {\n        return TODO(\"\"\"\n        If we have a random number greater than growthSpeed, return no update. \n        Otherwise add a new cell.\n        \"\"\")\n    }\n}\nThen we should allow the Snake to eat them, make sure to add eat(apples: Apples) method and implement it for Snake.kt.\nfun eat(apples: Apples): Pair<Snake, Apples> {\n    return TODO(\"\"\"\n    If our head is on a Apple location, return a pair of Snake and Apple untouched.\n    Otherwise make sure to remove the apple from apples and increase body size of snake!\n    \"\"\")\n}\nAt the end of all this we need the Game.kt to allow this logic to be used. This is done through updating update to allow the snake to eat apples and also grow apples to add new ones.\nGreat! We can eat apples, add new apples and all. But we’re still pretty invincible and we’ll just keep going forever. We need to make sure that the end can be lost, let’s do it by adding a new attribute isOver to Game.kt\nval score: Int = TODO(\"Score based on snakes size, e.g. cell size\")\nval isOver: Boolean = TODO(\"Game is over if snake head in tail or snake head not on the map!\")\n\nfun update(dir: Direction): Game {\n    if (isOver) return this\n    { /** same code as was here before */ }\n}"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#wrapping-up-the-code-with-some-minor-refactoring-new-functionality",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#wrapping-up-the-code-with-some-minor-refactoring-new-functionality",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "Wrapping up the code with some minor refactoring / new functionality",
    "text": "Wrapping up the code with some minor refactoring / new functionality\nKotlin has a wonderful concept of extension functions, which simply is incredible. An extension function extends a class with new functionality. Did you ever wish Double had a rounding to string? fun Double.roundTo(n: Int): String = \"%.${decimals}f\".format(this) solves this for you! Now your Double’s automatically gives you a hint to use .roundTo as one of Double’s built-in functions!\nWith these we can update our main-method to be a tiny bit cleaner.\ng2d.color = Color.GREEN\ngame.snake.tail.forEach { cell -> g2d.fill(Rectangle(cell.x, cell.y, 1, 1)) }\n\n// Turns into -->\n\nfun Graphics2D.renderCells(color: Color, cells: Iterable<Cell>) {\n    this.color = color\n    cells.forEach { cell -> fill(Rectangle(cell.x, cell.y, 1, 1)) }\n}\n\n/**\n    Which allows us to just call `g2d.renderCells(Color.GREEN, game.snake.tail)` etc.\n*/\nWhat more improvements can be made?\nExercises left for the reader:\n\nAdd Score on the loosing screen\nAdd a win-condition (basically impossible, but taking all apples)\nReinforcement learning to train a bot (might be a future blog!)\nBetter & cleaner code!"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#jsbrowser-target",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#jsbrowser-target",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "JS/Browser target",
    "text": "JS/Browser target\nNow create src/jsMain/kotlin/main.kt.\nIn this file we need to define how to draw the browser-based GUI. Some key methods, for the full code check out the git repository.\nKeyListener document.onkeydown = { event -> onkeydown(event).also { keyDir -> dir = keyDir } } where onkeydown is your own method that handles key-events.\nTimer window.setInterval({ game = game.update(dir); render(canvas, game) }, 200)\nCanvas\nval canvas = document.getElementById(\"snake-canvas\") as HTMLCanvasElement\nval ctx = canvas.getContext(\"2d\") as CanvasRenderingContext2D\nOn this ctx from the canvas you can use fillRect to draw rectangles, and fillStyle to set color.\nHTML-Canvas\n<canvas id=\"snake-canvas\" width=\"400px\" height=\"300px\"></canvas> (put in index.html)\nThe game is run through ./gradlew jsBrowserRun, or selecting the gradle-icon at the top right (elephant) and typing jsBrowserRun.\nAnd the code for the GUI using these components is pretty much exactly the same as in Swing to be honest.\nCongratulations, you have now achieved creating a desktop game and a browser game!"
  },
  {
    "objectID": "posts/2020-11-07-snake-kotlin-multiplatform.html#native-target",
    "href": "posts/2020-11-07-snake-kotlin-multiplatform.html#native-target",
    "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
    "section": "Native target",
    "text": "Native target\nAnd onto our final target, Native Binary, that runs completely without a browser or a virtual machine.\nFor the Native target the GUI will be supported through the library ncurses which unfortunately is only supported on Linux & MacOS. If you’ve windows you can solve this through Windows Subsystem for Linux (WSL).\nBegin by creating src/nativeMain/kotlin/main.kt.\nTo begin, in the main-function do the following:\nfun maint(): Unit = memScoped {\n    // insert code\n}\nThe memScoped part means that all memory allocated in the block is automatically disposed at the end, incredibly useful! :happy:\nThen reading how to use ncurses we can figure out how to init this. The final end-goal being\nfun main(): Unit = memScoped {\n    initscr()\n\n    defer { endwin() }\n    noecho()\n    curs_set(0)\n    halfdelay(2)\n\n    var game: Game = TODO()\n    val window = newwin(game.height + 2, game.width + 2, 0, 0)!!\n    defer { delwin(window) }\n\n    var input = 0\n    while (input.toChar() != 'q') {\n        window.draw(game)\n\n        input = wgetch(window)\n        val direction = when (input.toChar()) {\n            'i' -> TODO(\"\")\n        }\n        game = game.update(direction)\n    }\n}\n\nprivate fun CPointer<WINDOW>.draw(game: Game) {\n    wclear(this)\n    box(this, 0u, 0u)\n\n    game.apples.cells.forEach { mvwprintw(this, it.y + 1, it.x + 1, \".\") }\n    game.snake.tail.forEach { mvwprintw(this, it.y + 1, it.x + 1, \"o\") }\n    game.snake.head.let { mvwprintw(this, it.y + 1, it.x + 1, \"Q\") }\n\n    if (game.isOver) {\n        mvwprintw(this, 0, 6, \"Game Over\")\n        mvwprintw(this, 1, 3, \"Your score is ${game.score}\")\n    }\n\n    wrefresh(this)\n}\nTry running it in the terminal.\nThis blog was created as a companion to a workshop I’m gonna do at AFRY, it has a bit more content including a presentation in person.\nAll the finished code is available here, if you prefer the unfinished code head to master-branch.\nThanks!\n~Hampus"
  },
  {
    "objectID": "posts/2021-04-11-til-pwa-github-pages.html",
    "href": "posts/2021-04-11-til-pwa-github-pages.html",
    "title": "TIL: GitHub Pages + Progressive Web App (PWA) = ❤️",
    "section": "",
    "text": "What\nProgressive Web Apps (PWAs) are web applications that are deployed on the Web and available on essentially every platform directly (Android, iOS, Linux, Windows, … you name it).\n\nThey’re created the same way regular Web Apps are, with minor modifications like a new separate .js-file that runs a serviceWorker. This serviceWorker is the core of the PWA and handle things like caching & installation.\nPWAs allows you to: - Share logic between all OS:s - Not have a heavy runtime associated (it uses the bundled browser engine) - On Windows/MacOS/Android/iOS the browser is included as part of OS. On Linux not as much, but who has Linux without a browser? - Use your existing knowledge AND technologies of regular webapps\nThe performance is good enough for most apps and you can easily cache content to make the app offline-first. Of course native apps are, and will always be, better but this is a good step in the right direction for simple developer to share their work on multiple platforms.\n\nThe best example of an PWA I know of is twitter, a PWA I prefer to the native app\n\nYou can test my PWA that’s written in Kotlin (targeting JS) with fritz2 on colorkidz.londogard.com. ColorKidz is a simple app that creates colouring pages out of regular images and I believe it really showcases the possibilities of PWA. ColorKidz is a MVP that was fast & easy and now shared across ALL platforms!\n\nBonus: PWAs can be deployed to Android/Google & Windows App Store, just like a regular app\n\nPlease note that GitHub TOS prohibits deployment of commercial apps through GitHub pages. If you wish to do that take a look at the alternative section for better alternatives that are free/cheap.\n\n\nHow\nBuilding, or transforming, a Web App into a PWA is really easy and has few requirements (depending on browser). The requirements follows: 1. A web manifest file 2. Serve app using HTTPS 3. A Service Worker with a fetch event handler - Allowing the app to work offline (!) 4. (not for all browser) a favicon/icon\nAdding more data makes your PWA even better, like safari/iOS specific icons to improve interoperability.\nThese requirements are pretty easy to achieve and I’ll show the two most important, serviceWorker and webmanifest.\n\nServiceWorker.js\nThe serviceWorker handles the app caching, installation and potentially some other things if you ask it too. From the docs:\n\nService workers essentially act as proxy servers that sit between web applications, and the browser and network (when available). They are intended to (amongst other things) enable the creation of effective offline experiences, intercepting network requests and taking appropriate action based on whether the network is available and updated assets reside on the server. They will also allow access to push notifications and background sync APIs.\n\nAn example of this can be found on my GitHub, find the code a little bit below.\nIf you wish to deep dive into serviceWorker’s then head over to the (awesome) documentation at Mozilla & Google.\nvar CACHE_NAME = 'londogard-colorkidz';\nvar urlsToCache = [\n  \"/index.html\",\n  \"/NEW.js\",\n  \"/android-chrome-192x192.png\",\n  ...\n];\n\nself.addEventListener('install', function(event) {\n  // Perform install steps\n  event.waitUntil(\n    caches.open(CACHE_NAME)\n      .then(function(cache) {\n        console.log('Opened cache');\n        return cache.addAll(urlsToCache);\n      })\n    );\n  });\nself.addEventListener('fetch',() => console.log(\"fetch\"));\n\n\nWebmanifest\nThe next step is to create your webmanifest which is the “metadata” of the app. What name should it have once installed? Which icon? Is the orientation locked? There’s a few knobs to tweak and turn.\nMy webmanifest ended up something like this:\n{\n  \"short_name\": \"ColorKidz\",\n  \"name\": \"ColorKidz\",\n  \"description\": \"Turn your images into Colouring Pages\",\n  \"icons\": [\n    {\n      \"src\": \"/android-chrome-192x192.png\",\n      \"sizes\": \"192x192\",\n      \"type\": \"image/png\"\n    },\n    ...\n  ],\n  \"start_url\": \"/\",\n  \"orientation\": \"portrait\",\n  \"background_color\": \"#FFFFFF\",\n  \"display\": \"standalone\",\n  \"scope\": \"/\",\n  \"theme_color\": \"#FFFFFF\"\n}\n\n\nDeploying to GitHub Pages\nFor the final step we’re to deploy the PWA!\nFirst off place all your files in root folder of your GitHub repo. Remember to also have a index.html which contains both your JS app & your serviceWorker.js and reference those scripts in body, e.g.\n<body>\n...\n<script src=\"NEW.js\"></script>\n<script src=\"serviceWorker.js\"></script>\n</body>\nThen you need to activate GitHub Pages for the repository, whereas the official documentation is great!\nGitHub pages recently got their own tab in settings and all you need to do now is to head over there, https://github.com/<USERNAME>/<REPO_NAME>/settings/pages and activate GitHub pages. Make sure to also activate https-only if possible!\nAnd that’s it, simple as that. Now you’ve your PWA deployed through GitHub pages. The PWA should be available @ <repo>.github.io, but better check it in settings to be sure that it’s set up that way.\n\n\n\nAlternatives\nThere exists a few alternatives that I’m aware off - Heroku - Cloudflare Pages - Netlify\nAll which are supposed to be great and simple to use. The free versions of everyone is pretty darn good too!\nOBS Please remember that it’s not allowed to put up applications on GitHub Pages that are commercial according to the ToS."
  },
  {
    "objectID": "posts/2022-09-07-docker-simplified-presentation.html",
    "href": "posts/2022-09-07-docker-simplified-presentation.html",
    "title": "Docker (Presentation)",
    "section": "",
    "text": "Just a week ago I did a presentation on Docker and why it’s so incredibly useful as a Data * (* = Engineer, Scientist, etc..).\n\nI simply thought I would share the slides with the world. :)\n(OBS: Presentation in Swedish)"
  },
  {
    "objectID": "posts/2022-11-06-babymonitor-pt-1.html",
    "href": "posts/2022-11-06-babymonitor-pt-1.html",
    "title": "Babymonitor #1",
    "section": "",
    "text": "Hi 👋\nI’m building a babymonitor. It’s not gonna be anything novel, neither the first or last in history. But it’s a relevant project to me, and it makes me happy! 🤓\nIn this blog I’ll walk through different ways to stream video from the raspberry pi to the network, capturing it in a client browser."
  },
  {
    "objectID": "posts/2022-11-06-babymonitor-pt-1.html#background",
    "href": "posts/2022-11-06-babymonitor-pt-1.html#background",
    "title": "Babymonitor #1",
    "section": "Background",
    "text": "Background\nIt all started when I talked to an old friend and he said that his in-laws gifted them the “Rolls-Royce” of babymonitors. The monitor has:\n\nBidirectional audio.\nUnidirectional video.\n\nNight Vision.\nRotation Horizontally.\nZoom.\n\nTemperature\nPlay soothing bedtime songs.\n\nIncredibly cool!  \nThis led to a simple equation:\nawesome + expensive + programmer = Do It Yourself (DIY)\nObviously I need to have the same specifications and a little better, while “cheaper” (my hours are “free” 🤓).\nThe greatest part? I have a natural deadline which enforce me to finish the project!"
  },
  {
    "objectID": "posts/2022-11-06-babymonitor-pt-1.html#goals",
    "href": "posts/2022-11-06-babymonitor-pt-1.html#goals",
    "title": "Babymonitor #1",
    "section": "Goals",
    "text": "Goals\nKISS - Keep It Simple Stupid\nI’ve collected the following equipment:\n\nRaspberry Pi 3B (had one already)\n5 MP camera with mechanical IR filter\n2 servos (rotating camera)\nTemperature / Humidity Sensor\nMicrophone Array\nSpeaker\n\nMy bottleneck is the Raspberry Pi’s performance really. And with performance comes optimisations, which I love! It makes following the KISS principle a tad harder! 😅\nI have settled on one of three languages to write the video streaming in, either golang, rust or python.\nMy initial idea is that the simpler parts will be a FastAPI (Python) server, like temperature and moving servos. Python really is lingua franca on the Raspberry Pi and the support is amazing.  \nFrom my initial experimentation I found Python to require a shit ton of CPU power to livestream video, as such I believe rust or golang will be the way to go. 🚀"
  },
  {
    "objectID": "posts/2022-11-06-babymonitor-pt-1.html#live-streaming-initial-experimentation",
    "href": "posts/2022-11-06-babymonitor-pt-1.html#live-streaming-initial-experimentation",
    "title": "Babymonitor #1",
    "section": "Live Streaming: Initial Experimentation",
    "text": "Live Streaming: Initial Experimentation\nI’ve tried multiple things, HTTP, HLS, Websocket & WebRTC. Each step proves a more complex, albeit more optimal, solution. Each with it’s trade-offs.\nSome worthy mentions of other solutions is Real Time Streaming Protocol (RTSP).\n\nProtocols / Variants\nDescribing the protocol and how it’s implemented, in a very general way.\n\nHTTPHLSWebsocketWebRTC\n\n\nHypertext Transfer Protocol, lingua franca protocol of the internet, is a way to stream both video and audio. It’s easy, but not efficient.\nHow: Stream chunks using HTTP messages and let your <video> element handle the consumption of stream.\n\n\nHTTP Live Streaming is a simple and pretty efficient way to stream both video and audio over the internet.\nHow: video-/audio-files, .m3u8, which are then picked up by client. The chunks are built from a “live” stream, e.g. webcam, or a file.\n\n\nWebsockets is a communication protocol which allows much lower overhead than raw HTTP while providing bi-directional (duplex) communication. It’s really optimal to stream things that move in real-time through HTTP, such as a video or audio stream.\nHow: Stream your byte-array realtime through a websocket, like you’d a HTTP. There’s less headers involved, but it’s much harder to consume on the client. You need a special JS media player which can decode the websocket stream into video/audio.\n\n\nWebRTC is a open framework that enables Real-Time Communications (RTC) inside the browser. From the get-go it supports bidirectional video/audio and also encrypted if required.\nIt’s the protocol to stream realtime really. It’s used in a lot of the tools you know today.\nHow: set up a WebRTC server and then stream your bytearrays directly after connecting with a client.\n\n\n\nEach protocol comes with positives and negatives.\n\n\n\n\n\n\n\n\n\n\n\nHTTP\nHLS\nWebsocket\nWebRTC\n\n\n\n\nPros\n+ Easy to implement.+ Simple protocol.\n+ Easy to implement+ CPU efficient.+ Easy to do “live” streams.\n+ Low latency.+ CPU efficient.\n+ Supports all my use-cases+ Low Latency.+ CPU efficient.\n\n\nCons\n- CPU inefficient (HTTP header overhead).\n- High latency (5-10s+). \n- Hard to consume on client.- Bi-directional streaming is also hard.\n- Not straightforward implementation- Less documentation than HLS/HTTP.\n\n\n\n\n\nImplementations\n\nHTTPHLSWebsocketWebRTC\n\n\nUsing MJPEG.\nThe provided MJPEG server from picamera2 is excelent show-case on how to stream the video. It sets up a simple HTML with a <img> element which streams new frames using MJPEG  which is Motion-JPEG.\nThe performance is pretty OK, considering it’s Python & MJPEG. Compared to H264 which works much more effectively. \nWe see the CPU hovering around 130-150%, but the largest drawback is the network bandwidth, at ~50Mb/s compared to H.264 at ~3.5Mb/s.\nThis is because MJPEG sends the full frame each time, H.264 sends a frame and then some delta frame until it sends a full frame again. This has drawbacks and positives, the bandwidth is low but quality can suffer.\n\n\nCode\n\n#!/usr/bin/python3\n\n# Mostly copied from https://picamera.readthedocs.io/en/release-1.13/recipes2.html\n# Run this script, then point a web browser at http:<this-ip-address>:8000\n# Note: needs simplejpeg to be installed (pip3 install simplejpeg).\n\nimport io\nimport logging\nimport socketserver\nfrom http import server\nfrom threading import Condition, Thread\n\nfrom picamera2 import Picamera2\nfrom picamera2.encoders import JpegEncoder\nfrom picamera2.outputs import FileOutput\n\nPAGE = \"\"\"\\\n<html>\n<head>\n<title>picamera2 MJPEG streaming demo</title>\n</head>\n<body>\n<h1>Picamera2 MJPEG Streaming Demo</h1>\n<img src=\"stream.mjpg\" width=\"640\" height=\"480\" />\n</body>\n</html>\n\"\"\"\n\n\nclass StreamingOutput(io.BufferedIOBase):\n    def __init__(self):\n        self.frame = None\n        self.condition = Condition()\n\n    def write(self, buf):\n        with self.condition:\n            self.frame = buf\n            self.condition.notify_all()\n\n\nclass StreamingHandler(server.BaseHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == '/':\n            self.send_response(301)\n            self.send_header('Location', '/index.html')\n            self.end_headers()\n        elif self.path == '/index.html':\n            content = PAGE.encode('utf-8')\n            self.send_response(200)\n            self.send_header('Content-Type', 'text/html')\n            self.send_header('Content-Length', len(content))\n            self.end_headers()\n            self.wfile.write(content)\n        elif self.path == '/stream.mjpg':\n            self.send_response(200)\n            self.send_header('Age', 0)\n            self.send_header('Cache-Control', 'no-cache, private')\n            self.send_header('Pragma', 'no-cache')\n            self.send_header('Content-Type', 'multipart/x-mixed-replace; boundary=FRAME')\n            self.end_headers()\n            try:\n                while True:\n                    with output.condition:\n                        output.condition.wait()\n                        frame = output.frame\n                    self.wfile.write(b'--FRAME\\r\\n')\n                    self.send_header('Content-Type', 'image/jpeg')\n                    self.send_header('Content-Length', len(frame))\n                    self.end_headers()\n                    self.wfile.write(frame)\n                    self.wfile.write(b'\\r\\n')\n            except Exception as e:\n                logging.warning(\n                    'Removed streaming client %s: %s',\n                    self.client_address, str(e))\n        else:\n            self.send_error(404)\n            self.end_headers()\n\n\nclass StreamingServer(socketserver.ThreadingMixIn, server.HTTPServer):\n    allow_reuse_address = True\n    daemon_threads = True\n\n\npicam2 = Picamera2()\npicam2.configure(picam2.create_video_configuration(main={\"size\": (640, 480)}))\noutput = StreamingOutput()\npicam2.start_recording(JpegEncoder(), FileOutput(output))\n\ntry:\n    address = ('', 8000)\n    server = StreamingServer(address, StreamingHandler)\n    server.serve_forever()\nfinally:\n    picam2.stop_recording()\n\n\n\nVery simple using ffmpeg and picamera2  in conjunction. \nThe performance of HLS is great at ~40% CPU, but the latency is awful at 5-10s easily!\nPython\nimport time\nfrom picamera2.outputs import FfmpegOutput\nfrom picamera2.encoders import H264Encoder, Quality\nfrom picamera2 import Picamera2\n\n\npicam2 = Picamera2()\npicam2.configure(picam2.create_video_configuration(main={\"size\": (640, 480)}))\nencoder = H264Encoder(bitrate=1000000, repeat=True, iperiod=15)\noutput = FfmpegOutput(\"-f hls -hls_time 4 -hls_list_size 5 -hls_flags delete_segments -hls_allow_cache 0 stream.m3u8\")\npicam2.start_recording(encoder, output)\ntime.sleep(9999999)\nHTML\n<video width=\"320\" height=\"240\" controls autoplay>\n    <source src=\"stream.m3u8\" type=\"application/x-mpegURL\" />\n    Your browser does not support the video tag.\n</video>\n\n\nI’ve basically re-used go-h264-streamer which is a very simple yet efficient golang-implementation of a websocket driven H264 streamer that streams through websocket to a client. The client has a JS script which is WebASM/emscripten compiled to use WebGL-accelerated graphics to decode the H264 into frames. \nWhat’s really cool about this implementation is that it only starts the video stream when people connect and shut it down when there’s no connected websockets.\n\n\nI kept myself to KISS, unlike all this testing of approaches, and went with golangs pion (https://github.com/pion/webrtc)  and their excellent mediadevices (https://github.com/pion/mediadevices) which wraps the legacy pi camera stack (non-open source drivers, unlike the modern pi camera stack which builds on libcamera).\n\n\n\n\n\nPerformance\nStats taken from top.\n\n\n\n\n\n\n\n\n\n\n\n\nHardware\nHTTP - MJPEG\nHLS\nWebsocket no connection\nWebsocket\nWebRTC\nWebRTC (aiortc)\n\n\n\n\nCPU\n150%\n40%\n<=0.2%\n40%\n170%\n250-350%\n\n\nRAM\n6%\n4%\n<=0.4%\n6%\n5%"
  },
  {
    "objectID": "posts/2022-11-06-babymonitor-pt-1.html#ending-notes",
    "href": "posts/2022-11-06-babymonitor-pt-1.html#ending-notes",
    "title": "Babymonitor #1",
    "section": "Ending Notes",
    "text": "Ending Notes\nThis is what I have currently. In the next blog I’ll go through how we’ll set up a backend which will allow us to use the sensors, move the servos and stream audio/video.\nI think the bidirectional communication will require a third blog, and then manufacturing a 3D-printed case as a fourth!\nUntil next time,\nHampus Londögård"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "During the years I’ve done a few presentations and workshops.\n\n\nNov 12th 2021 - Managing the ML Lifecycle without a headache\n\nUnfortunately I had to re-record the recording at home because the video recorder crashed during the live presentation.\nPresenting how to streamline your ML development through MLOps, showcased through using DVC & CML.\n\nSep 7th 2021 - Panel: How can we accelerate the industry forward? #IndustryDay\n\nI was asked to join as a expert on AI with focus on how to accelerate the industry forward.\n\nJul 5th 2021 - Vikten av att verkligen förstå AI (swe)\n\nPresenting the importance of understanding model. A light talk that requires no prior experience in ML.\nA blend of Ethical AI and Explainable AI (XAI)\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]