{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Automated Data Validation & Exploration\"\n",
    "description: \"\"\n",
    "categories: [machine-learning, data]\n",
    "author: Hampus Londögård\n",
    "date: \"2023-03-20\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install deepchecks -U --user\n",
    "!pip install pandas -U --user\n",
    "!pip install polars -U --user\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Validation & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Today we'll dive into automated _Data Validation_ and _Data Exploration_.\n",
    "\n",
    "Every day we work through a multitude of data using heurestics, statistics and many tools. But is there better tools out there? Is there a way to automate some of the process to put greater emphasis on the important things? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Validation Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a few tools.\n",
    "\n",
    "1. [Deepchecks](https://github.com/deepchecks/deepchecks) _Tests for Continuous Validation of ML Models & Data_\n",
    "2. [ydata-profiling](https://github.com/ydataai/ydata-profiling) (previously _pandas-profiling) _Create HTML profiling reports from pandas DataFrame objects_\n",
    "3. [greatexpectations](https://github.com/great-expectations/great_expectations) _Always know what to expect from your data._\n",
    "4. [pandera](https://pandera.readthedocs.io/en/stable/) _A Statistical Data Testing Toolkit_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll focus on a few discussion points today\n",
    "\n",
    "- When does it make sense to introduce this type of tool?\n",
    "- How do you use this type of tool today?\n",
    "- How can it be improved?\n",
    "- Can it be used as part of Data Analysis?\n",
    "- Can it be used in any other part of the process?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "As we all know to be true data is incredibly important when developing Machine Learning Applications.\n",
    "\n",
    "> Shit in, shit out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First we'll make a quick introduction to each tool and their strengths.\n",
    "\n",
    "Second I'll share a few use-case examples.\n",
    "\n",
    "Finally we'll end up discussing how we can use, or use, these tools."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deepchecks\n",
    "\n",
    "![[Deepchecks Checks](https://github.com/deepchecks/deepchecks)](https://github.com/deepchecks/deepchecks/raw/main/docs/source/_static/images/general/checks-and-conditions.png){#fig-deepchecks}\n",
    "\n",
    "> Deepchecks is a Python package for comprehensively validating your machine learning models and data with minimal effort. This includes checks related to various types of issues, such as model performance, data integrity, distribution mismatches, and more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Formats\n",
    "\n",
    "Deepchecks supports the following formats:\n",
    "\n",
    "1. Tabular\n",
    "2. Computer Vision\n",
    "3. NLP (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "![Video of a Deepcheck Evaluation Suite](https://github.com/deepchecks/deepchecks/raw/main/docs/source/_static/images/general/model_evaluation_suite.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of checks\n",
    "\n",
    "The types of checks are divided into 3 variants,\n",
    "\n",
    "![Deepchecks Types and where they run](https://github.com/deepchecks/deepchecks/raw/main/docs/source/_static/images/general/pipeline_when_to_validate.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running a Deepcheck\n",
    "\n",
    "Either you run a full suite or a single feature. You choose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "suite = model_evaluation()\n",
    "result = suite.run(train_dataset=train_dataset, test_dataset=test_dataset, model=model)\n",
    "result.save_as_html() # replace this with result.show() or result.show_in_window() to see results inline or in window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import FeatureDrift\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "# Initialize and run desired check\n",
    "FeatureDrift().run(train_df, test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ydata-profiling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great Expectations\n",
    "\n",
    "![great expectations](https://docs.greatexpectations.io/assets/images/gx_oss_process-050a4264f415a1bff3ceea3ac6f9b3a0.png)\n",
    "\n",
    "> Great Expectations (GX) helps data teams build a shared understanding of their data through quality testing, documentation, and profiling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "expect_column_values_to_be_between(\n",
    "    column=\"passenger_count\",\n",
    "    min_value=1,\n",
    "    max_value=6\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![automated data docs](https://docs.greatexpectations.io/assets/images/datadocs-8d8bc71d8aec770a38656ce60cc1e073.png)\n",
    "\n",
    "Even has _Data Assistant_ to build automated checks based on Golden Dataset!\n",
    "\n",
    "There's > 50 built-in expexctations and >300 including community added!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandera\n",
    "1. Define a schema once and use it to validate different dataframe types.\n",
    "2. Check the types and properties of columns/values.\n",
    "3. Perform more complex statistical validation like hypothesis testing.\n",
    "4. Seamlessly integrate with existing data analysis/processing pipelines via function decorators.\n",
    "5. Define dataframe models with the class-based API with pydantic-style syntax and validate dataframes using the typing syntax.\n",
    "6. Synthesize data from schema objects for property-based testing with pandas data structures.\n",
    "7. Lazily Validate dataframes so that all validation rules are executed before raising an error.\n",
    "8. Integrate with a rich ecosystem of python tools like pydantic, fastapi and mypy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported\n",
    "\n",
    "| Tool | Data Stores (Pandas, Spark, DB, Other) | Validation | Profiling Data | Drift | Hypothesis | Data Generation | Data Types | Personal Favorite(s) |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|deepchecks||||||||||\n",
    "|ydata-profiling||||||||||\n",
    "|greatexpectations||||||||||\n",
    "|pandera||||||||||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
