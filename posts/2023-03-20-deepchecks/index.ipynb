{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---\n",
    "title: \"Automated Data Validation & Exploration\"\n",
    "description: \"\"\n",
    "categories: [machine-learning, data]\n",
    "author: Hampus Londögård\n",
    "date: \"2023-03-20\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install deepchecks -U --user\n",
    "!pip install pandas -U --user\n",
    "!pip install polars -U --user\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Validation & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Today we'll dive into automated _Data Validation_ and _Data Exploration_.\n",
    "\n",
    "Every day we work through a multitude of data using heurestics, statistics and many tools. But is there better tools out there? Is there a way to automate some of the process to put greater emphasis on the important things? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Validation Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a few tools.\n",
    "\n",
    "1. [Deepchecks](https://github.com/deepchecks/deepchecks) _Tests for Continuous Validation of ML Models & Data_\n",
    "2. [ydata-profiling](https://github.com/ydataai/ydata-profiling) (previously _pandas-profiling) _Create HTML profiling reports from pandas DataFrame objects_\n",
    "3. [greatexpectations](https://github.com/great-expectations/great_expectations) _Always know what to expect from your data._\n",
    "4. [pandera](https://pandera.readthedocs.io/en/stable/) _A Statistical Data Testing Toolkit_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll focus on a few discussion points today\n",
    "\n",
    "- When does it make sense to introduce this type of tool?\n",
    "- How do you use this type of tool today?\n",
    "- How can it be improved?\n",
    "- Can it be used as part of Data Analysis?\n",
    "- Can it be used in any other part of the process?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "As we all know to be true data is incredibly important when developing Machine Learning Applications.\n",
    "\n",
    "> Shit in, shit out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First we'll make a quick introduction to each tool and their strengths.\n",
    "\n",
    "~Second I'll share a few use-case examples.~\n",
    "\n",
    "Finally we'll end up discussing how we can use, or use, these tools."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deepchecks\n",
    "\n",
    "![[Deepchecks Checks](https://github.com/deepchecks/deepchecks)](https://github.com/deepchecks/deepchecks/raw/main/docs/source/_static/images/general/checks-and-conditions.png){#fig-deepchecks}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> Deepchecks is a Python package for **comprehensively validating** your **machine learning models** and **data** with minimal effort. This includes checks related to various types of issues, such as **model performance, data integrity, distribution mismatches**, and more."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data Formats\n",
    "\n",
    "Deepchecks supports the following formats:\n",
    "\n",
    "1. Tabular\n",
    "2. Computer Vision\n",
    "3. NLP (text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "![Video of a Deepcheck Evaluation Suite](https://github.com/deepchecks/deepchecks/raw/main/docs/source/_static/images/general/model_evaluation_suite.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Types of checks\n",
    "\n",
    "The types of checks are divided into 3 variants,\n",
    "\n",
    "![Deepchecks Types and where they run](https://github.com/deepchecks/deepchecks/raw/main/docs/source/_static/images/general/pipeline_when_to_validate.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Running a Deepcheck\n",
    "\n",
    "Either you run a full suite or a single feature. You choose!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Full Evaluation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "suite = model_evaluation()\n",
    "result = suite.run(train_dataset=train_dataset, test_dataset=test_dataset, model=model)\n",
    "result.save_as_html() # replace this with result.show() or result.show_in_window() to see results inline or in window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Single Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import FeatureDrift\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "# Initialize and run desired check\n",
    "FeatureDrift().run(train_df, test_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ydata-profiling\n",
    "\n",
    "`ydata-profiling`, previously `pandas-profiling` is a tool that allows you to easily profile a dataset quickly and grok the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key features\n",
    "\n",
    "- **Type inference**: automatic detection of columns' data types (*Categorical*, *Numerical*, *Date*, etc.)\n",
    "- **Warnings**: A summary of the problems/challenges in the data that you might need to work on (*missing data*, *inaccuracies*, *skewness*, etc.)\n",
    "- **Univariate analysis**: including descriptive statistics (mean, median, mode, etc) and informative visualizations such as distribution histograms\n",
    "- **Multivariate analysis**: including correlations, a detailed analysis of missing data, duplicate rows, and visual support for variables pairwise interaction\n",
    "- **Time-Series**: including different statistical information relative to time dependent data such as auto-correlation and seasonality, along ACF and PACF plots.\n",
    "- **Text analysis**: most common categories (uppercase, lowercase, separator), scripts (Latin, Cyrillic) and blocks (ASCII, Cyrilic)\n",
    "- **File and Image analysis**: file sizes, creation dates, dimensions, indication of truncated images and existence of EXIF metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- **Compare datasets**: one-line solution to enable a fast and complete report on the comparison of datasets\n",
    "- **Flexible output formats**: all analysis can be exported to an HTML report that can be easily shared with different parties, as JSON for an easy integration in automated systems and as a widget in a Jupyter Notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The report contains three additional sections:\n",
    "\n",
    "- **Overview**: mostly global details about the dataset (number of records, number of variables, overall missigness and duplicates, memory footprint)\n",
    "- **Alerts**: a comprehensive and automatic list of potential data quality issues (high correlation, skewness, uniformity, zeros, missing values, constant values, between others)\n",
    "- **Reproduction**: technical details about the analysis (time, version and configuration)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### How to use\n",
    "\n",
    "`ydata-profiling` is incredibly simple to use!\n",
    "\n",
    "All that needs to be done is\n",
    "```python\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "```\n",
    "\n",
    "Examples found on [github](https://github.com/ydataai/ydata-profiling/blob/master/README.md#-examples).  \n",
    "[titanic](https://ydata-profiling.ydata.ai/examples/master/titanic/titanic_report.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Great Expectations\n",
    "\n",
    "![great expectations](https://docs.greatexpectations.io/assets/images/gx_oss_process-050a4264f415a1bff3ceea3ac6f9b3a0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "> Great Expectations (GX) helps data teams build a shared understanding of their data through quality testing, documentation, and profiling.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "GX is a well-known tool with a huge community. This means that there's multiple plugins in other tools to support this framework.\n",
    "\n",
    "It support things like Snowflake, BigQuery, Spark, Pandas, ..!\n",
    "\n",
    "It's easy to use and gives Data Documentation of the tests which can be saved in S3 or other places giving everyone a possibility to view and share these!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of GX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# great expectations check example\n",
    "# can also be JSON\n",
    "expect_column_values_to_be_between(\n",
    "    column=\"passenger_count\",\n",
    "    min_value=1,\n",
    "    max_value=6\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![automated data docs](https://docs.greatexpectations.io/assets/images/datadocs-8d8bc71d8aec770a38656ce60cc1e073.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Even has _Data Assistant_ to build automated checks based on Golden Dataset!\n",
    "\n",
    "There's > 50 built-in expexctations and >300 including community added!\n",
    "\n",
    "> Our stakeholders would notice data issues before we did – which eroded trust in our data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### pandera\n",
    "1. Define a schema once and use it to validate different dataframe types.\n",
    "2. Check the types and properties of columns/values.\n",
    "3. Perform more complex statistical validation like hypothesis testing.\n",
    "4. Seamlessly integrate with existing data analysis/processing pipelines via function decorators.\n",
    "5. Define dataframe models with the class-based API with pydantic-style syntax and validate dataframes using the typing syntax.\n",
    "6. Synthesize data from schema objects for property-based testing with pandas data structures.\n",
    "7. Lazily Validate dataframes so that all validation rules are executed before raising an error.\n",
    "8. Integrate with a rich ecosystem of python tools like pydantic, fastapi and mypy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "# data to validate\n",
    "df = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# define schema\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n",
    "    \"column3\": pa.Column(str, checks=[\n",
    "        pa.Check.str_startswith(\"value_\"),\n",
    "        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n",
    "    ]),\n",
    "})\n",
    "\n",
    "validated_df = schema(df)\n",
    "print(validated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pandera.typing import Series\n",
    "\n",
    "class Schema(pa.DataFrameModel):\n",
    "    column1: Series[int] = pa.Field(le=10)\n",
    "    column2: Series[float] = pa.Field(lt=-1.2)\n",
    "    column3: Series[str] = pa.Field(str_startswith=\"value_\")\n",
    "\n",
    "    @pa.check(\"column3\")\n",
    "    def column_3_check(cls, series: Series[str]) -> Series[bool]:\n",
    "        \"\"\"Check that column3 values have two elements after being split with '_'\"\"\"\n",
    "        return series.str.split(\"_\", expand=True).shape[1] == 2\n",
    "\n",
    "Schema.validate(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supported\n",
    "\n",
    "| Tool | Data Stores (Pandas, Spark, DB, Other) | Steps (Analysis, Training, Production, Non-ML) | Drift | Hypothesis | Data Generation | Data Types | Personal Favorite(s) |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | \n",
    "|deepchecks|✅😐❌✅|✅✅✅✅|✅|❌|❌|❌|✅|\n",
    "|ydata-profiling|✅✅✅❌|✅❌❌✅|✅|❌|❌|✅|✅|\n",
    "|greatexpectations|✅✅✅✅|❌❌✅✅|❌|❌|❌|✅|😐|\n",
    "|pandera|✅✅⏳✅|❌✅✅✅|❌|✅|✅|✅|✅|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bonus: Additional Great Frameworks\n",
    "\n",
    "- [**Fairlearn**](https://fairlearn.org/) _Fairlearn is an open-source, community-driven project to help data scientists improve fairness of AI systems._\n",
    "- [**Torchdrift**](http://torchdrift.org/)\n",
    "- [**alibi-detect**](https://github.com/SeldonIO/alibi-detect)\n",
    "- [**Evidently**](https://www.evidentlyai.com/) (superb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus: PyGWalker\n",
    "\n",
    "I found a new tool lately called PyGWalker which was really cool! It cannot handle really large data, but it's excellent for smaller datasets :)\n",
    "\n",
    "> Turn your pandas dataframe into a Tableau-style User Interface for visual analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![PyGWalker](https://user-images.githubusercontent.com/8137814/221879671-70379d15-81ac-44b9-b267-a8fa3842a0d9.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![PyGWalker GIF](https://camo.githubusercontent.com/c84599f68423b89c2ea0c1c6762b7a585910f9ba01235d4604a5292d32591462/68747470733a2f2f646f63732d75732e6f73732d75732d776573742d312e616c6979756e63732e636f6d2f696d672f70796777616c6b65722f74726176656c2d616e692d302d6c696768742e676966)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
