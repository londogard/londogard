<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hampus Londögård">
<meta name="dcterms.date" content="2019-02-04">
<meta name="description" content="This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment).">

<title>Londogard Blog - AFRY NLP Competence Meeting: Text Classification IMDB</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Londogard Blog - AFRY NLP Competence Meeting: Text Classification IMDB">
<meta name="twitter:description" content="This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment).">
<meta name="twitter:creator" content="@hlondogard">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Londogard Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/presentations.html"> 
<span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://londogard.com/projects"> 
<span class="menu-text">Londogard Projects↗</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../pages/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/code_setup/code_setup.html"> 
<span class="menu-text">Dev Setup</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hampus-londögård/"> <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hlondogard"> <i class="bi bi-twitter" role="img" aria-label="Twitter">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="button" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/lundez"><i class="bi bi-github" role="img">
</i> 
 <span class="dropdown-text">Lundez</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/londogard"><i class="bi bi-github" role="img">
</i> 
 <span class="dropdown-text">Londogard</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"> <i class="bi bi-rss" role="img" aria-label="Londogard Blog RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">AFRY NLP Competence Meeting: Text Classification IMDB</h1>
                  <div>
        <div class="description">
          This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment).
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">nlp</div>
                <div class="quarto-category">workshop</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hampus Londögård </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 4, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#afry-nlp-competence-meeting-text-classification-imdb" id="toc-afry-nlp-competence-meeting-text-classification-imdb" class="nav-link active" data-scroll-target="#afry-nlp-competence-meeting-text-classification-imdb">2019-02-04 AFRY NLP Competence Meeting: Text Classification IMDB</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#text-classification" id="toc-text-classification" class="nav-link" data-scroll-target="#text-classification">Text Classification</a>
  <ul class="collapse">
  <li><a href="#a-good-baseline" id="toc-a-good-baseline" class="nav-link" data-scroll-target="#a-good-baseline">A good baseline</a></li>
  <li><a href="#classes-features" id="toc-classes-features" class="nav-link" data-scroll-target="#classes-features">Classes &amp; Features</a></li>
  <li><a href="#back-to-text-classification" id="toc-back-to-text-classification" class="nav-link" data-scroll-target="#back-to-text-classification">Back to text classification</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  <li><a href="#lets-do-something-fun-out-of-this" id="toc-lets-do-something-fun-out-of-this" class="nav-link" data-scroll-target="#lets-do-something-fun-out-of-this">Let’s do something fun out of this!</a></li>
  <li><a href="#lets-create-our-classifier" id="toc-lets-create-our-classifier" class="nav-link" data-scroll-target="#lets-create-our-classifier">Let’s create our classifier</a></li>
  <li><a href="#comparison-to-state-of-the-art" id="toc-comparison-to-state-of-the-art" class="nav-link" data-scroll-target="#comparison-to-state-of-the-art">Comparison to state-of-the-art</a></li>
  <li><a href="#improving-the-model" id="toc-improving-the-model" class="nav-link" data-scroll-target="#improving-the-model">Improving the model</a></li>
  <li><a href="#implementation-details" id="toc-implementation-details" class="nav-link" data-scroll-target="#implementation-details">Implementation details</a></li>
  <li><a href="#conclusion-of-tf-idf" id="toc-conclusion-of-tf-idf" class="nav-link" data-scroll-target="#conclusion-of-tf-idf">Conclusion of TF-IDF</a></li>
  <li><a href="#use-of-context" id="toc-use-of-context" class="nav-link" data-scroll-target="#use-of-context">Use of context</a></li>
  <li><a href="#conclusion-of-n-gram" id="toc-conclusion-of-n-gram" class="nav-link" data-scroll-target="#conclusion-of-n-gram">Conclusion of N-gram</a></li>
  </ul></li>
  <li><a href="#conclusion-of-phase-1" id="toc-conclusion-of-phase-1" class="nav-link" data-scroll-target="#conclusion-of-phase-1">Conclusion of phase 1</a></li>
  <li><a href="#phase-2" id="toc-phase-2" class="nav-link" data-scroll-target="#phase-2">Phase 2</a></li>
  <li><a href="#word-embeddings" id="toc-word-embeddings" class="nav-link" data-scroll-target="#word-embeddings">Word Embeddings</a>
  <ul class="collapse">
  <li><a href="#word-embeddings-1" id="toc-word-embeddings-1" class="nav-link" data-scroll-target="#word-embeddings-1">Word Embeddings</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="afry-nlp-competence-meeting-text-classification-imdb" class="level1">
<h1>2019-02-04 AFRY NLP Competence Meeting: Text Classification IMDB</h1>
<p>I’ve set a goal to create one blog post per Competence Meeting I’ve held at AFRY to spread the knowledge further. This goal will also grab all the older meetings, my hope is that I’ll be finished before summer 2020, but we’ll see. <!--truncate--></p>
<hr>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Most of my Competence Meetings take place in the form of Jupyter Notebooks (<code>.ipynb</code>). Notebooks are awesome as they allow us to:</p>
<ol type="1">
<li>Mix and match <em>markdown</em> &amp; <em>code</em>-blocks</li>
<li>Keep the state of the program, i.e.&nbsp;very explorative</li>
</ol>
<p>This is really good in combination with the workshop-format that we usually have. Using services such as <a href="https://colab.research.google.com/">Google Colab</a> one can take the file and open it in the browser and run it there. This means that we don’t need any downloads and pretty often we also have a speed gain because the node used is faster than a laptop with its GPU.</p>
<p>Let’s get on to the competence evening.</p>
<hr>
</section>
<section id="text-classification" class="level2">
<h2 class="anchored" data-anchor-id="text-classification">Text Classification</h2>
<p>Today we’ll go through text classification, what it is, how it is used and how to make it yourself while trying to keep have a great mix of both theory and practical use. Text classification is just what the name suggest, a way to classify texts. Let it be spam or reviews, you train it and it’ll predict what class the text belongs to.</p>
<hr>
<section id="a-good-baseline" class="level3">
<h3 class="anchored" data-anchor-id="a-good-baseline">A good baseline</h3>
<p>To have a good baseline is incredibly important in Machine Learning. In summary you want the following</p>
<ul>
<li>Simple model to predict outcome</li>
<li>Use this model to compare your new, more complex model to</li>
</ul>
<p>This is to be able to know what progress you’re making. You don’t want to do anything more complex without any gains.</p>
<p>One pretty common simple baseline is just to pick a random class as prediction.</p>
</section>
<section id="classes-features" class="level3">
<h3 class="anchored" data-anchor-id="classes-features">Classes &amp; Features</h3>
<p>What is a class and feature?</p>
<p>Features are the input to the model, you can see a machine learning system as a "consumer" of features. You can view this as a cookie monster consuming cookies and then he says if they taste good or bad. He has the input, cookie, that can be a feature. He then has a output, class, that is good/bad. Repeat this a lot of times and you can retrieve statistics if Cookie Y is good or bad.</p>
<p>To generalize this system we would divide the feature into multiple feature, like what ingredients the cookie contains. So instead of saying this is a "Chocolate Chip Cookie" we know tell the system the features are:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>chocolate: yes</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>sugar:yes</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>honey:no</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>oat:no</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>cinnamon: no</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>sweet: yes</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>sour: no\<span class="st">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>. In numerical input it would translate to something as <code>[1,1,0,0,0,1,0]</code>.</p>
<section id="one-hot-encoding---how-we-represent-features-classes" class="level4">
<h4 class="anchored" data-anchor-id="one-hot-encoding---how-we-represent-features-classes">One-Hot-Encoding - how we represent features &amp; classes</h4>
<p>As shown in the translation to numerical vectors we don’t represent words as actual words. We always use numbers, often we even use something called <em>One-Hot-Encoding</em>.</p>
<p>One-Hot-Encoding means that we have an array of one 1 and the rest is 0s. This is to optimize math performed by the GPU (or CPU).</p>
<p>Using the example of <em>Good</em> &amp; <em>Bad</em> cookies with the extension of <em>Decent</em> we will One-Hot-Encode these as the following</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Good   <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>Bad    <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>Decent <span class="op">=</span> [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The same is applied to our features. If you’re using a framework (such as Keras) it is pretty common that they include an method to do this, or even that it is done automatically for you.</p>
</section>
</section>
<section id="back-to-text-classification" class="level3">
<h3 class="anchored" data-anchor-id="back-to-text-classification">Back to text classification</h3>
<p>To classify a text we do what is called an <em>sentiment analysis</em> meaning that we try to estimate the <em>sentiment polarity</em> of a text body. In the first part of this workshop we’ll be assuming that there’s only two sentiments, <em>Negative</em> and <em>Positive</em>. Then we can express this as the following classification problem:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Feature: String body</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Class:   Bad<span class="op">|</span>Good</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The output, <em>Classes</em>, are easy to One-Hot-Encode but how do we succesfully One-Hot-Encode a string? A character can be seen as a class but is that really something we can learn from? To solve this we need to preprocess our input somehow.</p>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>Preprocessing is an incredibly important part of Machine Learning. Combining preprocessing with <em>Data Mining</em> is actually around 70% of the workload (IBM) when developing models through the CRISP-DM. From my experience this is true.</p>
<p>Having good data and finding the most important features is incredibly important to have a competent system. In this task we need to preprocess the text to simplify the learning process for our system. We will do the following:</p>
<ul>
<li>Clean the text</li>
<li>Vectorize the texts into numerical vectors</li>
</ul>
<section id="cleaning-the-text" class="level4">
<h4 class="anchored" data-anchor-id="cleaning-the-text">Cleaning the text</h4>
<p>Why do we need to clean the text? It is to remove weird stuff &amp; outliers. If we have the text <code>I'm a cat.</code>we want to simplify this into <code>[i'm, a, cat]</code> or even <code>[im, a, cat]</code>.</p>
<p>Removing data such as non-alphabetical characters and the letter case makes more data look a like and reduces the dimension of our input – this simplifies the learning of the system. But removing features can be bad also, if someone writes in all CAPS we can guess that they’re angry. But let’s take that later.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> regex <span class="im">as</span> re</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    \<span class="st">"</span><span class="ch">\"\"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="er">    Applies some pre-processing on the given text.</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    Steps :</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="op">-</span> Removing punctuation</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">-</span> Lowering text</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    \<span class="st">"</span><span class="ch">\"\"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="er">    </span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove the characters [\\], ['] and [\"]</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(r\<span class="st">"</span><span class="ch">\\\\\"</span><span class="st">, </span><span class="ch">\"\"</span><span class="st">, text)    </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="er">    text = re.sub</span>(r\<span class="st">"</span><span class="ch">\\</span><span class="st">'</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"\"</span><span class="st">, text)    # Extra: Is regex needed? Other ways to accomplish this.</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="er">    text = re.sub</span>(r\<span class="st">"</span><span class="ch">\\\"\"</span><span class="st">, </span><span class="ch">\"\"</span><span class="st">, text)</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="er">    # replace all non alphanumeric with space </span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(r\<span class="st">"</span><span class="ch">\\</span><span class="st">W+</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st"> </span><span class="ch">\"</span><span class="st">, text)</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="er">    # text = re.sub</span>(r\<span class="st">"&lt;.+?&gt;</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st"> </span><span class="ch">\"</span><span class="st">, text) # &lt;br&gt;&lt;/br&gt;hej&lt;br&gt;&lt;/br&gt;</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="er">    </span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extra: How would we go ahead and remove HTML? Time to learn some Regex!</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.strip().lower()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>clean_text(\<span class="st">"Wow, we can clean text now. Isn't that amazing!?</span><span class="ch">\"</span><span class="st">).split()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="vectorization" class="level4">
<h4 class="anchored" data-anchor-id="vectorization">Vectorization</h4>
<p>Now that we can extract text we need to be able to input it to the system. We have to vectorize it. In this part we’ll vectorize each word as a number. The simplest approach to this is using <em>Bag of Words</em> (BOW).</p>
<p>Bag of Words creates a list of words which is called the <em>Dictionary</em>. The Dictionary is just a list of the words from the training data.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Training data: [\<span class="st">"ÅF is a big company</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">ÅF making future</span><span class="ch">\"</span><span class="st">]</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="er">--&gt; Dictionary: </span>[ÅF, <span class="kw">is</span>, a, big, company, making, future]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>New text: \<span class="st">"ÅF company is a future company</span><span class="ch">\"</span><span class="st"> --&gt; [1,1,1,0,2,0,1]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Our new text is vectorized on top of the dictionary. You take the dictionary and replace the words position with the count of it that is found in the new text.</p>
</section>
<section id="finalizing-the-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="finalizing-the-preprocessing">Finalizing the preprocessing</h4>
<p>We can actually do some more things to improve the system which I won’t go into detail about (read the code). We remove stop-words and so on.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>training_texts <span class="op">=</span> [</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    \<span class="st">"ÅF is a big company</span><span class="ch">\"</span><span class="st">, </span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="er">    \"ÅF making future\"</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>test_texts <span class="op">=</span> [</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    \<span class="st">"ÅF company is a future company</span><span class="ch">\"</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="er">]</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="er"># this is the vectorizer</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    stop_words<span class="op">=</span>\<span class="st">"english</span><span class="ch">\"</span><span class="st">,    # Removes english stop words (such as 'a', 'is' and so on.)</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="er">    preprocessor=clean_text  # Customized preprocessor</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the vectorizer on the training text</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>vectorizer.fit(training_texts)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># get the vectorizer's vocabulary</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>inv_vocab <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> vectorizer.vocabulary_.items()}</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>vocabulary <span class="op">=</span> [inv_vocab[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(inv_vocab))]</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co"># vectorization example</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    data<span class="op">=</span>vectorizer.transform(test_texts).toarray(),</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>[\<span class="st">"Test sentence</span><span class="ch">\"</span><span class="st">],</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="er">    columns=vocabulary</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="lets-do-something-fun-out-of-this" class="level3">
<h3 class="anchored" data-anchor-id="lets-do-something-fun-out-of-this">Let’s do something fun out of this!</h3>
<p>To begin with we need data. Luckily I know a perfect dataset for this – the IMDB movie reviews from stanford. This is a widely used dataset throughout <em>Sentiment Analysis</em>. The data contains 50 000 reviews where 50 % is positive and the rest negative. First we fetch a dataset. Download <a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">this file</a> and unpack it (into <code>aclImdb</code>) if the first code-snippet was unsuccessful.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_train_test_imdb_data(data_dir):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    \<span class="st">"</span><span class="ch">\"\"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="er">    Loads the IMDB train/test datasets from a folder path.</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    Input:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    data_dir: path to the \<span class="st">"aclImdb</span><span class="ch">\"</span><span class="st"> folder.</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="er">    </span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    Returns:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    train<span class="op">/</span>test datasets <span class="im">as</span> pandas dataframes.</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    \<span class="st">"</span><span class="ch">\"\"</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="er">    data = {}</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> split <span class="kw">in</span> [\<span class="st">"train</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">test</span><span class="ch">\"</span><span class="st">]:</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="er">        data</span>[split] <span class="op">=</span> []</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentiment <span class="kw">in</span> [\<span class="st">"neg</span><span class="ch">\"</span><span class="st">, </span><span class="ch">\"</span><span class="st">pos</span><span class="ch">\"</span><span class="st">]:</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="er">            score = 1 if sentiment == \"pos\" else 0</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            path <span class="op">=</span> os.path.join(data_dir, split, sentiment)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            file_names <span class="op">=</span> os.listdir(path)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> f_name <span class="kw">in</span> file_names:</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> <span class="bu">open</span>(os.path.join(path, f_name), \<span class="st">"r</span><span class="ch">\"</span><span class="st">) as f:</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="er">                    review = f.read</span>()</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                    data[split].append([review, score])</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We shuffle the data to make sure we don't train on sorted data. This results in some bad training.</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(data[\<span class="st">"train</span><span class="ch">\"</span><span class="st">])        </span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="er">    data</span>[\<span class="st">"train</span><span class="ch">\"</span><span class="st">] = pd.DataFrame(data[</span><span class="ch">\"</span><span class="st">train</span><span class="ch">\"</span><span class="st">],</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="er">                                 columns=</span>[<span class="st">'text'</span>, <span class="st">'sentiment'</span>])</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(data[\<span class="st">"test</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="er">    data</span>[\<span class="st">"test</span><span class="ch">\"</span><span class="st">] = pd.DataFrame(data[</span><span class="ch">\"</span><span class="st">test</span><span class="ch">\"</span><span class="st">],</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="er">                                columns=</span>[<span class="st">'text'</span>, <span class="st">'sentiment'</span>])</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data[\<span class="st">"train</span><span class="ch">\"</span><span class="st">], data[</span><span class="ch">\"</span><span class="st">test</span><span class="ch">\"</span><span class="st">]</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="er">train_data, test_data = load_train_test_imdb_data</span>(</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    data_dir<span class="op">=</span>\<span class="st">"aclImdb/</span><span class="ch">\"</span><span class="st">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="lets-create-our-classifier" class="level3">
<h3 class="anchored" data-anchor-id="lets-create-our-classifier">Let’s create our classifier</h3>
<p>We now have a dataset that we have successfully partitioned into a dictionary so that we can use it for our classifier.</p>
<p>Do you see an issue with our baseline right now?</p>
<p>…As mentioned we want to only have important features to simplify training. Right now we have an enormous amount of features, our BOW-approach result in an 80 000-dimensional vector. Because of this we <em>must</em> use simple algorithms that learn fast &amp; easy, e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Support-vector_machine">Linear SVM</a>, <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> or <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>.</p>
<p>Let’s create some code that actually let’s us train a Linear SVM!</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform each text into a vector of word counts</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(stop_words<span class="op">=</span>\<span class="st">"english</span><span class="ch">\"</span><span class="st">,</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="er">                             preprocessor=clean_text</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>training_features <span class="op">=</span> vectorizer.fit_transform(train_data[\<span class="st">"text</span><span class="ch">\"</span><span class="st">])    </span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="er">test_features = vectorizer.transform</span>(test_data[\<span class="st">"text</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="er"># Training</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearSVC()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>model.fit(training_features, train_data[\<span class="st">"sentiment</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="er">y_pred = model.predict</span>(test_features)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(test_data[\<span class="st">"sentiment</span><span class="ch">\"</span><span class="st">], y_pred)</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="er">print</span>(\<span class="st">"Accuracy on the IMDB dataset: </span><span class="sc">{:.2f}</span><span class="ch">\"</span><span class="st">.format(acc*100))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="comparison-to-state-of-the-art" class="level3">
<h3 class="anchored" data-anchor-id="comparison-to-state-of-the-art">Comparison to state-of-the-art</h3>
<p>Our accuracy is somewhere around 83.5-84 % which is really good! With this simple model and incredibly simplistic feature extraction we achieve a really high amount of correct answer! Comparing this to state-of-the-art we’re around 11 percent units beneat (~95% accuracy achieved <a href="https://arxiv.org/pdf/1801.06146.pdf">here</a>).</p>
<p>Incredible right? Exciting!? For me it is at least!</p>
<p>How do we improve from here?</p>
</section>
<section id="improving-the-model" class="level3">
<h3 class="anchored" data-anchor-id="improving-the-model">Improving the model</h3>
<p>We have some huge improvements to make outside of fine-tuning, so we’ll skip the fine-tuning from now.</p>
<p>The first step is to improve our vectorization.</p>
<section id="tf-idf" class="level4">
<h4 class="anchored" data-anchor-id="tf-idf">TF-IDF</h4>
<p>If you were at <em>first friday (<span class="citation" data-cites="ÅF">(<a href="#ref-ÅF" role="doc-biblioref"><strong>ÅF?</strong></a>)</span>)</em> you have heard about TF-IDF earlier. TF-IDF stands for <em>Term Frequence-Inverse Document Frequency</em> and is a measurement that aims to fight imbalances in texts.</p>
<p>In our vectorization step we look at the word-count meaning that we’ll have some biases to how much a word is present, the longer the text the more the bias. To reduce this we can take the word-count divided by the total amount of words in the text (TF). We also want to downscale the words that are incredibly frequent such as stop words and topic-related words, and upscale unusual words somewhat, e.g.<em>glamorous</em> might not be frequent but it is important to the text most likely. We use <em>IDF</em> for this. We then take these two and combine.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://cdn-images-1.medium.com/max/800/1*FgQgJYozG7colT9rys066w.png" class="img-fluid figure-img"></p>
<figcaption>alt text</figcaption>
</figure>
</div>
</section>
</section>
<section id="implementation-details" class="level3">
<h3 class="anchored" data-anchor-id="implementation-details">Implementation details</h3>
<p>This is actually really easy to do as <em>sklearn</em> already has a finished <code>TfIdfVectorizer</code> so all we have to do is to replace the <code>CountVectorizer</code>. Let’s see how it goes!</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform each text into a vector of word counts</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(stop_words<span class="op">=</span>\<span class="st">"english</span><span class="ch">\"</span><span class="st">,</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="er">                             preprocessor=clean_text</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>training_features <span class="op">=</span> vectorizer.fit_transform(train_data[\<span class="st">"text</span><span class="ch">\"</span><span class="st">])    </span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="er">test_features = vectorizer.transform</span>(test_data[\<span class="st">"text</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="er"># Training</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearSVC()</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>model.fit(training_features, train_data[\<span class="st">"sentiment</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="er">y_pred = model.predict</span>(test_features)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(test_data[\<span class="st">"sentiment</span><span class="ch">\"</span><span class="st">], y_pred)</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="er">print</span>(\<span class="st">"Accuracy on the IMDB dataset: </span><span class="sc">{:.2f}</span><span class="ch">\"</span><span class="st">.format(acc*100))</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="er"># Extra: Implement our own TfIdfVectorizer.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="conclusion-of-tf-idf" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-of-tf-idf">Conclusion of TF-IDF</h3>
<p>The <code>TfIdVectorizer</code> improved our scoring with 2 percent units, that’s incredible for such an easy improvement!</p>
<p>This for me shows how important it is to understand the data and what is important. You really need to grasp how to extract the important and what tools are available.</p>
<p>But let’s not stop here, lets reiterate and improve further.</p>
<p>What is the next natural step? Context I believe. During my master-thesis on spell correction of Street Names it was very obvious how important context is to increase the models understanding. Unfortunately we couldn’t use the context of a sentence in the thesis (as of the nature of street names) but here we can!</p>
</section>
<section id="use-of-context" class="level3">
<h3 class="anchored" data-anchor-id="use-of-context">Use of context</h3>
<p>Words by themself prove some meaning but sometimes they’re used in a negated sense, e.g.&nbsp;<em>not good</em>. <em>Good</em> in itself would most likely be positive but if we can get the context around the word we can be more sure about in what manner it is applied.</p>
<p>We call this <em>N-grams</em> where N is equal to the amount of words taken into consideration for each word. Using bigrams (N=2) we get the following:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>companies often use corporate bs <span class="op">=&gt;</span> [companies, often, use, slogans, (companies, often), (often,use), (use,slogans)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Sometimes you include a start &amp; ending word so that it would be <code>(\\t, companies)</code> and <code>(slogans, \\r)</code> or such. In this case as we are not finetuning we won’t go into that. We’ll keep it simple.</p>
<p>The all-mighty sklearn <code>TfIdfVectorizer</code> actually already have included N-gram support using the parameter <code>ngram_range=(1, N)</code>. So let’s make it simple for us and make use of that!</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> LinearSVC</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Transform each text into a vector of word counts</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>),</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>                            strip_accents<span class="op">=</span><span class="st">'ascii'</span>,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                            max_df<span class="op">=</span><span class="fl">0.98</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>training_features <span class="op">=</span> vectorizer.fit_transform(train_data[\<span class="st">"text</span><span class="ch">\"</span><span class="st">])    </span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="er">test_features = vectorizer.transform</span>(test_data[\<span class="st">"text</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="er"># Training</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearSVC()</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>model.fit(training_features, train_data[\<span class="st">"sentiment</span><span class="ch">\"</span><span class="st">])</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="er">y_pred = model.predict</span>(test_features)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluation</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(test_data[\<span class="st">"sentiment</span><span class="ch">\"</span><span class="st">], y_pred)</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="er">print</span>(\<span class="st">"Accuracy on the IMDB dataset: </span><span class="sc">{:.2f}</span><span class="ch">\"</span><span class="st">.format(acc*100))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="conclusion-of-n-gram" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-of-n-gram">Conclusion of N-gram</h3>
<p>Once again we see a massive improvement. We’re almost touching 89 % now! That’s just a mere 6 percent units below state-of-the-art. What can we do to improve now?</p>
<p>Some possible improvements for you to try!</p>
<ul>
<li>Use a custom threshold to reduce the dimensions</li>
<li>Play around with the <code>ngram_range</code> (don’t forget a threshold if you do this)</li>
<li>Improve the preprocessing</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try some fun things here if you want too :)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="conclusion-of-phase-1" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-of-phase-1">Conclusion of phase 1</h2>
<p>We have created a strong baseline for text classification with great accuracy for its simplicity. The following steps has been done</p>
<ul>
<li>First a simple preprocessing step which is of great importance. We have to remember to not make it to complex, the complexity of preprocessing is like an evil circle in the end. In our case we remove punctuations, stopwords and lower the case.</li>
<li>Secondly we vectorize the data to make it readable by the system. A classifier requires numerical features. For this we had a <code>TfIdfVectorizer</code> that computes frequency of words while downsampling words that are to common &amp; upsampling unusual words.</li>
<li>Finally we added N-gram to the model to increase the understanding of the sentence by supplying context.</li>
</ul>
</section>
<section id="phase-2" class="level2">
<h2 class="anchored" data-anchor-id="phase-2">Phase 2</h2>
<p>How do we improve from here? TF-IDF has its cons and pros. Some of the cons are that they:</p>
<ul>
<li>Don’t account for any kind of positioning at all</li>
<li>The dimensions are ridiculous large</li>
<li>They can’t capture semantics.</li>
</ul>
<p>Improvements upon this is made by using neural networks and word embeddings.</p>
</section>
<section id="word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="word-embeddings">Word Embeddings</h2>
<p>Word Embeddings &amp; Neural Networks are where we left off. By change our model to instead utilize these two concepts we can improve the accuracy once again.</p>
<section id="word-embeddings-1" class="level3">
<h3 class="anchored" data-anchor-id="word-embeddings-1">Word Embeddings</h3>
<p>Word Embeddings (WE) are actually a type of Neural Network. It uses <em>embedding</em> to create the model. I quickly explained WE during my presentation on Summarization and how to build a great summarizer. Today we’ll go a little more into depth.</p>
<p>To begin with I’ll take the most common example, WE lets us do the following arithmetiric with words:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>King <span class="op">-</span> Man <span class="op">+</span> Woman <span class="op">=</span> Queen</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is, in my opinion, completely amazing and fascinating. How does this work? Where do I learn more? Those are my first thoughts. In fact the theory is pretty basic until you get to the nittygritty details, as with most things.</p>
<p>WE is built on the concept ot learn how words are related to eachother. What company do a word have? To make the example more complex we can redefine this too the following: <code>A is to B what C is to D</code>.</p>
<p>Currently there is three "big" models that are widely used. The first one Word2Vec (<a href="https://arxiv.org/abs/1301.3781">Mikolov et al 2013</a>), the second is GloVe (MIT <a href="https://nlp.stanford.edu/projects/glove/">MIT</a>, <a href="https://nlp.stanford.edu/pubs/glove.pdf">Pennington et al 2014</a>) and the final one is fastText (<a href="https://github.com/facebookresearch/fastText">facebook</a>).</p>
<p>We will look into how you can achieve this without Deep Learning / Neural Networks unlike the models mentioned.</p>
<section id="step-1-how-to-represent-words-in-a-numerical-vector" class="level4">
<h4 class="anchored" data-anchor-id="step-1-how-to-represent-words-in-a-numerical-vector">Step 1: How to represent words in a numerical vector</h4>
<p>The first thing we have to do to actually understand/achieve word embeddings is to represent words in a numerical vector. In relation to this a quick explanation of sparse &amp; dense representations would be great. Read more in detail at <a href="https://en.wikipedia.org/wiki/Sparse_matrix">Wikipedia: Sparse Matrix</a></p>
<p><strong>Sparse representation</strong> is when we represent something very sparsely. It tells us that the points in the space is very few in regards to the dimensions and that most elements are empty. Think one-hot-encoding.</p>
<p>A <strong>Dense representation</strong> in comparison has few dimensions in comparison to possible values and most elements are filled. Think of something continuous.</p>
<p>The most simple way to represent words in a numerical vector is something we touched earlier, by one-hot-encoding them, i.e.&nbsp;a sparse representation.</p>
<p><img src="https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png" class="img-fluid" alt="Source :(Marco Bonzanini, 2017)">(Source: Marco Bonzanini, 2017)</p>
<p>Because of how languages are structured having one-hot-encoding means that we will have an incredibly sparse matrix (can be good) but it will have an enormous amount of dimensions (bad).</p>
<p>On top of this how would we go ahead and measure the distance between words? Normally one would use the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> but if we have a one-hot-encoding all the words would be orthogonal against eachother meaning that the dot-product will be zero.</p>
<p>Creating a dense representation however would indeed capture similarity as we could make use of cosine-similarity and more. Introducing Word2Vec.</p>
</section>
<section id="step-2-word2vec-representing-data-densely" class="level4">
<h4 class="anchored" data-anchor-id="step-2-word2vec-representing-data-densely">Step 2: Word2Vec, representing data densely</h4>
<p>The goal of Word2Vec, at least to my understanding, is to actually predict the context of a word. Or in other words we learn embeddings by prediciting the context of the word. The <em>context</em> here being the same definition as in N-grams. Word2Vec uses <em>shallow neural network</em> to learn word vectors so that each word is good at predicting its own contexts (more about his in <strong>Skip-Grams</strong>) and how to predict a word given a context (more about this in <strong>CBOW</strong>).</p>
</section>
<section id="skip-gram" class="level4">
<h4 class="anchored" data-anchor-id="skip-gram">Skip-gram</h4>
<p>Skip-gram very simplified is when you train on the N-grams but without the real word. <img src="https://cdn-images-1.medium.com/max/800/1*swlaqv7p_3xI4eL37C1pAA.png" class="img-fluid" alt="alt text"></p>
<p>As of now we have empirical results showing how this technique is very successful at learning the meaning of the words. On top of this the embedding that we get has both <em>direction of semantic and syntatic meaning</em> that are exposed in example such as <code>King - Man...</code>.</p>
<p>Another example would be: <code>Vector(Madrid) - Vector(Spain) + Vector(Sweden) ~ Vector(Stockholm)</code></p>
</section>
<section id="so-how-do-the-arithmetic-of-words-actually-work" class="level4">
<h4 class="anchored" data-anchor-id="so-how-do-the-arithmetic-of-words-actually-work">So how do the arithmetic of words actually work?</h4>
<p>I won’t go into details (some complicated math, see <a href="http://www.aclweb.org/anthology/P17-1007">Gittens et al</a>) but if we assume the following to be true:</p>
<ul>
<li>All words are distributed uniformly</li>
<li>The embedding model is linear</li>
<li>The conditional distributions of words are indepedent</li>
</ul>
<p>Then we can prove that the embedding of the paraphrase of a set of words is obtained by taking the sum over the embeddings of all of the individual words.</p>
<p>Using this result it’s easy to show how the famous man-woman, king-queen relationship works.</p>
<p>Extra note: You can show this then by havingn <code>King</code> and <code>Queen</code> having the same <code>Male-Female</code>relationship as the <code>King</code> then is the paraphrase of the set of words <code>{Queen, X}</code></p>
<p>I want to note that these assumptions are not 100 percent accurate. In reality word distributions are thought to follow Zipf’s law.</p>
</section>
<section id="glove" class="level4">
<h4 class="anchored" data-anchor-id="glove">GloVe</h4>
<p>A year after Word2Vec was a fact to the world the scientist decided to reiterate again. This time we got GloVe. GloVe tried to improve upon Word2Vec by that given a word its relationship(s) can be recovered from co-occurence statistics of a large corpus. GloVe is expensive and memory hungry, but it’s only one load so the issue isn’t that big. Nitty bitty details</p>
</section>
<section id="fasttext" class="level4">
<h4 class="anchored" data-anchor-id="fasttext">fastText</h4>
<p>With fastText one of the biggest problems is solved, both GloVe and Word2Vec only learn embeddings of word of the vocabulary. Because of this we can’t find an embedding for a word that isn’t in the dictionary.</p>
<p>Bojanowski et al solved this by learning the word embeddings using subword information. To summarize fastText learns embeddings of character n-grams instead.</p>
</section>
<section id="the-simple-way" class="level4">
<h4 class="anchored" data-anchor-id="the-simple-way">The simple way</h4>
<p>A simple approach to create your own word embeddings without a neural network is by factorizing a co-occurence matrix using SVD (singular-value-decomposition). As mentioned Word2Vec is barely a neural network as it has no hidden layers nor an y non-linearities. GloVe factorizes a co-occurense matrix while gaining even better results.</p>
<p>I highly recommend you to go check this blog out: https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ by Stitch Fix. An awesome read and we can go implement this too!</p>


</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="londogard/londogard" data-repo-id="MDEwOlJlcG9zaXRvcnkyOTcyNzE0MzE=" data-category="General" data-category-id="MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMwODYxNzM4" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://www.buymeacoffee.com/hlondogard">
<p><img src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" class="img-fluid" width="100"></p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/sponsors/Lundez?o=esb">
<p><img src="https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;color=%23fe8e86.png" class="img-fluid" width="100"></p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../pages/about.html">
<p>About</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hampus-londögård/">
      <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hlondogard">
      <i class="bi bi-twitter" role="img" aria-label="Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lundez">
      <i class="bi bi-github" role="img" aria-label="Lundez GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../index.xml">
      <i class="bi bi-rss" role="img" aria-label="Londogard Blog RSS">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>