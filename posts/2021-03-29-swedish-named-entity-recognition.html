<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hampus Lond√∂g√•rd">
<meta name="dcterms.date" content="2021-03-29">
<meta name="description" content="Learn how to fine-tune a Flair NER model and quantize a BERT model from Huggingface to achieve SotA performance &amp; a much more efficient model.">

<title>Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface) ‚Äì Londogard Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.ico" rel="icon">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bc163a3a22d509ec1bc4838f08992d42.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface) ‚Äì Londogard Blog">
<meta name="twitter:description" content="Learn how to fine-tune a Flair NER model and quantize a BERT model from Huggingface to achieve SotA performance &amp; a much more efficient model.">
<meta name="twitter:creator" content="@hlondogard">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Londogard Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../"> 
<span class="menu-text">Blogs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/presentations.html"> 
<span class="menu-text">Presentations</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://londogard.com/projects"> 
<span class="menu-text">Londogard Projects‚Üó</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../pages/about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../pages/code_setup/code_setup.html"> 
<span class="menu-text">Dev Setup</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hampus-lond√∂g√•rd/"> <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hlondogard"> <i class="bi bi-twitter" role="img" aria-label="Twitter">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-bi-github" role="link" data-bs-toggle="dropdown" aria-expanded="false">
      <i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-bi-github">    
        <li>
    <a class="dropdown-item" href="https://github.com/lundez"><i class="bi bi-github" role="img">
</i> 
 <span class="dropdown-text">Lundez</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/londogard"><i class="bi bi-github" role="img">
</i> 
 <span class="dropdown-text">Londogard</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml"> <i class="bi bi-rss" role="img" aria-label="Londogard Blog RSS">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Building a Swedish Named Entity Recognition (NER) model (Flair/Huggingface)</h1>
                  <div>
        <div class="description">
          Learn how to fine-tune a Flair NER model and quantize a BERT model from Huggingface to achieve SotA performance &amp; a much more efficient model.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">nlp</div>
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">workshop</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hampus Lond√∂g√•rd </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 29, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#building-a-swedish-named-entity-recognition-ner-model" id="toc-building-a-swedish-named-entity-recognition-ner-model" class="nav-link active" data-scroll-target="#building-a-swedish-named-entity-recognition-ner-model">Building a Swedish Named Entity Recognition (NER) model</a>
  <ul class="collapse">
  <li><a href="#named-entity-recognition-and-how-it-can-do-your-bidding" id="toc-named-entity-recognition-and-how-it-can-do-your-bidding" class="nav-link" data-scroll-target="#named-entity-recognition-and-how-it-can-do-your-bidding">Named Entity Recognition and how it can do your bidding</a></li>
  <li><a href="#flairing-the-way-to-success" id="toc-flairing-the-way-to-success" class="nav-link" data-scroll-target="#flairing-the-way-to-success">Flair(ing) the way to success</a></li>
  <li><a href="#swedish-data" id="toc-swedish-data" class="nav-link" data-scroll-target="#swedish-data">Swedish data</a></li>
  <li><a href="#training-the-flair" id="toc-training-the-flair" class="nav-link" data-scroll-target="#training-the-flair">Training the flair</a>
  <ul class="collapse">
  <li><a href="#setting-up-the-corpus-dataset" id="toc-setting-up-the-corpus-dataset" class="nav-link" data-scroll-target="#setting-up-the-corpus-dataset">Setting up the Corpus / Dataset</a></li>
  <li><a href="#model-setup" id="toc-model-setup" class="nav-link" data-scroll-target="#model-setup">Model Setup</a></li>
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">Training the model</a></li>
  <li><a href="#loading-model-from-checkpoint" id="toc-loading-model-from-checkpoint" class="nav-link" data-scroll-target="#loading-model-from-checkpoint">Loading model from checkpoint</a></li>
  </ul></li>
  <li><a href="#result" id="toc-result" class="nav-link" data-scroll-target="#result">Result</a></li>
  <li><a href="#deploying-on-streamlit.iosharing" id="toc-deploying-on-streamlit.iosharing" class="nav-link" data-scroll-target="#deploying-on-streamlit.iosharing">Deploying on streamlit.io/sharing</a></li>
  <li><a href="#outro" id="toc-outro" class="nav-link" data-scroll-target="#outro">Outro</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Not interested in reading the whole article and just wanna play around with the model(s)? Head over to <a href="https://londogard.com/ner">londogard.com/ner</a>. <img src="https://user-images.githubusercontent.com/7490199/113611097-543f9280-964e-11eb-9e59-bd030c32ad0c.png" class="img-fluid" alt="image"> <strong>P.S.</strong> The Flair model is available for simple installation through <a href="https://huggingface.co/londogard/flair-swe-ner">huggingface.co‚Äôs model hub</a> <!--truncate--></p>
<section id="building-a-swedish-named-entity-recognition-ner-model" class="level1">
<h1>Building a Swedish Named Entity Recognition (NER) model</h1>
<p>At <em>Londogard</em> we aim to employ Natural Language Processing (NLP) in a practical manner. The goal is not to create the models of OpenAI or Google, but rather something that is usable from the get-go and performant leading to a simple to use product.<br>
In this post I‚Äôll cover how we at Londogard developed a State-of-the-Art (SotA) Named Entity Recognition (NER) model for Swedish using Flair &amp; huggingface. :tada:</p>
<p>It all started last weekend when I was allowed into the <a href="https://streamlit.io/">streamlit.io</a>‚Äôs <em>sharing</em> beta.<br>
If you don‚Äôt know what streamlit, here‚Äôs an excerpt from their frontpage: &gt;<strong>The fastest way to build and share data apps</strong> &gt;Streamlit turns data scripts into shareable web apps in minutes.<br>
&gt;All in Python. All for free. No front‚Äëend experience required.</p>
<p>Essentially streamlit is a way to combine backend &amp; frontend into a unified script-like experience where the default UI looks pretty good. On top of this script-like experience streamlit has built a powerful yet simple to use cache system.</p>
<blockquote class="blockquote">
<p>In my opinion creating demos has never been simpler than with streamlit.io</p>
</blockquote>
<p>Back to the problem at hands, I wished to deploy a model through streamlit that actually was a meaningful experience where efficiency and performance are combined, according to the Londogard motto.<br>
As such I embarked on the journey that was to deploy a NER model for <em>Swedish</em> where Swedish actually isn‚Äôt all to common in NLP. Lately <em>Kungliga Biblioteket</em> has been trying to improve this through their <a href="https://github.com/Kungbib/swedish-spacy">spaCy-contribution</a>, which yet has to be included in spaCy, and their <a href="https://huggingface.co/KB/">HuggingFace-contributions</a> where we can find BERT, Electra &amp; Albert pre-trained.<br>
My first idea was to take one of these and fine-tune to finally deploy, but the size of BERT is too large as is.</p>
<p>What choices are left to allow deploy of these models? - Distilling ‚öóÔ∏è - Quantizing - Fine-tuning ALBERT on NER - Performance has been shown to be quite a bit below BERT (7% units) in a paper by KTH, for Swedish.</p>
<p><em>So what did I do?</em> I did as any other professional and google‚Äôd.<br>
A library I hadn‚Äôt heard the name of in a year popped up at the top of the results, I was intrigued.<br>
Flair, a library that was created by Zalando Research, now under the flag <code>/flairnlp</code> which in practice means that the core contributor-group has been changed to Humbold-University of Berlin.<br>
Flair contains the so-called <em>Flair Embeddings</em> which are contextual embeddings of high quality. Flair retains SotA for NER in multiple languages through these and the performance is pretty damn good over all.</p>
<p>Before I dive into the details on how I trained my own model you can find a demo on <a href="https://londogard.com/ner">londogard.com/ner</a>, where the model is deployed through <a href="https://streamlit.io/">streamlit.io</a>.</p>
<section id="named-entity-recognition-and-how-it-can-do-your-bidding" class="level2">
<h2 class="anchored" data-anchor-id="named-entity-recognition-and-how-it-can-do-your-bidding">Named Entity Recognition and how it can do your bidding</h2>
<p>As the name suggests NER is the task to recognize entities in text. Entities can be a lot of different things such as the obvious <em>Person</em> but also <em>Location</em>, <em>Organisation</em> &amp; <em>Time</em>. More entities exists and they can really become whatever your data allows (<em>Brand</em>, <em>Medicine</em> or <em>Dosage</em>? You got it!)</p>
<p><strong>Practical use-cases of NER</strong> 1. Automatic anonymization of data 2. Medical prescription 3. Automatically tag data - e.g.&nbsp;News tagged by Organisations, Persons &amp; Locations included</p>
<p>‚Ä¶ &amp; much more</p>
<p>In my case I‚Äôm simply aiming for the traditional NER model which categorize things like <em>Location</em>, <em>Person</em> &amp; <em>Organisation</em>.</p>
</section>
<section id="flairing-the-way-to-success" class="level2">
<h2 class="anchored" data-anchor-id="flairing-the-way-to-success">Flair(ing) the way to success</h2>
<p>Flair is a SotA NLP library developed by <a href="https://www.informatik.hu-berlin.de/en/forschung-en/gebiete/ml-en/">Humboldt University of Berlin</a> and friends. As mentioned its core contributors are from Humboldt University of Berlin and the whole idea is to provide contextual embeddings. Some of the things provided through Flair:</p>
<ol type="1">
<li>Flair Embeddings</li>
<li>(Easily) Stacked Embeddings
<ul>
<li>e.g.&nbsp;combine Transformer, Flair &amp; GloVe for your end-model</li>
</ul></li>
<li>Easy access to multiple embeddings
<ul>
<li>GloVe, Transformer, ELMo &amp; many more</li>
</ul></li>
<li>Simple training of high-performant NER (Token Classifier) Model and a Text Classifier model</li>
</ol>
<p>As mentioned Flair retain SotA in multiple languages for NER, but they do the same for POS.</p>
<p><strong>The Language Model</strong><br>
If you‚Äôre curious the simplest Flair embeddings are essentially a Language Model built on Dropout, LSTM &amp; a Linear Layer. Pretty simple.</p>
<p><strong>The Token Classifier (NER/POS)</strong><br>
It‚Äôs based on a small LSTM-network with a CRF on top. The LSTM exists to create features for the CRF to learn and tag from. This is a very common approach which yields high accuracy. If you‚Äôre aware of what features you wish to use a pure CRF can be very strong, Stanford NLP library was actually for very long based on a CRF and had SotA, but the manual feature engineering can be expensive &amp; hard.</p>
<p><strong>The Text Classifier</strong> Simply a linear layer on top of the embeddings.</p>
<p><strong>More Models</strong><br>
Flair actually supports two other tasks, <em>Text Regression</em> &amp; <em>Similarity</em> but I won‚Äôt go in to those.</p>
<p>More about how I trained my NER will come a bit further down. To read more about Flair and how they work please check out their <a href="https://github.com/flairNLP/flair">GitHub</a> which also links to the papers.</p>
</section>
<section id="swedish-data" class="level2">
<h2 class="anchored" data-anchor-id="swedish-data">Swedish data</h2>
<p>First of all I had to go find some data and I found cr√®me de la cr√®me in <a href="https://spraakbanken.gu.se/en/resources/suc3">SUC 3.0</a>, because we really do sentence by sentence training in NER it‚Äôs not the end of the world that the ‚Äòfree‚Äô variant that doesn‚Äôt require a research licence is scrambled. Unscrambled data would lead to a better model but it‚Äôs still doable.</p>
<p>But as Jeremy Howard proposes, start with small and simple data then expand into your full task. SUC 3.0 is pretty large and slow to train. With some fast googling I found a saviour, <em>klintan</em>. Klintan has created a open Swedish NER dataset based on Webbnyheter 2020 from Spr√•kbanken, it‚Äôs semi-manually annotated. This means that he first based it on <em>Gazetters</em>, essentially dataset(s) of entities, and then manually reviewed the data with two different native Swedish Speakers. More people have later added some improvements on top of that, find the full dataset <a href="https://github.com/klintan/swedish-ner-corpus">here</a>, but please note that <em>it‚Äôs much smaller</em> than SUC 3.0.<br>
After finding this dataset I read more into Flair and I found out that they actually provide this dataset through their API and in this dataset we have 4 categories PER, ORG, LOC and MISC.</p>
<p>With these two datasets in mind I went ahead to train.</p>
</section>
<section id="training-the-flair" class="level2">
<h2 class="anchored" data-anchor-id="training-the-flair">Training the flair</h2>
<p>First let me say the <a href="https://github.com/flairNLP/flair/tree/master/resources/docs">documentation</a> is actually pretty good! First part is to set up the <code>Corpus</code>.</p>
<section id="setting-up-the-corpus-dataset" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-the-corpus-dataset">Setting up the Corpus / Dataset</h3>
<p><strong>The built-in <em>klintan/ner-swedish-corpus</em></strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. get the corpus</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>corpus: Corpus <span class="op">=</span> NER_SWEDISH()</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(corpus)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. what tag do we want to predict?</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>tag_type <span class="op">=</span> <span class="st">'ner'</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. make the tag dictionary from the corpus</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>tag_dictionary <span class="op">=</span> corpus.make_tag_dictionary(tag_type<span class="op">=</span>tag_type)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tag_dictionary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Custom dataset (SUC 3.0, in my case scrambled)</strong><br>
Remember to convert the SUC tags into a IOB format before training. Emil Stenstr√∂m has kindly created a simple Python-script for this available through <a href="https://github.com/EmilStenstrom/suc_to_iob">github.com/EmilStenstrom/suc_to_iob</a>. First transform the data and later you can run the following code</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> {<span class="dv">0</span>: <span class="st">'text'</span>, <span class="dv">1</span>: <span class="st">'ner'</span>}</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># this is the folder in which train, test and dev files reside</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>data_folder <span class="op">=</span> <span class="st">'path/to/data/suc'</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># init a corpus using column format, data folder and the names of the train, dev and test files</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>corpus: Corpus <span class="op">=</span> ColumnCorpus(data_folder, columns, train_file<span class="op">=</span><span class="st">'train.txt'</span>, test_file<span class="op">=</span><span class="st">'test.txt'</span>, dev_file<span class="op">=</span><span class="st">'dev.txt'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. what tag do we want to predict?</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>tag_type <span class="op">=</span> <span class="st">'ner'</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. make the tag dictionary from the corpus</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>tag_dictionary <span class="op">=</span> corpus.make_tag_dictionary(tag_type<span class="op">=</span>tag_type)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tag_dictionary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With this in mind we‚Äôre ready to set up our model for training.</p>
</section>
<section id="model-setup" class="level3">
<h3 class="anchored" data-anchor-id="model-setup">Model Setup</h3>
<p>Our model will build on <code>FlairEmbeddings</code> (e.g.&nbsp;contextual embeddings) and <code>BytePairEmbeddings</code> which are a bit like classic <code>WordEmbeddings</code> but done on BPE-tokenized text. This is a really interesting approach which achieves similar performance as <code>fastText</code> using ~ 0.2 % of the total size (11mb vs 6gb).<br>
The model itself will use a LSTM with a hidden size of 256 and a CRF classifier on top.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. initialize embeddings</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>embedding_types <span class="op">=</span> [</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># WordEmbeddings('sv'), # uncomment to add WordEmb</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    BytePairEmbeddings(<span class="st">'sv'</span>),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    FlairEmbeddings(<span class="st">"sv-forward"</span>),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    FlairEmbeddings(<span class="st">"sv-backward"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>embeddings: StackedEmbeddings <span class="op">=</span> StackedEmbeddings(embeddings<span class="op">=</span>embedding_types)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. initialize sequence tagger</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>tagger: SequenceTagger <span class="op">=</span> SequenceTagger(hidden_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                                        embeddings<span class="op">=</span>embeddings,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                                        tag_dictionary<span class="op">=</span>tag_dictionary,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>                                        tag_type<span class="op">=</span>tag_type,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>                                        use_crf<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">Training the model</h3>
<p>Because I run through google colab and the machine can be terminated any second I run using <code>checkpoint=True</code> which means you can continue training where you left off. My models are saved to my Google Drive, real handy! &gt;Pro-tip: use <code>checkpoint=True</code> in combination with Google Drive on your Google Colab.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. start training</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>trainer.train(<span class="st">'drive/MyDrive/path/to/model/save/'</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                learning_rate<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                <span class="co"># set chunk size to lower memory requirements</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                <span class="co">#mini_batch_chunk_size=16,</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                mini_batch_size<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                checkpoint<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                embeddings_storage_mode<span class="op">=</span><span class="st">'none'</span>, <span class="co"># only required for SUC 3.0 which grows too large</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                <span class="co">#batch_growth_annealing=True,</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                <span class="co">#anneal_with_restarts=True,</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                max_epochs<span class="op">=</span><span class="dv">150</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="loading-model-from-checkpoint" class="level3">
<h3 class="anchored" data-anchor-id="loading-model-from-checkpoint">Loading model from checkpoint</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> ModelTrainer.load_checkpoint(<span class="st">'drive/MyDrive/path/to/model/save/checkpoint.pt'</span>, corpus)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>And that‚Äôs it!</p>
</section>
</section>
<section id="result" class="level2">
<h2 class="anchored" data-anchor-id="result">Result</h2>
<p>For me the results looks really good and close to what I expected, I had hoped that Flair would achieve at least 0.88+ F1 but 0.855 isn‚Äôt too bad. The size, speed and simplicity of Flair makes it a great contender!</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Dataset</th>
<th>Size</th>
<th>Avg F1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>klintan/swedish-ner-corpus</td>
<td>320MB</td>
<td>~<strong>0.89</strong></td>
</tr>
<tr class="even">
<td>SUC 3.0 (PER, LOC &amp; ORG)</td>
<td>320MB</td>
<td>~<strong>0.89</strong></td>
</tr>
<tr class="odd">
<td>SUC 3.0 (PER, LOC, ORG, TME, MSR, ‚Ä¶)</td>
<td>320MB</td>
<td><strong>0.855</strong></td>
</tr>
<tr class="even">
<td>SUC 3.0 (PER, LOC, ORG, TME, MSR, ‚Ä¶) Quantized</td>
<td>80MB</td>
<td><strong>0.853</strong></td>
</tr>
<tr class="odd">
<td>SUC 3.0 (PER, LOC, ORG, TME, MSR, ‚Ä¶) w/ ALBERT</td>
<td>50MB</td>
<td><strong>0.85</strong> (via <a href="http://kth.diva-portal.org/smash/get/diva2:1451804/FULLTEXT01.pdf">KTH</a>)</td>
</tr>
<tr class="even">
<td>SUC 3.0 (PER, LOC, ORG, TME, MSR, ‚Ä¶) w/ BERT (<a href="https://github.com/Kungbib/swedish-bert-models#bert-base-fine-tuned-for-swedish-ner">KungBib</a>)</td>
<td>480MB</td>
<td><strong>0.928</strong></td>
</tr>
<tr class="odd">
<td>SUC 3.0 (PER, LOC, ORG, TME, MSR, ‚Ä¶) w/ BERT Quantized</td>
<td>120MB</td>
<td><strong>0.928</strong></td>
</tr>
</tbody>
</table>
<p>I believe it‚Äôs important to note that Quantized models are also much faster running ~ 4 times faster (avg 360ms went to 80ms on a CPU for flair).<br>
Quantization updates the f32 into int8 which allows the model to more efficiently utilize CPU and the ONNX-runtime also makes the whole model better at using CPU-instructions.</p>
</section>
<section id="deploying-on-streamlit.iosharing" class="level2">
<h2 class="anchored" data-anchor-id="deploying-on-streamlit.iosharing">Deploying on streamlit.io/sharing</h2>
<p>And for the final part! :tada: First you need a new public repository on GitHub with the streamlit &amp; model code. This requires to set up a <code>requirements.txt</code> with all necessary dependencies.</p>
<p>Then you need to figure out how you‚Äôll host your model if it‚Äôs too large. I found GitHub LFS to work out decently, but the cap was pretty small (1GB / Month) and I broke the limit on my 3rd model. I went ahead and registered on <a href="https://www.backblaze.com">Backblaze</a> which has great reviews, but I think the best solution in my shoes would be to host it through HuggingFace Model storage (free if public!). <strong>edit:</strong> I actually ended up storing the flair model on <a href="https://huggingface.co/londogard/flair-swe-ner">huggingface.co/londogard/flair-swe-ner</a> ü§ó.</p>
<p>Setting up the script itself was quite easy for Flair.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load tagger for POS and</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="at">@st.cache</span>(allow_output_mutation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_model():</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    tagger <span class="op">=</span> SequenceTagger.load(<span class="st">'best-model-large-data.pt'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tagger</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="at">@st.cache</span>(allow_output_mutation<span class="op">=</span><span class="va">True</span>, hash_funcs<span class="op">=</span>{SequenceTagger: <span class="kw">lambda</span>  _: <span class="va">None</span>})</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(model, text):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    manual_sentence <span class="op">=</span> Sentence(manual_user_input)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    model.predict(manual_sentence)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> render_ner_html(manual_sentence, wrap_page<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>tagger <span class="op">=</span> load_model()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>st.title(<span class="st">"Swedish Named Entity Recognition (NER) tagger"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>st.subheader(<span class="st">"Created by [Londogard](https://londogard.com) (Hampus Lond√∂g√•rd)"</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>st.title(<span class="st">"Please type something in the box below"</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>manual_user_input <span class="op">=</span> st.text_area(<span class="st">""</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(manual_user_input) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> predict(tagger, manual_user_input)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    st.success(<span class="st">"Below is your tagged string."</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    st.write(sentence, unsafe_allow_html<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>It‚Äôs important to note how I‚Äôve placed the caching solution. I both cache the model loading &amp; predictions to keep it as speedy as possible.</p>
<p>The <code>allow_output_mutation</code> option skips hashing the output to validate that the cache is correct, we don‚Äôt care if output has been modified really.</p>
<p>The <code>hash_funcs={SequenceTagger: lambda  _: None}</code> is <strong>incredibly important</strong>.<br>
The flair model are pretty slow to hash, especially if quantized. It‚Äôs possible to use <code>id</code> which is a unique ID for the python object that lasts the full lifetime, but because I know that the model wont change I simply use <code>lambda _: None</code> to not do any lookup at all.<br>
If the model input would change in-between using <code>id</code> is the best approach. Note that neither of this approaches are any good if you wanna compare an object to another (e.g.&nbsp;two string inputs), there we should just keep standard hashing.</p>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>I trained Flair embeddings which is a much simpler approach than Transformers and achieved almost SotA while having a much smaller &amp; simpler model (~2/3rd of the size). But in the end I was very impressed by how well quantization applies for CPU utilization so I also applied the same approach for BERT-ner by KB, where I even did a ONNX Quantization which has been shown to be even more effective than PyTorch own quantization, but then again it requires the ONNX runtime.</p>
<p>Both models are available on the same device / streamlit configuration, find them on <a href="https://londogard.com/ner">londogard.com/ner</a>.<br>
The flair model is available through HuggingFace ü§ó through the following: <a href="https://huggingface.co/londogard">londogard (huggingface.co)</a>.</p>
<p>Thanks for this time, Hampus Lond√∂g√•rd @ Londogard</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/blog\.londogard\.com\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="londogard/londogard" data-repo-id="MDEwOlJlcG9zaXRvcnkyOTcyNzE0MzE=" data-category="General" data-category-id="MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMwODYxNzM4" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://www.buymeacoffee.com/hlondogard">
<p><img src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" class="img-fluid" width="100"></p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="https://github.com/sponsors/Lundez?o=esb">
<p><img src="https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;color=%23fe8e86.png" class="img-fluid" width="100"></p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../pages/about.html">
<p>About</p>
</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/hampus-lond√∂g√•rd/">
      <i class="bi bi-linkedin" role="img" aria-label="LinkedIn">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/hlondogard">
      <i class="bi bi-twitter" role="img" aria-label="Twitter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lundez">
      <i class="bi bi-github" role="img" aria-label="Lundez GitHub">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../index.xml">
      <i class="bi bi-rss" role="img" aria-label="Londogard Blog RSS">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>