<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>[2019-02-04] AFRY NLP Competence Meeting: Text Classification IMDB | Londogard</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="[2019-02-04] AFRY NLP Competence Meeting: Text Classification IMDB" />
<meta name="author" content="Hampus Londögård" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment)." />
<meta property="og:description" content="This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment)." />
<link rel="canonical" href="https://blog.londogard.com/machine-learning/nlp/workshop/2020/02/23/competence-meeting-imdb-text-classification.html" />
<meta property="og:url" content="https://blog.londogard.com/machine-learning/nlp/workshop/2020/02/23/competence-meeting-imdb-text-classification.html" />
<meta property="og:site_name" content="Londogard" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-02-23T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://blog.londogard.com/machine-learning/nlp/workshop/2020/02/23/competence-meeting-imdb-text-classification.html","@type":"BlogPosting","headline":"[2019-02-04] AFRY NLP Competence Meeting: Text Classification IMDB","dateModified":"2020-02-23T00:00:00-06:00","datePublished":"2020-02-23T00:00:00-06:00","author":{"@type":"Person","name":"Hampus Londögård"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.londogard.com/machine-learning/nlp/workshop/2020/02/23/competence-meeting-imdb-text-classification.html"},"description":"This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment).","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.londogard.com/feed.xml" title="Londogard" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header" role="banner">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Londogard</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/education/">Educational</a><a class="page-link" href="/londogard">Londogard↗</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
        </nav></div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">[2019-02-04] AFRY NLP Competence Meeting: Text Classification IMDB</h1><p class="page-description">This blog contains my first Competence Meeting where basic NLP concepts where taught and an classifier with good performance was implemented (on IMDB sentiment).</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-02-23T00:00:00-06:00" itemprop="datePublished">
        Feb 23, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Hampus Londögård</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      18 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#machine-learning">machine-learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#workshop">workshop</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#2019-02-04-afry-nlp-competence-meeting-text-classification-imdb">2019-02-04 AFRY NLP Competence Meeting: Text Classification IMDB</a>
<ul>
<li class="toc-entry toc-h3"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#text-classification">Text Classification</a>
<ul>
<li class="toc-entry toc-h3"><a href="#a-good-baseline">A good baseline</a></li>
<li class="toc-entry toc-h3"><a href="#classes--features">Classes &amp; Features</a>
<ul>
<li class="toc-entry toc-h4"><a href="#one-hot-encoding---how-we-represent-features--classes">One-Hot-Encoding - how we represent features &amp; classes</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#back-to-text-classification">Back to text classification</a></li>
<li class="toc-entry toc-h3"><a href="#preprocessing">Preprocessing</a>
<ul>
<li class="toc-entry toc-h4"><a href="#cleaning-the-text">Cleaning the text</a></li>
<li class="toc-entry toc-h4"><a href="#vectorization">Vectorization</a></li>
<li class="toc-entry toc-h4"><a href="#finalizing-the-preprocessing">Finalizing the preprocessing</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#lets-do-something-fun-out-of-this">Let’s do something fun out of this!</a></li>
<li class="toc-entry toc-h3"><a href="#lets-create-our-classifier">Let’s create our classifier</a></li>
<li class="toc-entry toc-h3"><a href="#comparison-to-state-of-the-art">Comparison to state-of-the-art</a></li>
<li class="toc-entry toc-h3"><a href="#improving-the-model">Improving the model</a>
<ul>
<li class="toc-entry toc-h4"><a href="#tf-idf">TF-IDF</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#implementation-details">Implementation details</a></li>
<li class="toc-entry toc-h3"><a href="#conclusion-of-tf-idf">Conclusion of TF-IDF</a></li>
<li class="toc-entry toc-h3"><a href="#use-of-context">Use of context</a></li>
<li class="toc-entry toc-h3"><a href="#conclusion-of-n-gram">Conclusion of N-gram</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#conclusion-of-phase-1">Conclusion of phase 1</a></li>
<li class="toc-entry toc-h2"><a href="#phase-2">Phase 2</a></li>
<li class="toc-entry toc-h2"><a href="#word-embeddings">Word Embeddings</a>
<ul>
<li class="toc-entry toc-h3"><a href="#word-embeddings-1">Word Embeddings</a>
<ul>
<li class="toc-entry toc-h4"><a href="#step-1-how-to-represent-words-in-a-numerical-vector">Step 1: How to represent words in a numerical vector</a></li>
<li class="toc-entry toc-h4"><a href="#step-2-word2vec-representing-data-densely">Step 2: Word2Vec, representing data densely</a></li>
<li class="toc-entry toc-h4"><a href="#skip-gram">Skip-gram</a></li>
<li class="toc-entry toc-h4"><a href="#so-how-do-the-arithmetic-of-words-actually-work">So how do the arithmetic of words actually work?</a></li>
<li class="toc-entry toc-h4"><a href="#glove">GloVe</a></li>
<li class="toc-entry toc-h4"><a href="#fasttext">fastText</a></li>
<li class="toc-entry toc-h4"><a href="#the-simple-way">The simple way</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul><h1 id="2019-02-04-afry-nlp-competence-meeting-text-classification-imdb">
<a class="anchor" href="#2019-02-04-afry-nlp-competence-meeting-text-classification-imdb" aria-hidden="true"><span class="octicon octicon-link"></span></a>2019-02-04 AFRY NLP Competence Meeting: Text Classification IMDB</h1>

<p>I’ve set a goal to create one blog post per Competence Meeting I’ve held at AFRY to spread the knowledge further. This goal will also grab all the older meetings, my hope is that I’ll be finished before summer 2020, but we’ll see.</p>

<hr>

<h3 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h3>

<p>Most of my Competence Meetings take place in the form of Jupyter Notebooks (<code class="language-plaintext highlighter-rouge">.ipynb</code>). Notebooks are awesome as they allow us to:</p>

<ol>
  <li>Mix and match <em>markdown</em> &amp; <em>code</em>-blocks</li>
  <li>Keep the state of the program, i.e. very explorative</li>
</ol>

<p>This is really good in combination with the workshop-format that we usually have. 
Using services such as <a href="colab.research.google.com">Google Colab</a> one can take the file and open it in the browser and run it there. This means that we don’t need any downloads and pretty often we also have a speed gain because the node used is faster than a laptop with its GPU.</p>

<p>Let’s get on to the competence evening.</p>

<hr>

<h2 id="text-classification">
<a class="anchor" href="#text-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Text Classification</h2>

<p>Today we’ll go through text classification, what it is, how it is used and how to make it yourself while trying to keep have a great mix of both theory and practical use. Text classification is just what the name suggest, a way to classify texts. Let it be spam or reviews, you train it and it’ll predict what class the text belongs to.</p>

<hr>

<h3 id="a-good-baseline">
<a class="anchor" href="#a-good-baseline" aria-hidden="true"><span class="octicon octicon-link"></span></a>A good baseline</h3>

<p>To have a good baseline is incredibly important in Machine Learning. In summary you want the following</p>

<ul>
  <li>Simple model to predict outcome</li>
  <li>Use this model to compare your new, more complex model to</li>
</ul>

<p>This is to be able to know what progress you’re making. You don’t want to do anything more complex without any gains.</p>

<p>One pretty common simple baseline is just to pick a random class as prediction.</p>

<h3 id="classes--features">
<a class="anchor" href="#classes--features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Classes &amp; Features</h3>

<p>What is a class and feature?</p>

<p>Features are the input to the model, you can see a machine learning system as a "consumer" of features. You can view this as a cookie monster consuming cookies and then he says if they taste good or bad. He has the input, cookie, that can be a feature. He then has a output, class, that is good/bad. Repeat this a lot of times and you can retrieve statistics if Cookie Y is good or bad.</p>

<p>To generalize this system we would divide the feature into multiple feature, like what ingredients the cookie contains. So instead of saying this is a "Chocolate Chip Cookie" we know tell the system the features are:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chocolate</span><span class="p">:</span> <span class="n">yes</span>
<span class="n">sugar</span><span class="p">:</span><span class="n">yes</span>
<span class="n">honey</span><span class="p">:</span><span class="n">no</span>
<span class="n">oat</span><span class="p">:</span><span class="n">no</span>
<span class="n">cinnamon</span><span class="p">:</span> <span class="n">no</span>
<span class="n">sweet</span><span class="p">:</span> <span class="n">yes</span>
<span class="n">sour</span><span class="p">:</span> <span class="n">no</span>\<span class="s">"
</span></code></pre></div></div>

<p>. In numerical input it would translate to something as <code class="language-plaintext highlighter-rouge">[1,1,0,0,0,1,0]</code>.</p>

<h4 id="one-hot-encoding---how-we-represent-features--classes">
<a class="anchor" href="#one-hot-encoding---how-we-represent-features--classes" aria-hidden="true"><span class="octicon octicon-link"></span></a>One-Hot-Encoding - how we represent features &amp; classes</h4>

<p>As shown in the translation to numerical vectors we don’t represent words as actual words. We always use numbers, often we even use something called <em>One-Hot-Encoding</em>.</p>

<p>One-Hot-Encoding means that we have an array of one 1 and the rest is 0s. This is to optimize math performed by the GPU (or CPU).</p>

<p>Using the example of <em>Good</em> &amp; <em>Bad</em> cookies with the extension of <em>Decent</em> we will One-Hot-Encode these as the following</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Good</span>   <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Bad</span>    <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Decent</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>The same is applied to our features. If you’re using a framework (such as Keras) it is pretty common that they include an method to do this, or even that it is done automatically for you.</p>

<h3 id="back-to-text-classification">
<a class="anchor" href="#back-to-text-classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>Back to text classification</h3>

<p>To classify a text we do what is called an <em>sentiment analysis</em> meaning that we try to estimate the <em>sentiment polarity</em> of a text body. In the first part of this workshop we’ll be assuming that there’s only two sentiments, <em>Negative</em> and <em>Positive</em>. Then we can express this as the following classification problem:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Feature</span><span class="p">:</span> <span class="n">String</span> <span class="n">body</span>
<span class="n">Class</span><span class="p">:</span>   <span class="n">Bad</span><span class="o">|</span><span class="n">Good</span>
</code></pre></div></div>

<p>The output, <em>Classes</em>, are easy to One-Hot-Encode but how do we succesfully One-Hot-Encode a string? A character can be seen as a class but is that really something we can learn from? To solve this we need to preprocess our input somehow.</p>

<h3 id="preprocessing">
<a class="anchor" href="#preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Preprocessing</h3>

<p>Preprocessing is an incredibly important part of Machine Learning. Combining preprocessing with <em>Data Mining</em> is actually around 70% of the workload (IBM) when developing models through the CRISP-DM. From my experience this is true.</p>

<p>Having good data and finding the most important features is incredibly important to have a competent system. In this task we need to preprocess the text to simplify the learning process for our system. We will do the following:</p>

<ul>
  <li>Clean the text</li>
  <li>Vectorize the texts into numerical vectors</li>
</ul>

<h4 id="cleaning-the-text">
<a class="anchor" href="#cleaning-the-text" aria-hidden="true"><span class="octicon octicon-link"></span></a>Cleaning the text</h4>

<p>Why do we need to clean the text? It is to remove weird stuff &amp; outliers. If we have the text <code class="language-plaintext highlighter-rouge">I'm a cat.</code>we want to simplify this into <code class="language-plaintext highlighter-rouge">[i'm, a, cat]</code> or even <code class="language-plaintext highlighter-rouge">[im, a, cat]</code>.</p>

<p>Removing data such as non-alphabetical characters and the letter case makes more data look a like and reduces the dimension of our input – this simplifies the learning of the system. But removing features can be bad also, if someone writes in all CAPS we can guess that they’re angry. But let’s take that later.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">regex</span> <span class="k">as</span> <span class="n">re</span>


<span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    \<span class="s">"</span><span class="se">\"\"</span><span class="s">
    Applies some pre-processing on the given text.

    Steps :
    - Removing punctuation
    - Lowering text
    </span><span class="se">\"\"\"</span><span class="s">
    
    # remove the characters [</span><span class="se">\\</span><span class="s">], ['] and [</span><span class="se">\"</span><span class="s">]
    text = re.sub(r</span><span class="se">\"\\\\\"</span><span class="s">, </span><span class="se">\"\"</span><span class="s">, text)    
    text = re.sub(r</span><span class="se">\"\\</span><span class="s">'</span><span class="se">\"</span><span class="s">, </span><span class="se">\"\"</span><span class="s">, text)    # Extra: Is regex needed? Other ways to accomplish this.
    text = re.sub(r</span><span class="se">\"\\\"\"</span><span class="s">, </span><span class="se">\"\"</span><span class="s">, text)
    # replace all non alphanumeric with space 
    text = re.sub(r</span><span class="se">\"\\</span><span class="s">W+</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s"> </span><span class="se">\"</span><span class="s">, text)
    # text = re.sub(r</span><span class="se">\"</span><span class="s">&lt;.+?&gt;</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s"> </span><span class="se">\"</span><span class="s">, text) # &lt;br&gt;&lt;/br&gt;hej&lt;br&gt;&lt;/br&gt;
    
    # Extra: How would we go ahead and remove HTML? Time to learn some Regex!
    
    return text.strip().lower()
clean_text(</span><span class="se">\"</span><span class="s">Wow, we can clean text now. Isn't that amazing!?</span><span class="se">\"</span><span class="s">).split()
</span></code></pre></div></div>

<h4 id="vectorization">
<a class="anchor" href="#vectorization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Vectorization</h4>

<p>Now that we can extract text we need to be able to input it to the system. We have to vectorize it. In this part we’ll vectorize each word as a number. The simplest approach to this is using <em>Bag of Words</em> (BOW).</p>

<p>Bag of Words creates a list of words which is called the <em>Dictionary</em>. The Dictionary is just a list of the words from the training data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Training</span> <span class="n">data</span><span class="p">:</span> <span class="p">[</span>\<span class="s">"ÅF is a big company</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s">ÅF making future</span><span class="se">\"</span><span class="s">]
--&gt; Dictionary: [ÅF, is, a, big, company, making, future]

New text: </span><span class="se">\"</span><span class="s">ÅF company is a future company</span><span class="se">\"</span><span class="s"> --&gt; [1,1,1,0,2,0,1]
</span></code></pre></div></div>

<p>Our new text is vectorized on top of the dictionary. You take the dictionary and replace the words position with the count of it that is found in the new text.</p>

<h4 id="finalizing-the-preprocessing">
<a class="anchor" href="#finalizing-the-preprocessing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Finalizing the preprocessing</h4>

<p>We can actually do some more things to improve the system which I won’t go into detail about (read the code). We remove stop-words and so on.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>


<span class="n">training_texts</span> <span class="o">=</span> <span class="p">[</span>
    \<span class="s">"ÅF is a big company</span><span class="se">\"</span><span class="s">, 
    </span><span class="se">\"</span><span class="s">ÅF making future</span><span class="se">\"</span><span class="s">
]

test_texts = [
    </span><span class="se">\"</span><span class="s">ÅF company is a future company</span><span class="se">\"</span><span class="s">
]

# this is the vectorizer
vectorizer = CountVectorizer(
    stop_words=</span><span class="se">\"</span><span class="s">english</span><span class="se">\"</span><span class="s">,    # Removes english stop words (such as 'a', 'is' and so on.)
    preprocessor=clean_text  # Customized preprocessor
)

# fit the vectorizer on the training text
vectorizer.fit(training_texts)

# get the vectorizer's vocabulary
inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()}
vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))]

# vectorization example
pd.DataFrame(
    data=vectorizer.transform(test_texts).toarray(),
    index=[</span><span class="se">\"</span><span class="s">Test sentence</span><span class="se">\"</span><span class="s">],
    columns=vocabulary
)
</span></code></pre></div></div>

<h3 id="lets-do-something-fun-out-of-this">
<a class="anchor" href="#lets-do-something-fun-out-of-this" aria-hidden="true"><span class="octicon octicon-link"></span></a>Let’s do something fun out of this!</h3>

<p>To begin with we need data. Luckily I know a perfect dataset for this – the IMDB movie reviews from stanford. This is a widely used dataset throughout <em>Sentiment Analysis</em>. The data contains 50 000 reviews where 50 % is positive and the rest negative. First we fetch a dataset. Download <a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">this file</a> and unpack it (into <code class="language-plaintext highlighter-rouge">aclImdb</code>) if the first code-snippet was unsuccessful.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">load_train_test_imdb_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">):</span>
    \<span class="s">"</span><span class="se">\"\"</span><span class="s">
    Loads the IMDB train/test datasets from a folder path.
    Input:
    data_dir: path to the </span><span class="se">\"</span><span class="s">aclImdb</span><span class="se">\"</span><span class="s"> folder.
    
    Returns:
    train/test datasets as pandas dataframes.
    </span><span class="se">\"\"\"</span><span class="s">

    data = {}
    for split in [</span><span class="se">\"</span><span class="s">train</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s">test</span><span class="se">\"</span><span class="s">]:
        data[split] = []
        for sentiment in [</span><span class="se">\"</span><span class="s">neg</span><span class="se">\"</span><span class="s">, </span><span class="se">\"</span><span class="s">pos</span><span class="se">\"</span><span class="s">]:
            score = 1 if sentiment == </span><span class="se">\"</span><span class="s">pos</span><span class="se">\"</span><span class="s"> else 0

            path = os.path.join(data_dir, split, sentiment)
            file_names = os.listdir(path)
            for f_name in file_names:
                with open(os.path.join(path, f_name), </span><span class="se">\"</span><span class="s">r</span><span class="se">\"</span><span class="s">) as f:
                    review = f.read()
                    data[split].append([review, score])
  
    # We shuffle the data to make sure we don't train on sorted data. This results in some bad training.
    np.random.shuffle(data[</span><span class="se">\"</span><span class="s">train</span><span class="se">\"</span><span class="s">])        
    data[</span><span class="se">\"</span><span class="s">train</span><span class="se">\"</span><span class="s">] = pd.DataFrame(data[</span><span class="se">\"</span><span class="s">train</span><span class="se">\"</span><span class="s">],
                                 columns=['text', 'sentiment'])

    np.random.shuffle(data[</span><span class="se">\"</span><span class="s">test</span><span class="se">\"</span><span class="s">])
    data[</span><span class="se">\"</span><span class="s">test</span><span class="se">\"</span><span class="s">] = pd.DataFrame(data[</span><span class="se">\"</span><span class="s">test</span><span class="se">\"</span><span class="s">],
                                columns=['text', 'sentiment'])

    return data[</span><span class="se">\"</span><span class="s">train</span><span class="se">\"</span><span class="s">], data[</span><span class="se">\"</span><span class="s">test</span><span class="se">\"</span><span class="s">]
train_data, test_data = load_train_test_imdb_data(
    data_dir=</span><span class="se">\"</span><span class="s">aclImdb/</span><span class="se">\"</span><span class="s">)
</span></code></pre></div></div>

<h3 id="lets-create-our-classifier">
<a class="anchor" href="#lets-create-our-classifier" aria-hidden="true"><span class="octicon octicon-link"></span></a>Let’s create our classifier</h3>

<p>We now have a dataset that we have successfully partitioned into a dictionary so that we can use it for our classifier.</p>

<p>Do you see an issue with our baseline right now?</p>

<p>…As mentioned we want to only have important features to simplify training. Right now we have an enormous amount of features, our BOW-approach result in an 80 000-dimensional vector. Because of this we <em>must</em> use simple algorithms that learn fast &amp; easy, e.g. <a href="https://en.wikipedia.org/wiki/Support-vector_machine">Linear SVM</a>, <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> or <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>.</p>

<p>Let’s create some code that actually let’s us train a Linear SVM!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>


<span class="c1"># Transform each text into a vector of word counts
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span>\<span class="s">"english</span><span class="se">\"</span><span class="s">,
                             preprocessor=clean_text)

training_features = vectorizer.fit_transform(train_data[</span><span class="se">\"</span><span class="s">text</span><span class="se">\"</span><span class="s">])    
test_features = vectorizer.transform(test_data[</span><span class="se">\"</span><span class="s">text</span><span class="se">\"</span><span class="s">])

# Training
model = LinearSVC()
model.fit(training_features, train_data[</span><span class="se">\"</span><span class="s">sentiment</span><span class="se">\"</span><span class="s">])
y_pred = model.predict(test_features)

# Evaluation
acc = accuracy_score(test_data[</span><span class="se">\"</span><span class="s">sentiment</span><span class="se">\"</span><span class="s">], y_pred)

print(</span><span class="se">\"</span><span class="s">Accuracy on the IMDB dataset: {:.2f}</span><span class="se">\"</span><span class="s">.format(acc*100))
</span></code></pre></div></div>

<h3 id="comparison-to-state-of-the-art">
<a class="anchor" href="#comparison-to-state-of-the-art" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparison to state-of-the-art</h3>

<p>Our accuracy is somewhere around 83.5-84 % which is really good! With this simple model and incredibly simplistic feature extraction we achieve a really high amount of correct answer! Comparing this to state-of-the-art we’re around 11 percent units beneat (~95% accuracy achieved <a href="https://arxiv.org/pdf/1801.06146.pdf">here</a>).</p>

<p>Incredible right? Exciting!? For me it is at least!</p>

<p>How do we improve from here?</p>

<h3 id="improving-the-model">
<a class="anchor" href="#improving-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Improving the model</h3>

<p>We have some huge improvements to make outside of fine-tuning, so we’ll skip the fine-tuning from now.</p>

<p>The first step is to improve our vectorization.</p>

<h4 id="tf-idf">
<a class="anchor" href="#tf-idf" aria-hidden="true"><span class="octicon octicon-link"></span></a>TF-IDF</h4>

<p>If you were at <em>first friday (@ÅF)</em> you have heard about TF-IDF earlier. TF-IDF stands for <em>Term Frequence-Inverse Document Frequency</em> and is a measurement that aims to fight imbalances in texts.</p>

<p>In our vectorization step we look at the word-count meaning that we’ll have some biases to how much a word is present, the longer the text the more the bias. To reduce this we can take the word-count divided by the total amount of words in the text (TF). We also want to downscale the words that are incredibly frequent such as stop words and topic-related words, and upscale unusual words somewhat, e.g.<em>glamorous</em> might not be frequent but it is important to the text most likely. We use <em>IDF</em> for this. We then take these two and combine.</p>

<p><img src="https://cdn-images-1.medium.com/max/800/1*FgQgJYozG7colT9rys066w.png" alt="alt text"></p>

<h3 id="implementation-details">
<a class="anchor" href="#implementation-details" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation details</h3>

<p>This is actually really easy to do as <em>sklearn</em> already has a finished <code class="language-plaintext highlighter-rouge">TfIdfVectorizer</code> so all we have to do is to replace the <code class="language-plaintext highlighter-rouge">CountVectorizer</code>. Let’s see how it goes!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>


<span class="c1"># Transform each text into a vector of word counts
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span>\<span class="s">"english</span><span class="se">\"</span><span class="s">,
                             preprocessor=clean_text)

training_features = vectorizer.fit_transform(train_data[</span><span class="se">\"</span><span class="s">text</span><span class="se">\"</span><span class="s">])    
test_features = vectorizer.transform(test_data[</span><span class="se">\"</span><span class="s">text</span><span class="se">\"</span><span class="s">])

# Training
model = LinearSVC()
model.fit(training_features, train_data[</span><span class="se">\"</span><span class="s">sentiment</span><span class="se">\"</span><span class="s">])
y_pred = model.predict(test_features)

# Evaluation
acc = accuracy_score(test_data[</span><span class="se">\"</span><span class="s">sentiment</span><span class="se">\"</span><span class="s">], y_pred)

print(</span><span class="se">\"</span><span class="s">Accuracy on the IMDB dataset: {:.2f}</span><span class="se">\"</span><span class="s">.format(acc*100))

# Extra: Implement our own TfIdfVectorizer.
</span></code></pre></div></div>

<h3 id="conclusion-of-tf-idf">
<a class="anchor" href="#conclusion-of-tf-idf" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion of TF-IDF</h3>

<p>The <code class="language-plaintext highlighter-rouge">TfIdVectorizer</code> improved our scoring with 2 percent units, that’s incredible for such an easy improvement!</p>

<p>This for me shows how important it is to understand the data and what is important. You really need to grasp how to extract the important and what tools are available.</p>

<p>But let’s not stop here, lets reiterate and improve further.</p>

<p>What is the next natural step? Context I believe. During my master-thesis on spell correction of Street Names it was very obvious how important context is to increase the models understanding. Unfortunately we couldn’t use the context of a sentence in the thesis (as of the nature of street names) but here we can!</p>

<h3 id="use-of-context">
<a class="anchor" href="#use-of-context" aria-hidden="true"><span class="octicon octicon-link"></span></a>Use of context</h3>

<p>Words by themself prove some meaning but sometimes they’re used in a negated sense, e.g. <em>not good</em>. <em>Good</em> in itself would most likely be positive but if we can get the context around the word we can be more sure about in what manner it is applied.</p>

<p>We call this <em>N-grams</em> where N is equal to the amount of words taken into consideration for each word. Using bigrams (N=2) we get the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">companies</span> <span class="n">often</span> <span class="n">use</span> <span class="n">corporate</span> <span class="n">bs</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">companies</span><span class="p">,</span> <span class="n">often</span><span class="p">,</span> <span class="n">use</span><span class="p">,</span> <span class="n">slogans</span><span class="p">,</span> <span class="p">(</span><span class="n">companies</span><span class="p">,</span> <span class="n">often</span><span class="p">),</span> <span class="p">(</span><span class="n">often</span><span class="p">,</span><span class="n">use</span><span class="p">),</span> <span class="p">(</span><span class="n">use</span><span class="p">,</span><span class="n">slogans</span><span class="p">)]</span>
</code></pre></div></div>

<p>Sometimes you include a start &amp; ending word so that it would be <code class="language-plaintext highlighter-rouge">(\\t, companies)</code> and <code class="language-plaintext highlighter-rouge">(slogans, \\r)</code> or such. In this case as we are not finetuning we won’t go into that. We’ll keep it simple.</p>

<p>The all-mighty sklearn <code class="language-plaintext highlighter-rouge">TfIdfVectorizer</code> actually already have included N-gram support using the parameter <code class="language-plaintext highlighter-rouge">ngram_range=(1, N)</code>. So let’s make it simple for us and make use of that!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>


<span class="c1"># Transform each text into a vector of word counts
</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
                            <span class="n">strip_accents</span><span class="o">=</span><span class="s">'ascii'</span><span class="p">,</span>
                            <span class="n">max_df</span><span class="o">=</span><span class="mf">0.98</span><span class="p">)</span>

<span class="n">training_features</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">[</span>\<span class="s">"text</span><span class="se">\"</span><span class="s">])    
test_features = vectorizer.transform(test_data[</span><span class="se">\"</span><span class="s">text</span><span class="se">\"</span><span class="s">])

# Training
model = LinearSVC()
model.fit(training_features, train_data[</span><span class="se">\"</span><span class="s">sentiment</span><span class="se">\"</span><span class="s">])
y_pred = model.predict(test_features)

# Evaluation
acc = accuracy_score(test_data[</span><span class="se">\"</span><span class="s">sentiment</span><span class="se">\"</span><span class="s">], y_pred)

print(</span><span class="se">\"</span><span class="s">Accuracy on the IMDB dataset: {:.2f}</span><span class="se">\"</span><span class="s">.format(acc*100))
</span></code></pre></div></div>

<h3 id="conclusion-of-n-gram">
<a class="anchor" href="#conclusion-of-n-gram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion of N-gram</h3>

<p>Once again we see a massive improvement. We’re almost touching 89 % now! That’s just a mere 6 percent units below state-of-the-art. What can we do to improve now?</p>

<p>Some possible improvements for you to try!</p>

<ul>
  <li>Use a custom threshold to reduce the dimensions</li>
  <li>Play around with the <code class="language-plaintext highlighter-rouge">ngram_range</code> (don’t forget a threshold if you do this)</li>
  <li>Improve the preprocessing</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Try some fun things here if you want too :)
</span></code></pre></div></div>

<h2 id="conclusion-of-phase-1">
<a class="anchor" href="#conclusion-of-phase-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion of phase 1</h2>

<p>We have created a strong baseline for text classification with great accuracy for its simplicity. The following steps has been done</p>

<ul>
  <li>First a simple preprocessing step which is of great importance. We have to remember to not make it to complex, the complexity of preprocessing is like an evil circle in the end. In our case we remove punctuations, stopwords and lower the case.</li>
  <li>Secondly we vectorize the data to make it readable by the system. A classifier requires numerical features. For this we had a <code class="language-plaintext highlighter-rouge">TfIdfVectorizer</code> that computes frequency of words while downsampling words that are to common &amp; upsampling unusual words.</li>
  <li>Finally we added N-gram to the model to increase the understanding of the sentence by supplying context.</li>
</ul>

<h2 id="phase-2">
<a class="anchor" href="#phase-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Phase 2</h2>

<p>How do we improve from here? TF-IDF has its cons and pros. Some of the cons are that they:</p>

<ul>
  <li>Don’t account for any kind of positioning at all</li>
  <li>The dimensions are ridiculous large</li>
  <li>They can’t capture semantics.</li>
</ul>

<p>Improvements upon this is made by using neural networks and word embeddings.</p>

<h2 id="word-embeddings">
<a class="anchor" href="#word-embeddings" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word Embeddings</h2>

<p>Word Embeddings &amp; Neural Networks are where we left off. By change our model to instead utilize these two concepts we can improve the accuracy once again.</p>

<h3 id="word-embeddings-1">
<a class="anchor" href="#word-embeddings-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Word Embeddings</h3>

<p>Word Embeddings (WE) are actually a type of Neural Network. It uses <em>embedding</em> to create the model. I quickly explained WE during my presentation on Summarization and how to build a great summarizer. Today we’ll go a little more into depth.</p>

<p>To begin with I’ll take the most common example, WE lets us do the following arithmetiric with words:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">King</span> <span class="o">-</span> <span class="n">Man</span> <span class="o">+</span> <span class="n">Woman</span> <span class="o">=</span> <span class="n">Queen</span>
</code></pre></div></div>

<p>This is, in my opinion, completely amazing and fascinating. How does this work? Where do I learn more? Those are my first thoughts. In fact the theory is pretty basic until you get to the nittygritty details, as with most things.</p>

<p>WE is built on the concept ot learn how words are related to eachother. What company do a word have? To make the example more complex we can redefine this too the following: <code class="language-plaintext highlighter-rouge">A is to B what C is to D</code>.</p>

<p>Currently there is three "big" models that are widely used. The first one Word2Vec (<a href="https://arxiv.org/abs/1301.3781">Mikolov et al 2013</a>), the second is GloVe (MIT <a href="https://nlp.stanford.edu/projects/glove/">MIT</a>, <a href="https://nlp.stanford.edu/pubs/glove.pdf">Pennington et al 2014</a>) and the final one is fastText (<a href="https://github.com/facebookresearch/fastText">facebook</a>).</p>

<p>We will look into how you can achieve this without Deep Learning / Neural Networks unlike the models mentioned.</p>

<h4 id="step-1-how-to-represent-words-in-a-numerical-vector">
<a class="anchor" href="#step-1-how-to-represent-words-in-a-numerical-vector" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 1: How to represent words in a numerical vector</h4>

<p>The first thing we have to do to actually understand/achieve word embeddings is to represent words in a numerical vector. In relation to this a quick explanation of sparse &amp; dense representations would be great. Read more in detail at <a href="https://en.wikipedia.org/wiki/Sparse_matrix">Wikipedia: Sparse Matrix</a></p>

<p><strong>Sparse representation</strong> is when we represent something very sparsely. It tells us that the points in the space is very few in regards to the dimensions and that most elements are empty. Think one-hot-encoding.</p>

<p>A <strong>Dense representation</strong> in comparison has few dimensions in comparison to possible values and most elements are filled. Think of something continuous.</p>

<p>The most simple way to represent words in a numerical vector is something we touched earlier, by one-hot-encoding them, i.e. a sparse representation.</p>

<p><img src="https://cdn-images-1.medium.com/max/1200/1*YEJf9BQQh0ma1ECs6x_7yQ.png" alt="Source :(Marco Bonzanini, 2017)">(Source: Marco Bonzanini, 2017)</p>

<p>Because of how languages are structured having one-hot-encoding means that we will have an incredibly sparse matrix (can be good) but it will have an enormous amount of dimensions (bad).</p>

<p>On top of this how would we go ahead and measure the distance between words? Normally one would use the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> but if we have a one-hot-encoding all the words would be orthogonal against eachother meaning that the dot-product will be zero.</p>

<p>Creating a dense representation however would indeed capture similarity as we could make use of cosine-similarity and more. Introducing Word2Vec.</p>

<h4 id="step-2-word2vec-representing-data-densely">
<a class="anchor" href="#step-2-word2vec-representing-data-densely" aria-hidden="true"><span class="octicon octicon-link"></span></a>Step 2: Word2Vec, representing data densely</h4>

<p>The goal of Word2Vec, at least to my understanding, is to actually predict the context of a word. Or in other words we learn embeddings by prediciting the context of the word. The <em>context</em> here being the same definition as in N-grams. Word2Vec uses <em>shallow neural network</em> to learn word vectors so that each word is good at predicting its own contexts (more about his in <strong>Skip-Grams</strong>) and how to predict a word given a context (more about this in <strong>CBOW</strong>).</p>

<h4 id="skip-gram">
<a class="anchor" href="#skip-gram" aria-hidden="true"><span class="octicon octicon-link"></span></a>Skip-gram</h4>

<p>Skip-gram very simplified is when you train on the N-grams but without the real word. <img src="https://cdn-images-1.medium.com/max/800/1*swlaqv7p_3xI4eL37C1pAA.png" alt="alt text"></p>

<p>As of now we have empirical results showing how this technique is very successful at learning the meaning of the words. On top of this the embedding that we get has both <em>direction of semantic and syntatic meaning</em> that are exposed in example such as <code class="language-plaintext highlighter-rouge">King - Man...</code>.</p>

<p>Another example would be: <code class="language-plaintext highlighter-rouge">Vector(Madrid) - Vector(Spain) + Vector(Sweden) ~ Vector(Stockholm)</code></p>

<h4 id="so-how-do-the-arithmetic-of-words-actually-work">
<a class="anchor" href="#so-how-do-the-arithmetic-of-words-actually-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>So how do the arithmetic of words actually work?</h4>

<p>I won’t go into details (some complicated math, see <a href="http://www.aclweb.org/anthology/P17-1007">Gittens et al</a>) but if we assume the following to be true:</p>

<ul>
  <li>All words are distributed uniformly</li>
  <li>The embedding model is linear</li>
  <li>The conditional distributions of words are indepedent</li>
</ul>

<p>Then we can prove that the embedding of the paraphrase of a set of words is obtained by taking the sum over the embeddings of all of the individual words.</p>

<p>Using this result it’s easy to show how the famous man-woman, king-queen relationship works.</p>

<p>Extra note: You can show this then by havingn <code class="language-plaintext highlighter-rouge">King</code> and <code class="language-plaintext highlighter-rouge">Queen</code> having the same <code class="language-plaintext highlighter-rouge">Male-Female</code>relationship as the <code class="language-plaintext highlighter-rouge">King</code> then is the paraphrase of the set of words <code class="language-plaintext highlighter-rouge">{Queen, X}</code></p>

<p>I want to note that these assumptions are not 100 percent accurate. In reality word distributions are thought to follow Zipf’s law.</p>

<h4 id="glove">
<a class="anchor" href="#glove" aria-hidden="true"><span class="octicon octicon-link"></span></a>GloVe</h4>

<p>A year after Word2Vec was a fact to the world the scientist decided to reiterate again. This time we got GloVe. GloVe tried to improve upon Word2Vec by that given a word its relationship(s) can be recovered from co-occurence statistics of a large corpus. GloVe is expensive and memory hungry, but it’s only one load so the issue isn’t that big. Nitty bitty details</p>

<h4 id="fasttext">
<a class="anchor" href="#fasttext" aria-hidden="true"><span class="octicon octicon-link"></span></a>fastText</h4>

<p>With fastText one of the biggest problems is solved, both GloVe and Word2Vec only learn embeddings of word of the vocabulary. Because of this we can’t find an embedding for a word that isn’t in the dictionary.</p>

<p>Bojanowski et al solved this by learning the word embeddings using subword information. To summarize fastText learns embeddings of character n-grams instead.</p>

<h4 id="the-simple-way">
<a class="anchor" href="#the-simple-way" aria-hidden="true"><span class="octicon octicon-link"></span></a>The simple way</h4>

<p>A simple approach to create your own word embeddings without a neural network is by factorizing a co-occurence matrix using SVD (singular-value-decomposition). As mentioned Word2Vec is barely a neural network as it has no hidden layers nor an y non-linearities. GloVe factorizes a co-occurense matrix while gaining even better results.</p>

<p>I highly recommend you to go check this blog out: https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ by Stitch Fix. An awesome read and we can go implement this too!</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="londogard/londogard"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/machine-learning/nlp/workshop/2020/02/23/competence-meeting-imdb-text-classification.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog with a focus on Deep Learning, JVM and Performance.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/londogard" title="londogard"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
