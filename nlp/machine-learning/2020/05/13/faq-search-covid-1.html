<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>A simple FAQ search engine in Swedish using fastText &amp; Smooth Inverse Frequency | Londogard</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="A simple FAQ search engine in Swedish using fastText &amp; Smooth Inverse Frequency" />
<meta name="author" content="Hampus Londögård" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings &amp; Smooth Inverse Frequency to embed sentences. Basically scratched an itch I’ve had for a while" />
<meta property="og:description" content="A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings &amp; Smooth Inverse Frequency to embed sentences. Basically scratched an itch I’ve had for a while" />
<link rel="canonical" href="https://blog.londogard.com/nlp/machine-learning/2020/05/13/faq-search-covid-1.html" />
<meta property="og:url" content="https://blog.londogard.com/nlp/machine-learning/2020/05/13/faq-search-covid-1.html" />
<meta property="og:site_name" content="Londogard" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-13T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://blog.londogard.com/nlp/machine-learning/2020/05/13/faq-search-covid-1.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.londogard.com/nlp/machine-learning/2020/05/13/faq-search-covid-1.html"},"author":{"@type":"Person","name":"Hampus Londögård"},"headline":"A simple FAQ search engine in Swedish using fastText &amp; Smooth Inverse Frequency","dateModified":"2020-05-13T00:00:00-05:00","datePublished":"2020-05-13T00:00:00-05:00","description":"A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings &amp; Smooth Inverse Frequency to embed sentences. Basically scratched an itch I’ve had for a while","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.londogard.com/feed.xml" title="Londogard" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header" role="banner">

    <div class="wrapper"><a class="site-title" rel="author" href="/">Londogard</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger"><a class="page-link" href="/education/">Educational</a><a class="page-link" href="/londogard">Londogard↗</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
        </nav></div>
  </header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">A simple FAQ search engine in Swedish using fastText &amp; Smooth Inverse Frequency</h1><p class="page-description">A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings & Smooth Inverse Frequency to embed sentences. Basically scratched an itch I've had for a while</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-13T00:00:00-05:00" itemprop="datePublished">
        May 13, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Hampus Londögård</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#machine-learning">machine-learning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#covid-19-swedish-qa">CoViD-19 Swedish QA</a>
<ul>
<li class="toc-entry toc-h3"><a href="#introduction-to-the-problem-and-requirements">Introduction to the problem and requirements</a></li>
<li class="toc-entry toc-h3"><a href="#tools-chosen">Tools Chosen</a></li>
<li class="toc-entry toc-h3"><a href="#final-result">Final Result</a></li>
<li class="toc-entry toc-h3"><a href="#further-improvements-for-iteration-2-3-and-beyond">Further improvements for iteration 2, 3 and beyond!</a></li>
<li class="toc-entry toc-h3"><a href="#ending-words">Ending words</a></li>
</ul>
</li>
</ul><h1 id="covid-19-swedish-qa">
<a class="anchor" href="#covid-19-swedish-qa" aria-hidden="true"><span class="octicon octicon-link"></span></a>CoViD-19 Swedish QA</h1>
<p>I decided to scratch a small itch I’ve had for a while now - creating a search engine using an unsupervised approach. The final product, or the first iteration rather, ended up pretty good and I wanted to share what I’ve done so far.</p>

<h3 id="introduction-to-the-problem-and-requirements">
<a class="anchor" href="#introduction-to-the-problem-and-requirements" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction to the problem and requirements</h3>
<p>An unsupervised approach where we never edit the data nor supply any manually annotated data? Every Data Scientist dream I suppose. There’s a reason as of why supervised approaches generally result in better performance but there is some light at the end of the tunnel for unsupervised approaches too.</p>

<p>Let’s begin with my own requirements, which are mainly created to only keep the fun problem-solving left.</p>

<ul>
  <li>The end-product must be unsupervised
    <ul>
      <li>No manually annotated data</li>
      <li>No heuristic applied (at least in first iteration)</li>
    </ul>
  </li>
  <li>It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end)</li>
  <li>Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!)</li>
</ul>

<p>With this in mind I set out to build my own FAQ search engine.</p>

<p><strong>What is required to answer questions using a FAQ?</strong> We need to find the most relevant Q/A to the question posed.</p>

<p><strong>How do we do this?</strong> There is numerous types of ways to do this unsupervised. I’ll account for a few here:</p>

<ol>
  <li>Latent Dirichlet Allocation (LDA) which is a way to find topics through clever statistical analysis (basically soft clusters of documents)</li>
  <li>Embedding and <a href="https://en.wikipedia.org/wiki/Cosine_similarity">Cosine Similarity</a>, find the distance between the two arrays of numbers in the embedded space. One can also apply Euclidean Distance which isn’t especially good because of <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a>. Other possible approaches includes <a href="https://arxiv.org/pdf/1912.00509.pdf">Word Mover Distance</a>.</li>
  <li>Simple word counting and Bag of Words</li>
</ol>

<h3 id="tools-chosen">
<a class="anchor" href="#tools-chosen" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tools Chosen</h3>
<p>After a little research I found a few tools which fit my need.</p>

<p><strong>fastText</strong></p>

<p>fastText that came out of Facebook AI Research (FAIR) and <a href="https://arxiv.org/abs/1607.04606">this</a> paper. It’s a type of Word Embeddings where also subwords are embedded through ngrams of characters, this means that we are able to embedd words that are out of vocabulary, which can be the reason because of either misspelling or just a missing word.
On their <a href="https://fasttext.cc/">homepage</a> they have a plethora of models including a Swedish one that has been derived from Wikipedia, pretty awesome!</p>

<p><strong>Smooth Inverse Frequency</strong></p>

<p>Smooth Inverse Frequency (SIF) is an algorithm to embed sentences which was proposed in <a href="https://openreview.net/pdf?id=SyK00v5xx">"A Simple but Tough-To-Beat Baseline for Sentence Embeddings"</a> in 2017. In its essence they propose to embed the sentence using a weighted average and thereafter modify them a bit using PCA/SVD.</p>

<p><strong>Folkhälsomyndigheten FAQ</strong></p>

<p>Finally I need the FAQ to use, in my case it’s Covid-19 FAQ from Folkhälsomyndigheten. It was parsed into pandas dataframes using requests &amp; BeautifulSoup4 (bs4).</p>

<h3 id="final-result">
<a class="anchor" href="#final-result" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final Result</h3>
<p>So after all this was figured out I sat down an afternoon and cooked some code together, the result ended up more impressive than I had imagined. The questions posed are being responded with pretty good results. I’m especially impressed by question about <em>astma</em>, <em>son</em> and <em>regler</em>. Here’s a few of them:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Hur sjuk blir jag?

Hur sjuk blir man av covid-19? - 0.98
Hur länge är man sjuk av covid-19? - 0.97
Hur lång är inkubationstiden? - 0.81
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Hur vet jag om det är astma?

Hur vet jag om mina symtom beror på pollenallergi eller på covid-19? - 0.63
Hur sjuk blir man av covid-19? - 0.53
Hur länge är man sjuk av covid-19? - 0.53
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Hur förklarar jag corona för min son?

Hur pratar man med barn om det nya coronaviruset? - 0.58
Hur lång är inkubationstiden? - 0.53
Hur sjuk blir man av covid-19? - 0.49
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Hur minskar vi spridningen i sverige?

Hur gör ni för att mäta förekomsten av covid-19 i samhället? - 0.65
Hur övervakar ni på Folkhälsomyndigheten spridningen av covid-19? - 0.57
Hur stor är dödligheten till följd av covid-19? - 0.56
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; Vad för regler finns?

Vad gäller för olika verksamheter? - 0.76
Vad gäller för handeln? - 0.75
Vad är covid-19? - 0.71
</code></pre></div></div>

<p>One can directly note the correlation of the beginning. It seems like the first word has a high correlation with the most similar question. Weird. Removing stop words could probably improve this, but that’d be for the second implementation.</p>

<h3 id="further-improvements-for-iteration-2-3-and-beyond">
<a class="anchor" href="#further-improvements-for-iteration-2-3-and-beyond" aria-hidden="true"><span class="octicon octicon-link"></span></a>Further improvements for iteration 2, 3 and beyond!</h3>
<p><strong>Pre-processing</strong></p>

<p>As mentioned right above we can apply some basic pre-processing  such as removing stop words. In reality this should be handled by SIF but looking at our similarity scores there’s a 1-1 relation between the first word of the sentence.</p>

<p>Other improvements worth trying out is lemmatizing or stemming the words ("cutting them to the root" in simple terms) and further using a better tokenization is worth trying out (currently splitting on whitespace). <em>spaCy</em> offers a strong tokenizer, but I haven’t tried it out for Swedish yet. Once again <em>fastText</em> should handle this but it’s worth trying out if it improves or keep the result at the same level.</p>

<p><strong>Different Embedding Techniques</strong></p>

<p>There exist a certain Sentence Embedding that’s basically made for this task - MULE (Multimodal Universal Language Embeddings). MULE is even multilingual but unfortunately they’re not able to embed Swedish so we’d require a translation from Swedish to one of the 16 languages supported by MULE. This means that it is out of the question because of my requirements, but could still be fun to check out.</p>

<p>Other embeddings such as FLAIR (by Zalando), BERT (using BERT-as-a-service) or even training my own embeddings (perhaps using StarSpace) could prove interesting also.</p>

<p><strong>Completely other technique</strong></p>

<p>I mentioned first of all LDA, and I think LDA could be interesting. Most often LDA is applied to larger documents but as with everything it is never wrong to try out and verify the results.</p>

<p>Supervised approaches would certainly be able to show us some good performance but that requires annotating data in one way or another which is a boring task - but very important. Perhaps I’ll revisit and label some data, with todays Transfer Learning we can achieve higher accuracy with less data using other pre-trained  Language Models such as BERT or Multifit (from Ulmfit).</p>

<h3 id="ending-words">
<a class="anchor" href="#ending-words" aria-hidden="true"><span class="octicon octicon-link"></span></a>Ending words</h3>
<p>This was a really fun task and I’m happy that I tried it out. I’m sure I’ll revisit and improve it further by applying some of the possible improvements. Further I think I might actually try to do this for all FAQs available by our authorities to create a "Multi FAQ" which could prove pretty cool. With more data the results should also be better.</p>

<p>And as an ending note my model ended up using 2.5-3 GB of memory during run-time which means it’s possible to run on my Raspberry Pi 4! Further reduction of size can be done by removing the most uncommon words in the vocabulary (vocab is 2M words, which is very large). I applied a dimension reduction using the built in version of <em>fastText</em> (ending up using d=100 and still achieving good search results).</p>

<p>The implementation is available at my <a href="https://github.com/londogard/nlp-projects/blob/master/python/CoViD_19_QA.ipynb">GitHub (Londogard)</a> or directly launched in <a href="https://colab.research.google.com/github/londogard/nlp-projects/blob/master/python/CoViD_19_QA.ipynb">Google Colaboratory</a>.</p>

<p>Thanks for this time, I’ll be back with more!<br>
 Hampus Londögård</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="londogard/londogard"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/nlp/machine-learning/2020/05/13/faq-search-covid-1.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog with a focus on Deep Learning, JVM and Performance.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/londogard" target="_blank" title="londogard"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
