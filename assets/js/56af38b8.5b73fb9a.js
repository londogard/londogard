"use strict";(self.webpackChunklondogard=self.webpackChunklondogard||[]).push([[7805],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return c}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),u=p(n),c=r,h=u["".concat(l,".").concat(c)]||u[c]||m[c]||o;return n?a.createElement(h,s(s({ref:t},d),{},{components:n})):a.createElement(h,s({ref:t},d))}));function c(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,s=new Array(o);s[0]=u;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,s[1]=i;for(var p=2;p<o;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},7098:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return l},metadata:function(){return p},assets:function(){return d},toc:function(){return m},default:function(){return c}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),s=["components"],i={title:"CoViD-19 FAQ Search Engine 2.0",description:"In this post I improve the previous FAQ search engine by some low hanging fruits. The requirements stay the same thus SotA is not achieved but rather it's simply generic & easy on hardware (Raspberry Pi capable).",tags:["nlp","deep-learning","machine-learning","workshop"],authors:"hlondogard"},l=void 0,p={permalink:"/blog/2020/08/01/faq-search-covid-2",editUrl:"https://github.com/londogard/londogard/blog/2020-08-01-faq-search-covid-2/index.md",source:"@site/blog/2020-08-01-faq-search-covid-2/index.md",title:"CoViD-19 FAQ Search Engine 2.0",description:"In this post I improve the previous FAQ search engine by some low hanging fruits. The requirements stay the same thus SotA is not achieved but rather it's simply generic & easy on hardware (Raspberry Pi capable).",date:"2020-08-01T00:00:00.000Z",formattedDate:"August 1, 2020",tags:[{label:"nlp",permalink:"/blog/tags/nlp"},{label:"deep-learning",permalink:"/blog/tags/deep-learning"},{label:"machine-learning",permalink:"/blog/tags/machine-learning"},{label:"workshop",permalink:"/blog/tags/workshop"}],readingTime:14.57,truncated:!0,authors:[{name:"Hampus Lond\xf6g\xe5rd",title:"Main Contributor of Londogard",url:"https://github.com/lundez",imageURL:"https://github.com/lundez.png",key:"hlondogard"}],frontMatter:{title:"CoViD-19 FAQ Search Engine 2.0",description:"In this post I improve the previous FAQ search engine by some low hanging fruits. The requirements stay the same thus SotA is not achieved but rather it's simply generic & easy on hardware (Raspberry Pi capable).",tags:["nlp","deep-learning","machine-learning","workshop"],authors:"hlondogard"},prevItem:{title:"TIL: fastutil - fast & compact type-speciic collections for JVM (no autobox!)",permalink:"/blog/2020/09/03/til-fastutil-primitive-structures"},nextItem:{title:"SQL - Different Abstraction Levels (& how I came to love SQLDelight)",permalink:"/blog/2020/06/01/sqldelight-kotlin"}},d={authorsImageUrls:[void 0]},m=[{value:"Improvements to be done",id:"improvements-to-be-done",children:[],level:2},{value:"Re-adding the old code",id:"re-adding-the-old-code",children:[{value:"Loading all the models",id:"loading-all-the-models",children:[],level:4}],level:2},{value:"Going forward",id:"going-forward",children:[{value:"1. Tokenization &amp; lower-case",id:"1-tokenization--lower-case",children:[{value:"Lower-casing",id:"lower-casing",children:[],level:4}],level:3},{value:"Testing the new input-data",id:"testing-the-new-input-data",children:[],level:3},{value:"2. Lemmatization and Stop Words",id:"2-lemmatization-and-stop-words",children:[{value:"Lemmatization",id:"lemmatization",children:[],level:4},{value:"What is stop-words?",id:"what-is-stop-words",children:[],level:4},{value:"Analyzing the results",id:"analyzing-the-results",children:[],level:4},{value:"Stop words",id:"stop-words",children:[],level:4},{value:"Further analyzing",id:"further-analyzing",children:[{value:"Custom Stop Words (breaking the rules)",id:"custom-stop-words-breaking-the-rules",children:[],level:5}],level:4}],level:3}],level:2}],u={toc:m};function c(e){var t=e.components,i=(0,r.Z)(e,s);return(0,o.kt)("wrapper",(0,a.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"In this post I improve the previous FAQ search engine by some low hanging fruits. The requirements stay the same thus SotA is not achieved but rather it's simply generic & easy on hardware (Raspberry Pi capable)."),(0,o.kt)("h1",{id:"covid-19-faq-search-engine-20"},"CoViD-19 FAQ Search Engine 2.0"),(0,o.kt)("p",null,"(Open in Google Colab ",(0,o.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/londogard/nlp-projects/blob/master/python/CoViD_19_QA_cont.ipynb"},"here")," to run the code)"),(0,o.kt)("p",null,"As promised here's a new improved (or is it?) ",(0,o.kt)("em",{parentName:"p"},"FAQ Search Engine")," with some minor NLP-lessons added as we go, be ready to learn new (or old) things!",(0,o.kt)("br",{parentName:"p"}),"\n","Previously I added some requirements and I wish keep them, here they are as a refresher:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The end-product must be unsupervised",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"No manually annotated data"),(0,o.kt)("li",{parentName:"ul"},"No heuristic applied (i.e. understand the data and improve result by applying domain-specific knowledge on the task)"))),(0,o.kt)("li",{parentName:"ul"},"It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end)"),(0,o.kt)("li",{parentName:"ul"},"Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!)")),(0,o.kt)("p",null,"These specifications adds a bit of spice, keep manual labour to a minimum at the same time as they prove a challenge that doesn't aim to achieve State of the Art but rather to be applicable and light!  "),(0,o.kt)("p",null,"With that in mind, let's move onwards!"),(0,o.kt)("h2",{id:"improvements-to-be-done"},"Improvements to be done"),(0,o.kt)("p",null,"In the previous ",(0,o.kt)("a",{parentName:"p",href:"https://londogard.com/blog/4"},"blog")," & ",(0,o.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/londogard/nlp-projects/blob/master/python/CoViD_19_QA.ipynb"},"notebook")," I first implemented a basic FAQ search based on finding the nearest neighbour from the embedded sentences, in the end I used ",(0,o.kt)("em",{parentName:"p"},"Smooth Inverse Frequency Embeddings")," (",(0,o.kt)("a",{parentName:"p",href:"https://openreview.net/forum?id=SyK00v5xx"},"A Simple but Tough-to-Beat Baseline for Sentence Embeddings"),") to embed the sentence which is an improvement from simply averaging the embeddings of the words in the sentence.  "),(0,o.kt)("p",null,'In the end I discussed some potential improvements which I wished to investigate. In this notebook I\'ll deliver these "improvements" based on grabbing some low hanging fruit. The total "improvements" to try out:'),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Lowercase"),(0,o.kt)("li",{parentName:"ul"},"Better tokenization"),(0,o.kt)("li",{parentName:"ul"},"Lemmatizing"),(0,o.kt)("li",{parentName:"ul"},"Stop words"),(0,o.kt)("li",{parentName:"ul"},"Ngram & Custom Embeddings (will not be done because of time)")),(0,o.kt)("p",null,"To improve further I'd say that either A) ",(0,o.kt)("em",{parentName:"p"},"a lot")," of time to understand the data in depth and apply heuristics or B) a ",(0,o.kt)("em",{parentName:"p"},"supervised")," approach, which in turn require labeled data (a.k.a sweet valued time). A larger dataset would also be helpful.",(0,o.kt)("br",{parentName:"p"}),"\n","All which I don't have currently."),(0,o.kt)("h2",{id:"re-adding-the-old-code"},"Re-adding the old code"),(0,o.kt)("p",null,"First I'll add the code from \"part one\" and it'll not be commented as it has been walked through.",(0,o.kt)("br",{parentName:"p"}),"\n","Further I've removed the download & parsing of FAQ, now the data is directly downloaded as a ",(0,o.kt)("inlineCode",{parentName:"p"},"tsv"),"-file allowing us to skip some libraries / code-cells.",(0,o.kt)("br",{parentName:"p"}),"\n","Some new dependencies are also added, e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"stanza")," which is Stanfords new NLP-lib in Python (inspired by ",(0,o.kt)("inlineCode",{parentName:"p"},"spaCy"),")."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"%%capture\n!pip install -U gensim\n!pip install -U fse\n!pip install stanza\n!pip install stop-words\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\ntqdm.pandas()\n\nfrom pathlib import Path\n\nimport os\nimport random \nimport operator \nimport regex as re\n\n# gensim + fasttext\nfrom gensim.models.fasttext import FastText, load_facebook_vectors\nfrom gensim.models import KeyedVectors\n\nfrom stop_words import get_stop_words # stop-words from basically all languages\n\nimport stanza\n\nfrom fse import IndexedList\nfrom fse.models import uSIF\nfrom fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH\nprint(MAX_WORDS_IN_BATCH)\nprint(FAST_VERSION)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"10000\n1\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Download models etc\nstanza.download('sv', logging_level='ERROR')\nprint(\"OBS!!\\nPlease download the Swe fastText model & the CoViD FAQ data from links in this code cell!\")\n# Swe fastText reduced dimensions --\x3e   https://drive.google.com/open?id=1vaWtiSlRAZ3XCdtnSce_6dwQ0T5x0OEJ\n# CoViD FAQ data --\x3e                    https://github.com/londogard/nlp-projects/blob/master/datasets/covid.tsv\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"OBS!!\nPlease download the Swe fastText model & the CoViD FAQ data from links in this code cell!\n")),(0,o.kt)("h4",{id:"loading-all-the-models"},"Loading all the models"),(0,o.kt)("p",null,"This might take a little while, even though the dimensions are reduced the model is pretty large."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"ft_wv = load_facebook_vectors('~/git/nlp-projects/models/cc.sv.100.bin')\ndf = pd.read_csv('~/git/nlp-projects/datasets/covid.tsv', sep='\\t')\nnlp = stanza.Pipeline(lang='sv', processors='tokenize', logging_level='ERROR')\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\n\nflatten = lambda l: [item for sublist in l for item in sublist] # Helper function to flatten a list\n")),(0,o.kt)("h2",{id:"going-forward"},"Going forward"),(0,o.kt)("p",null,"Let's get on to adding our improvements"),(0,o.kt)("h3",{id:"1-tokenization--lower-case"},"1. Tokenization & lower-case"),(0,o.kt)("p",null,"The first and forthmost improvement is to lowercase the text and then tokenize it using a better method of tokenization.",(0,o.kt)("br",{parentName:"p"}),"\n","Let's take a look at how ",(0,o.kt)("em",{parentName:"p"},"stanza")," helps us out by applying a much better tokenization."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'q = "Hej d\xe4r borta! Jag k\xe4nner igen dig, Johan\'s kompis? Eller \xe4r det Johannas?"\nstanza_tokenize = lambda x: [token.text for sentence in nlp(x).sentences for token in sentence.tokens]\nprev = q.split()\nnew = stanza_tokenize(q)\n\nprint(f"Previously:\\t{prev[:12]}..")\nprint(f"After:\\t\\t{new[:12]}..")\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Previously: ['Hej', 'd\xe4r', 'borta!', 'Jag', 'k\xe4nner', 'igen', 'dig,', \"Johan's\", 'kompis?', 'Eller', '\xe4r', 'det']..\nAfter:      ['Hej', 'd\xe4r', 'borta', '!', 'Jag', 'k\xe4nner', 'igen', 'dig', ',', 'Johan', \"'\", 's']..\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"So, what are we looking at?"),(0,o.kt)("br",{parentName:"p"}),"\n","Stanza handled our tokenization and increased the number of tokens, can this really be good!?",(0,o.kt)("br",{parentName:"p"}),"\n","Yes! Keep calm and don't jump the ship yet, the increased number of tokens will be followed by a decrease of unique tokens, and indirectly out of vocobulary (OOV) tokens. Unlike what we set out to do we still don't lower-case the output, this will follow later, now let me explain what the tokenization helps us achieve:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Punctuation, e.g. ","[","!,?..], is tokenized into its own token."),(0,o.kt)("li",{parentName:"ol"},"Some compound words are split up, e.g. ",(0,o.kt)("em",{parentName:"li"},"Johan's")," is now ",(0,o.kt)("em",{parentName:"li"},"Johan"),", ",(0,o.kt)("em",{parentName:"li"},"'"),", ",(0,o.kt)("em",{parentName:"li"},"s")," which is three (3) separate tokens rather than one.")),(0,o.kt)("p",null,"Because of the updated tokenization ",(0,o.kt)("em",{parentName:"p"},"fredag")," and ",(0,o.kt)("em",{parentName:"p"},"fredag!")," is now tokenized as ","[",(0,o.kt)("em",{parentName:"p"},"fredag"),"] and ","[",(0,o.kt)("em",{parentName:"p"},"fredag"),", ",(0,o.kt)("em",{parentName:"p"},"!"),"], this in fact turns ",(0,o.kt)("em",{parentName:"p"},"fredag")," into the same token in both thus achieving the same vector when embedded which is great, because it really means the same. The exclamation mark itself also applies the same meaning to all places it's applied, which in itself is an improvement now also as we embed it separately. "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Why is this good?"),(0,o.kt)("br",{parentName:"p"}),"\n","Even though we see a direct increase in number of tokens we see a ",(0,o.kt)("strong",{parentName:"p"},"decrease")," of number of unique tokens because we now tokenize ",(0,o.kt)("em",{parentName:"p"},"borta"),", ",(0,o.kt)("em",{parentName:"p"},"borta?"),", & ",(0,o.kt)("em",{parentName:"p"},"borta!")," as the same token, with one additional for the punctuation in the two latter cases rather than 3 separate tokens which would map to different data.",(0,o.kt)("br",{parentName:"p"}),"\n","The coverage of our Word Embeddings also increase because we now tokenize the text better. Perhaps ",(0,o.kt)("em",{parentName:"p"},"borta!")," does not exist but ",(0,o.kt)("em",{parentName:"p"},"borta")," surely do exist in the embedding dictionary / lookup.   "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# A bit ugly, that's what happens when you're lazy\ndef test_dimensions(preprocessing=[stanza_tokenize]):\n    prev = flatten(df['question'].apply(lambda x: x.split()).tolist())\n    post = flatten(df['question'].apply(lambda x: preprocess(x, preprocessing)).tolist())\n\n    print(f\"Previously: {len(prev)} tokens ({len(set(prev))} unique)\")\n    print(f\"Post: {len(post)} tokens ({len(set(post))} unique)\")\n    print(f\"Token reduction by ~{100 * (1- len(set(post))/len(set(prev))):.1f} %\")\n    labels = ['#Tokens', '#Unique Tokens']\n    width = 0.35\n    x = np.arange(len(labels))\n    fig, ax = plt.subplots()\n    rects1 = ax.bar(x - width/2, [len(prev), len(set(prev))], width, label='Before')\n    rects2 = ax.bar(x + width/2, [len(post), len(set(post))], width, label='After')\n    ax.set_ylabel('Tokens')\n    ax.set_title('Tokens before and after')\n    ax.set_xticklabels(labels)\n    ax.set_xticks(x)\n    ax.legend()\n    fig.tight_layout()\n\n    plt.show()\n\n# preprocessing is a list of lambda functions to apply\ndef preprocess(text, preprocessing):\n    for f in preprocessing:\n        text = f(text)\n    return text\n")),(0,o.kt)("p",null,"Let's take a look how much this actually mattered!"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"test_dimensions()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Previously: 629 tokens (289 unique)\nPost: 713 tokens (273 unique)\nToken reduction by ~5.5 %\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"svg",src:n(4604).Z,width:"566",height:"373"})),(0,o.kt)("p",null,"The expectations set up has been achieved and we can clearly see that the raw number of tokens grew while the unique token count shrinked.",(0,o.kt)("br",{parentName:"p"}),"\n","Applying lower-case to the text will further reduce the number of unique tokens, and obviously keep the number of tokens at the same count."),(0,o.kt)("p",null,"Let's add lower-casing and see what happens!"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"lowercase = lambda x: x.lower()\npreprocess_funcs = [lowercase, stanza_tokenize]\ntest_dimensions(preprocessing=preprocess_funcs)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Previously: 629 tokens (289 unique)\nPost: 712 tokens (260 unique)\nToken reduction by ~10.0 %\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"svg",src:n(4837).Z,width:"566",height:"373"})),(0,o.kt)("h4",{id:"lower-casing"},"Lower-casing"),(0,o.kt)("p",null,"Going from 5.5 to 10 % reduction is nothing to sneeze at, by applying these two simple techniques we now have the same data in a better format which allows us to have a lower number of unique tokens.",(0,o.kt)("br",{parentName:"p"}),"\n","Pretty awesome right?"),(0,o.kt)("p",null,"Let's get on with this and apply the preprocessing to the questions and test it out with the FAQ-search!"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"df['X'] = df['question'].apply(lambda x: preprocess(x, preprocess_funcs))\ndf['X'].head()\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0                            [vad, \xe4r, coronavirus, ?]\n1                               [vad, \xe4r, covid-19, ?]\n2    [vad, skiljer, covid-19, fr\xe5n, s\xe4songsinfluens...\n3               [vilka, \xe4r, symtomen, p\xe5, covid-19, ?]\n4    [hur, vet, jag, om, mina, symtom, beror, p\xe5, p...\nName: X, dtype: object\n")),(0,o.kt)("h3",{id:"testing-the-new-input-data"},"Testing the new input-data"),(0,o.kt)("p",null,"Now that we've created our input data we need to test our model on this!",(0,o.kt)("br",{parentName:"p"}),"\n","By applying the ",(0,o.kt)("inlineCode",{parentName:"p"},"IndexedList")," which is the dataformat ",(0,o.kt)("inlineCode",{parentName:"p"},"SFE")," wants as input we can train the model and then test it."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"sfe_format = IndexedList(df['X'].tolist())\nmodel.train(sfe_format)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"(75, 712)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Helper method to test the closest questions\ndef get_n_closest_questions(question, preprocessing, n=4):\n    q_fixed = preprocess(question, preprocessing)\n    resp = model.sv.similar_by_sentence(q_fixed, model=model, indexable=df['question'].tolist()) # [([tokens], score)]\n    resp = [f'{result[2]:.2f}: {result[0]}' for result in resp]\n    print('\\n'.join(resp[:n]))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("kan min hamster bli smittad?", preprocess_funcs)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.67: Kan man bli smittad av en person som har covid-19 men som inte har n\xe5gra symtom?\n0.63: Kan covid-19 smitta mellan djur och m\xe4nniska och kan mitt husdjur smittas av viruset?\n0.54: Kan viruset smitta till m\xe4nniska via post och paket?\n0.42: Kan smitta \xf6verf\xf6ras fr\xe5n mygg till m\xe4nniska?\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("Hur f\xe5r jag min son att f\xf6rst\xe5?", preprocess_funcs)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.82: Hur pratar man med barn om det nya coronaviruset?\n0.80: Vad \xe4r covid-19?\n0.78: Hur sjuk blir man av covid-19?\n0.77: Hur l\xe4nge \xe4r man sjuk av covid-19?\n")),(0,o.kt)("h3",{id:"2-lemmatization-and-stop-words"},"2. Lemmatization and Stop Words"),(0,o.kt)("p",null,"Let's try to further improve this by actually lemmatizing and applying stop-words!"),(0,o.kt)("h4",{id:"lemmatization"},"Lemmatization"),(0,o.kt)("p",null,"So what is Lemmatization? Quoting Stanfords description:"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"}," The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: ")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"     am, are, is => be\n     car, cars, car's, cars' => car \n")),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"The result of this mapping of text will be something like:"),(0,o.kt)("pre",{parentName:"blockquote"},(0,o.kt)("code",{parentName:"pre"},"     the boy's cars are different colors =>\n     the boy car be differ color \n"))),(0,o.kt)("h4",{id:"what-is-stop-words"},"What is stop-words?"),(0,o.kt)("p",null,"Stop-words are words we want to throw away as they add no real purpose. In older Machine Learning approaches it was way more important to add stop-words but in newer Deep Learning with Neural Networks stop-words often can be a negative thing, removing understanding of the sentence and perhaps minor differences which makes the world for understanding."),(0,o.kt)("p",null,"A example of a stop-word list could be ",(0,o.kt)("inlineCode",{parentName:"p"},'["hej", "vem", "d\xe5", "och", ...]')," which means that these words would be removed from a sentence."),(0,o.kt)("p",null,"In our case it makes sense to remove words like 'vad', 'varf\xf6r'  and so on because the return of the FAQ seems to be very weighted towards these words."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"nlp = stanza.Pipeline(lang='sv', processors='tokenize,mwt,pos,lemma', logging_level='ERROR')\nstanza_lemma = lambda x: [token.lemma for sentence in nlp(x).sentences for token in sentence.words]\npreprocess_funcs_lemma = [lowercase, stanza_lemma]\n\nprint(f'Previously:\\t{preprocess(\"hur f\xf6rklarar jag f\xf6r min dotter och son?\", preprocess_funcs)}')\nprint(f'After:\\t\\t{preprocess(\"hur f\xf6rklarar jag f\xf6r min dotter och son?\", preprocess_funcs_lemma)}')\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Previously: ['hur', 'f\xf6rklarar', 'jag', 'f\xf6r', 'min', 'dotter', 'och', 'son', '?']\nAfter:      ['hur', 'f\xf6rklara', 'jag', 'f\xf6r', 'jag', 'dotter', 'och', 'son', '?']\n")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Some interesting notes"),(0,o.kt)("br",{parentName:"p"}),"\n","Seeing '",(0,o.kt)("em",{parentName:"p"},"min"),"' getting converted to '",(0,o.kt)("em",{parentName:"p"},"jag"),"' is both good and bad, in this case we reduce dimensionality of the problem but we loose context and understanding. ",(0,o.kt)("em",{parentName:"p"},"jag")," and ",(0,o.kt)("em",{parentName:"p"},"min")," certainly does not mean the same thing."),(0,o.kt)("p",null,"Let's see how it pans out..."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"test_dimensions(preprocess_funcs_lemma)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Previously: 629 tokens (289 unique)\nPost: 712 tokens (228 unique)\nToken reduction by ~21.1 %\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"svg",src:n(113).Z,width:"566",height:"373"})),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"del model\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocess_funcs_lemma))\nsfe_format = IndexedList(df['X'].tolist())\n\nmodel.train(sfe_format)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"(75, 712)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("kan min hamster bli smittad?", preprocess_funcs_lemma)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.75: Kan covid-19 smitta mellan djur och m\xe4nniska och kan mitt husdjur smittas av viruset?\n0.69: Hur smittar covid-19?\n0.68: Kan man smittas flera g\xe5nger av det nya coronaviruset?\n0.63: Smittar covid-19 via vatten och mat?\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("Hur f\xe5r jag min son att f\xf6rst\xe5?", preprocess_funcs_lemma)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.79: Vad \xe4r covid-19?\n0.75: Hur sjuk blir man av covid-19?\n0.74: Hur l\xe4nge \xe4r man sjuk av covid-19?\n0.66: Om en person i familjen \xe4r sjuk - m\xe5ste alla stanna hemma d\xe5?\n")),(0,o.kt)("h4",{id:"analyzing-the-results"},"Analyzing the results"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Improvements?"),(0,o.kt)("br",{parentName:"p"}),"\n","Not really, the model has an improved response to the 'hamster-question' but it's way off when asking about the son.  "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Why?"),(0,o.kt)("br",{parentName:"p"}),"\n","The most likely explanation is that even though we reduce the input dimensions an awful lot we remove dimensions that brings value, and removing value is bad - just as was touched upon previously. It might be helpful in some cases, perhaps this could prove helpful for a supervised approach such as TF-IDF + Support Vector Machine.  "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Any good parts?"),(0,o.kt)("br",{parentName:"p"}),"\n","Yes, we can see some pretty hefty memory-requirement reductions when working with other types of models by applying this. Actually, in the case of this we could reduce the memory requirement by lemmatizing the dictionary of the embeddings and removing all non-lemmas. All in all, this could lead to a small performance loss but great memory win."),(0,o.kt)("h4",{id:"stop-words"},"Stop words"),(0,o.kt)("p",null,"As promised we shall apply stop-words, but as we saw no performance gain with lemmatization we'll keep the old tokenization."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"stop_words = get_stop_words('sv')\nclean_stop = lambda x: [word for word in x if word not in stop_words]\npreprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop]\n\ndel model\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don't need to remove stop-words in the sentences in our \nsfe_format = IndexedList(df['X'].tolist())\n\nmodel.train(sfe_format)\n\npreprocess(\"hur f\xf6rklarar jag f\xf6r min dotter och son?\", preprocessing_func_stop)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"['f\xf6rklarar', 'dotter', 'son', '?']\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"test_dimensions(preprocessing_func_stop)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Previously: 629 tokens (289 unique)\nPost: 417 tokens (206 unique)\nToken reduction by ~28.7 %\n")),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"svg",src:n(7419).Z,width:"566",height:"373"})),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("kan min hamster bli smittad?", preprocessing_func_stop)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.66: Kan man bli smittad av en person som har covid-19 men som inte har n\xe5gra symtom?\n0.64: Kan covid-19 smitta mellan djur och m\xe4nniska och kan mitt husdjur smittas av viruset?\n0.54: Kan viruset smitta till m\xe4nniska via post och paket?\n0.41: Kan smitta \xf6verf\xf6ras fr\xe5n mygg till m\xe4nniska?\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("Hur f\xe5r jag min son att f\xf6rst\xe5?", preprocessing_func_stop)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.83: Vad \xe4r covid-19?\n0.83: Hur pratar man med barn om det nya coronaviruset?\n0.80: Hur sjuk blir man av covid-19?\n0.80: Hur l\xe4nge \xe4r man sjuk av covid-19?\n")),(0,o.kt)("h4",{id:"further-analyzing"},"Further analyzing"),(0,o.kt)("p",null,"In my mind we've some pretty good responses, in a way better and another way worse than lemmatizaton. Certainly not a set-back but neither a step forward.",(0,o.kt)("br",{parentName:"p"}),"\n","Testing different approaches and turning things on and off is a great way to increase data understanding and also gives a better sense of what different preprocessing functions actually does.",(0,o.kt)("br",{parentName:"p"}),"\n","In fact this is actually part of the most common Machine Learning development approach, working much like agile, which is iteratively circular and called ",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining"},"CRISP-DM"),". I won't go deeply into CRISP-DM (already did once in my Master Thesis), but the following image gives you the gist.",(0,o.kt)("br",{parentName:"p"}),"\n",(0,o.kt)("img",{parentName:"p",src:"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b9/CRISP-DM_Process_Diagram.png/220px-CRISP-DM_Process_Diagram.png",alt:"CRISP-DM"})),(0,o.kt)("p",null,"Finally, as we see no great impact by applying either lemmatization nor stop-words we might just give up at the lower-case + stanza tokenization, but I'd like to make one last shot in the dark - custom stop words! Let's see how it fares..."),(0,o.kt)("h5",{id:"custom-stop-words-breaking-the-rules"},"Custom Stop Words (breaking the rules)"),(0,o.kt)("p",null,"So I decided to break the rules and create a small simple heuristic by applying custom stop words.",(0,o.kt)("br",{parentName:"p"}),"\n","Let's figure out which words we should remove using the following steps (which could in fact be automated)! "),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Find the most common words"),(0,o.kt)("li",{parentName:"ol"},"Remove the ones which does not give any greater value")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"from collections import Counter\n\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocess_funcs))\n\ncounter = Counter(flatten(df['X'].tolist()))\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"sorted(counter.items(), key=lambda item: item[1], reverse=True)[:15]\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"[('?', 75),\n ('covid-19', 28),\n ('vad', 25),\n ('och', 22),\n ('hur', 21),\n ('f\xf6r', 20),\n ('det', 15),\n ('kan', 14),\n ('i', 14),\n ('jag', 13),\n ('av', 13),\n ('g\xe4ller', 12),\n ('som', 12),\n ('\xe4r', 11),\n ('en', 11)]\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"stop_words = ['?', 'och', 'jag', 'i', '\xe4r', 'en', 'min', '?']\nclean_stop = lambda x: [word for word in x if word not in stop_words]\npreprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop]\n\ndel model\nmodel = uSIF(ft_wv, workers=4, lang_freq=\"sv\")\ndf['X'] = df['question'].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don't need to remove stop-words in the sentences in our \nsfe_format = IndexedList(df['X'].tolist())\n\nmodel.train(sfe_format)\n\npreprocess(\"hur f\xf6rklarar jag f\xf6r min dotter och son?\", preprocessing_func_stop)\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"['hur', 'f\xf6rklarar', 'f\xf6r', 'dotter', 'son']\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("Hur f\xe5r jag min son att f\xf6rst\xe5?", preprocessing=preprocess_funcs)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.83: Hur pratar man med barn om det nya coronaviruset?\n0.83: Vad \xe4r covid-19?\n0.80: Hur sjuk blir man av covid-19?\n0.79: Hur l\xe4nge \xe4r man sjuk av covid-19?\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'get_n_closest_questions("kan min hamster bli smittad?", preprocessing=preprocess_funcs)\n')),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"0.66: Kan man bli smittad av en person som har covid-19 men som inte har n\xe5gra symtom?\n0.63: Kan covid-19 smitta mellan djur och m\xe4nniska och kan mitt husdjur smittas av viruset?\n0.54: Kan viruset smitta till m\xe4nniska via post och paket?\n0.41: Kan smitta \xf6verf\xf6ras fr\xe5n mygg till m\xe4nniska?\n")),(0,o.kt)("p",null,"Not bad, not amazing - I feel pretty happy about this."),(0,o.kt)("p",null,"So what can be done from now on if time and resources where available?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Add a classifier + TF-IDF"),(0,o.kt)("li",{parentName:"ul"},"BERT / ALBERT QA (the State-of-the-Art right now)")),(0,o.kt)("p",null,"Thanks for this time,",(0,o.kt)("br",{parentName:"p"}),"\n","-"," Hampus Lond\xf6g\xe5rd"))}c.isMDXComponent=!0},4604:function(e,t,n){t.Z=n.p+"assets/images/output_13_1-cc0496d7b672d1ee2005497392dd4611.svg"},4837:function(e,t,n){t.Z=n.p+"assets/images/output_15_1-a6131ac017ffe82d6bed8e1931273c60.svg"},113:function(e,t,n){t.Z=n.p+"assets/images/output_26_1-1f2c8cbba1a5c5813a268a8452a53e0b.svg"},7419:function(e,t,n){t.Z=n.p+"assets/images/output_32_1-b7481f784245e9758a3cdf466b6f29b2.svg"}}]);