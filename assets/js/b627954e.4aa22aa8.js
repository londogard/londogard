"use strict";(self.webpackChunklondogard=self.webpackChunklondogard||[]).push([[5315],{3905:function(e,t,i){i.d(t,{Zo:function(){return c},kt:function(){return h}});var n=i(7294);function r(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function a(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function o(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?a(Object(i),!0).forEach((function(t){r(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):a(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function s(e,t){if(null==e)return{};var i,n,r=function(e,t){if(null==e)return{};var i,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||(r[i]=e[i]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):o(o({},t),e)),i},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var i=e.components,r=e.mdxType,a=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(i),h=r,m=u["".concat(l,".").concat(h)]||u[h]||d[h]||a;return i?n.createElement(m,o(o({ref:t},c),{},{components:i})):n.createElement(m,o({ref:t},c))}));function h(e,t){var i=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=i.length,o=new Array(a);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<a;p++)o[p]=i[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,i)}u.displayName="MDXCreateElement"},4469:function(e,t,i){i.r(t),i.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return p},assets:function(){return c},toc:function(){return d},default:function(){return h}});var n=i(7462),r=i(3366),a=(i(7294),i(3905)),o=["components"],s={title:"Probabilistic Forecasting Made Simple",description:"While researching probabilistic forecasting in a client project I managed to find a paper which opens the door to\xa0**any**\xa0neural network\xa0_with dropout_\xa0- which is the majority. That is, we can do probabilistic forecasting with essentially any network!",slug:"timeseries-learnings",tags:["machine-learning","timeseries"],authors:"hlondogard"},l=void 0,p={permalink:"/blog/timeseries-learnings",editUrl:"https://github.com/londogard/londogard/blog/2022-11-28-probabilistic-forecasting/index.md",source:"@site/blog/2022-11-28-probabilistic-forecasting/index.md",title:"Probabilistic Forecasting Made Simple",description:"While researching probabilistic forecasting in a client project I managed to find a paper which opens the door to\xa0**any**\xa0neural network\xa0_with dropout_\xa0- which is the majority. That is, we can do probabilistic forecasting with essentially any network!",date:"2022-11-28T00:00:00.000Z",formattedDate:"November 28, 2022",tags:[{label:"machine-learning",permalink:"/blog/tags/machine-learning"},{label:"timeseries",permalink:"/blog/tags/timeseries"}],readingTime:2.835,truncated:!0,authors:[{name:"Hampus Lond\xf6g\xe5rd",title:"Main Contributor of Londogard",url:"https://github.com/lundez",imageURL:"https://github.com/lundez.png",key:"hlondogard"}],frontMatter:{title:"Probabilistic Forecasting Made Simple",description:"While researching probabilistic forecasting in a client project I managed to find a paper which opens the door to\xa0**any**\xa0neural network\xa0_with dropout_\xa0- which is the majority. That is, we can do probabilistic forecasting with essentially any network!",slug:"timeseries-learnings",tags:["machine-learning","timeseries"],authors:"hlondogard"},nextItem:{title:"Timeseries Learnings at AFRY",permalink:"/blog/timeseries-learnings"}},c={authorsImageUrls:[void 0]},d=[{value:"Probabilistic Forecasting Made Simple",id:"probabilistic-forecasting-made-simple",children:[],level:2},{value:"How to do probabilistic forecasting on any deep learning model",id:"how-to-do-probabilistic-forecasting-on-any-deep-learning-model",children:[],level:2},{value:"The possibilities",id:"the-possibilities",children:[{value:"1. Model Understanding (Weakness/Strength)",id:"1-model-understanding-weaknessstrength",children:[],level:4},{value:"2. Downstream Consumer Happiness",id:"2-downstream-consumer-happiness",children:[],level:4}],level:2},{value:"Sources",id:"sources",children:[],level:2}],u={toc:d};function h(e){var t=e.components,s=(0,r.Z)(e,o);return(0,a.kt)("wrapper",(0,n.Z)({},u,s,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"probabilistic-forecasting-made-simple"},"Probabilistic Forecasting Made Simple"),(0,a.kt)("p",null,"Probabilistic Forecasting is something very cool, but it is not approachable in the current state of affairs.\xa0"),(0,a.kt)("p",null,"While researching probabilistic forecasting in a client project I managed to find a paper which opens the door to ",(0,a.kt)("strong",{parentName:"p"},"any")," neural network ",(0,a.kt)("em",{parentName:"p"},"with dropout"),"\xa0- which is the majority. That is, we can do probabilistic forecasting with essentially any network!  "),(0,a.kt)("p",null,(0,a.kt)("a",{parentName:"p",href:"https://github.com/unit8co/darts",title:"https://github.com/unit8co/darts"},"Darts"),", a brilliant timeseries library, includes a very competent probabilistic forecasting but it\u2019s not really applicable to all models. This is the reason that I started diving into the whole space of probabilistic forecasting. A probabilistic model includes not only a raw prediction value but a distribution of possible points, which ends up with a prediction like:"),(0,a.kt)("p",null,(0,a.kt)("img",{src:i(253).Z,width:"1077",height:"789"}),"  "),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Probabilistic Model by ",(0,a.kt)("a",{parentName:"p",href:"https://unit8.com/resources/probabilistic-forecasting-in-darts/",title:"https://unit8.com/resources/probabilistic-forecasting-in-darts/"},"unit8/darts"),"  ")),(0,a.kt)("p",null,"Additionally models like ARIMA and ExponentialSmoothing allows to do this kind of thing very easily, simply sample running simulations of their state-spaced models with a bit of randomly sampled errors. To solve this on their deep learning models darts decided to model distribution using a ",(0,a.kt)("inlineCode",{parentName:"p"},"Likelihood"),"\xa0 class. What does this mean?",(0,a.kt)("br",{parentName:"p"}),"\n","The model does not actually predict a value but a distribution, using ",(0,a.kt)("inlineCode",{parentName:"p"},"Gaussian"),"\xa0 we\u2019d predict two values - ",(0,a.kt)("inlineCode",{parentName:"p"},"mean"),"\xa0 and ",(0,a.kt)("inlineCode",{parentName:"p"},"std"),"\xa0.  "),(0,a.kt)("h2",{id:"how-to-do-probabilistic-forecasting-on-any-deep-learning-model"},"How to do probabilistic forecasting on any deep learning model"),(0,a.kt)("p",null,"By combining the knowledge in ",(0,a.kt)("em",{parentName:"p"},(0,a.kt)("a",{parentName:"em",href:"https://arxiv.org/pdf/1709.01907.pdf",title:"https://arxiv.org/pdf/1709.01907.pdf"},"Deep and Confident Prediction Time Series at Uber")),"\xa0by L. Zhi & N. Laptev (2017) with\xa0",(0,a.kt)("em",{parentName:"p"},(0,a.kt)("a",{parentName:"em",href:"https://arxiv.org/pdf/1703.04977.pdf",title:"https://arxiv.org/pdf/1703.04977.pdf"},"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?")),"\xa0by A. Kendall & Y. Gal (2017) one can conclude that it\u2019s possible to model distributions using dropout during inference. In the Uber paper they use a special variant they call \u201cMonte Carlo dropout\u201d, which I don\u2019t believe is required to achieve interesting results. Using the pure dropout-module which randomly zeroes some elements by a probability p\xa0 sampling from a Bernoulli Distribution."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"How do we do this?")),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Activate Dropout during Inference."),(0,a.kt)("li",{parentName:"ol"},"Do x\xa0 predictions with a dropout probability p."),(0,a.kt)("li",{parentName:"ol"},"Based on these x\xa0predictions we have a distribution of data."),(0,a.kt)("li",{parentName:"ol"},"Build a ",(0,a.kt)("em",{parentName:"li"},"confidence interval"),"\xa0from the points.")),(0,a.kt)("h2",{id:"the-possibilities"},"The possibilities"),(0,a.kt)("p",null,"There\u2019s a lot of possiblities, I\u2019ll share two of our biggest ones."),(0,a.kt)("h4",{id:"1-model-understanding-weaknessstrength"},"1","."," Model Understanding (Weakness/Strength)"),(0,a.kt)("p",null,"By returning a ",(0,a.kt)("em",{parentName:"p"},"probabilistic forecast"),", i.e. a distribution/confidence interval, we can learn more about the model and its strengths/weaknesses.\xa0"),(0,a.kt)("p",null,"In our project(s) we\u2019ve seen that it opens a door to really figure out how to improve our models by focusing on the areas were the model is the most uncertain. This has proved to improve performance by a substantial amount which makes the effort worth it."),(0,a.kt)("h4",{id:"2-downstream-consumer-happiness"},"2","."," Downstream Consumer Happiness"),(0,a.kt)("p",null,"We see that our clients trust the model further by being able to see how confident they are. Building trust between model and downstream consumer is really important to deliver an actual successful project, which once again makes the effort totally worth it!"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Bonus:")," we also found that it opens new possibilities to chain of the inference power if you keep it in production, as your downstream tasks can now make use of a confidence interval rather than a raw data point. But\xa0the inference is very expensive compared to the usual (remember we do x\xa0predictions per prediction)!"),(0,a.kt)("h2",{id:"sources"},"Sources"),(0,a.kt)("p",null,(0,a.kt)("em",{parentName:"p"},(0,a.kt)("a",{parentName:"em",href:"https://arxiv.org/pdf/1709.01907.pdf",title:"https://arxiv.org/pdf/1709.01907.pdf"},"Deep and Confident Prediction Time Series at Uber")),"\xa0by L. Zhi & N. Laptev (2017) -\xa0",(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1709.01907.pdf"},"https://arxiv.org/pdf/1709.01907.pdf"),"  "),(0,a.kt)("p",null,(0,a.kt)("em",{parentName:"p"},(0,a.kt)("a",{parentName:"em",href:"https://arxiv.org/pdf/1703.04977.pdf",title:"https://arxiv.org/pdf/1703.04977.pdf"},"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?")),"\xa0by A. Kendall & Y. Gal (2017) -\xa0",(0,a.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1703.04977.pdf"},"https://arxiv.org/pdf/1703.04977.pdf")))}h.isMDXComponent=!0},253:function(e,t,i){t.Z=i.p+"assets/images/image-d18296cf9042fa153edce5ab35c7103f.png"}}]);