{
  
    
        "post0": {
            "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
            "content": "Disclaimer: this post is pretty long and I recommend reading one part at a time (it’s 3 parts). . Personally I hate unfinished blogs that are multiple parts, hence I uploaded all at once. So be assured, you’re getting all parts - right here, right now! :happy: . Finally, the first part is purely informational about how everything works and the second part is how to actually code the game. The second part is interactive and contains a lot of TODOs. The third, and final, part covers how we are able to use the same code on JS/Browser &amp; Native . All the code is available here. . Introduction . First off, what is Kotlin? . . Image from kotlinlang.org . By my own account it’s a language that has learned from many mistakes done in the past and tries to extend and embrace the good ones! . The most obvious one solved by Kotlin is “The Billion Dollar Mistake” as the inventor Tony Hoare calls it himself, namely null. Kotlin is not alone about this, but certainly off to a good start! . Some mentionable features on top of this is . Coroutines - A more efficient (lightweight) threading model, also called “green threads” sometimes. Feels very natural and easy. | Data-Science &amp; Jupyter support | Extension functions - Perhaps my favourite feature, do you feel a class is missing a function? No problem, you’re free to do so! | Excellent typing - Perhaps not Scala, but still very good. | &amp; more! | . All of this is available through Kotlins Multiplatform effort, where Multiplatform does not mean Mac/Windows/Linux but rather that we can compile into different platforms such as Java Bytecode (JVM), Native and JS/Browser. . Enough praises, let’s get onto how the multiplatform solution actually works through Part 1. . Part 1: How does Kotlin Multiplatform work? . Let’s start by explaining Native, what exactly is Native? From the landing page of kotlinlang.org/native. . Kotlin/Native is a technology for compiling Kotlin code to native binaries, which can run without a virtual machine. It is an LLVM based backend for the Kotlin compiler and native implementation of the Kotlin standard library. . By this statement we learn that native refers to binary executables that can run on a OS (natively). No virtual machine or browser required! What does this mean in practice? . ✔️ Small file size ✔️ No overhead ✔️ Incredibly fast starting-time . As usual it isn’t a win-win situation but you loose some . ❌Development speed ❌… and so on. . LLVM . What is LLVM? LLVM is probably the biggest project (compiler) that exists to build native binaries. Languages such as C, C++, Haskell, Rust &amp; Swift compiles to native binaries through LLVM. . Moving back to the info-text, . It is an LLVM based backend for the Kotlin compiler and native implementation of the Kotlin standard library. . So… What is a backend? More specifically, what is a backend for a compiler? . How the Kotlin Compiler works, Frontend to Backend . First, what is a compiler? A compiler is like a translator, just as you’d translate Swedish into English a compiler instead translates computer code written in one programming language into another one of lower level, e.g. assembly. . What are the steps then taken? In general all compilers follow the same pattern, and Kotlin is no different. Even though it’s a similar path it’s interesting to learn about, even more if you don’t know how it usually works! The Kotlin Compiler first compiles Kotlin code into a Intermediate Representation, or IR, which it later turns into Java Bytecode, when targeting the Java Virtual Machine (JVM). . . The first part is called the Compiler Frontend . The Kotlin Compiler Frontend . . As mentioned the Compiler Frontend turns the Kotlin code into a Intermediate Representation of the code which is represented by a abstract syntax tree, which is built from concrete syntax, e.g. strings. The process involves lexical analysis which creates tokens and pass it forward to the parser that finally builds said abstract syntax tree. For those interested this could be a really fun challenge and learning lesson to implement yourself! . Moving on, the second and final part is called the Compiler Backend. . The Kotlin Compiler Backend . . The Compiler Backend actually turns this abstract syntax tree, or IR, into computer output language. In this case it’s Java Bytecode which is understood by the JVM. The backend is the part that actually optimize code to remove for-loops where applicable, exchange variables into constants and so on. Just as with the frontend it’s a really good challenge to either implement a backend without optimizations, or focus on a existing one and adding a optimization yourself! . What is interesting about Kotlin is that it has different backends, which means that the IR compile not only into Java Bytecode but also JS/Browser &amp; Native binaries. . . Side-note: For Native Backend we actually have two Intermediate Representations, first Kotlin IR which is compiled into LLVM IR which LLVM then compiles into a native binary through its own Compiler Backend. This means that all the optimizations applied to C, C++, Swift &amp; many more are also applied to Kotlin Native code! . How Kotlin keeps multiplatform clean . It might sound messy to target multiple platforms like this, and how could it possibly end up clean? . By using the standard libraries that are included with Kotlin, which includes almost everything you need, and multiplatform-developed community libraries, e.g. SQLDelight, you get code that looks the same and works the same irregardless of if you target JS/Browser, Native or the JVM. . To give an example of how Kotlin std-lib works, let’s take one of the most common types - String. . . By using Kotlin.String rather than the usual Java.lang.String you do when programming Java you get a String type that works on multiple platform and has the same convenience functions (&amp; even more than the Java one). Imagine, you can write native code using .substring, .take(n) &amp; .replace - amazing compared to c right? :happy: . Thinking about the compiler, this means that the Backend Compiler automatically maps the IR of a Kotlin.String into the correct type. . You can take this concept and apply to anything such as IO , network &amp; more - all which are included in the std-lib! . Wrapping up how Native works . Let’s recollect what we’ve gone through . Kotlin Compiler compiles Kotlin code into a Intermediate Representation (IR) through the Compiler Frontend. This IR is a abstract syntax tree | | Kotlin Compiler then goes the Compiler Backend which turns the code into the lower level language, e.g. Java Bytecode, and applies optimisations | Kotlin has a std-lib which has functionalities as Kotlin.String, Kotlin.List, networking and much more. Kotlin.String turns into KString which is Kotlins own native strings with a lot of helper methods. | | What more could be good to know in relation to native? Native has a lot of quirks like pointers, address space and much more! . Kotlin has solved memory allocation through the same approach as Swift, namely reference counting which deallocates objects once they’ve got no references. There’s some advantages such as being really fast, but also downsides such as reference cycles which it handles poorly. . Kotlin also has some really nice convenience syntax such as the memScope-block. . Outro: Kotlin Multiplatform and why it matters . ✔️ One code-base for common logic . Serialization logic, e.g. parsing JSON into a data class | Networking | Database | . ✔️ Development speed ✔️ Required Knowledge . ❌ Still requires some code in said language . Especially for UI | . So all in all we can share our code between platforms which improves development speed &amp; quality in multiple ways. . The biggest “downside” is that even though we share the code we most likely will need some kind of specific code for the platform, for the GUI on iOS as an example. Perhaps compose can help us get closer to that reality soon - who knows. The final, and perhaps obvious, one I’d like to mention straight away is that platform specific libraries of course are not usable on multiplatform. This includes libraries such as React (JS) &amp; ncurses (native). . Personally I see Kotlin Multiplatform as a great way to share core logic between different targets, but one must use it with care and not try to force it into being used everywhere in every way. . Part 2: How to set up Multiplatform and build Snake . So first we need to understand how to set up a Multiplatform project. The official guide is actually really good, and if you’re using IntelliJ it’s a breeze to setup! Just as in the guide make sure to select Library. . . Let the build run, gradle is a really good build-tool that I’d like to discuss more. But for now let’s just enjoy the simplicity of how our whole project is setup with builds possible for JS, JVM &amp; Native which also contains our common-code which is the glue! . Building a JVM App (Snake) . Let’s start simple, Keep It Simple Stupid (KISS) principle applied, and create a JVM app which is easily runnable on all OS:es and has great debugging! . First off, we need to draw something. This is easiest done through the Swing library which is included in the default jdk, some might call it old but hey - it does the job.Create a file called main.kt in src/jvmMain/kotlin. . Swing has a built-in threading solution (almost too bad, because Coroutines are awesome in Kotlin!) and the best way to start the GUI is by using the existing EventQueue class and its invokeLater function. invokeLater makes sure the code runs last in the EventQueue if you add more methods, which makes sense - you want to draw the UI as the final thing. . fun main() { EventQueue.invokeLater { JFrame().apply { title = &quot;Snake&quot; isVisible = true } } } . Where the apply is a context wrapper that takes the object and uses it as context (this) inside of the block/scope ({}). See its signature: . inline fun &lt;T&gt; T.apply(block: T.() -&gt; Unit): T . This would equate to . val jframe = JFrame() jframe.title = &quot;Snake&quot; jframe.isVisible = true . in more Java-like syntax. Why use apply? It allows us to achieve some interesting chaining concepts which I really enjoy. . Now run the main-function, there should be a green “run”-button at the left, press that. Hopefully it compiles and a window will appear, with the title set to “Snake”. . Awesome! We need to render something inside of the box, a game soon enough, let’s see how we can achieve that. . Adding some minor refactoring and some new classes we can draw something . class Board: JPanel() { init { TODO(&quot;&quot;&quot; - Set background to black - Allow focus - Set preferredSize to some 200x300 &quot;&quot;&quot;) } } class GUI: JFrame() { init { title = &quot;Snake&quot; isVisible = true isResizable = false setLocationRelativeTo(null) defaultCloseOperation = EXIT_ON_CLOSE TODO(&quot;Add the Board to the JFrame, through add()&quot;) } } fun main() { EventQueue.invokeLater { GUI() } } . What are we doing? JFrame was refactored GUI which then is a subclass of JFrame, with a few extra attributes were added such as defaultCloseOperation = EXIT_ON_CLOSE that makes sure the program exits if we close the window, feel free to test it out! Further a Board was added which extends JPanel, it’s in the Board the game will be rendered. Finally, add(Board()) allows us to add our Board to the JFrame. . Run! Something is not right.. The background seems black enough, but the size is most likely not correct. We can’t even resize as isResizable=false was set. Make sure to add pack() at the end, as in . class GUI: JFrame() { init { { /** same code as before */ } add(Board()) pack() } } . What pack() does is that it packs and resizes the JFrame to include all its component(s) and their current size(s). . Super! We’re now able to render our Board and see the whole deal. . Drawing the snake &amp; apple . We’ve got the canvas (Board), now we just need to get artsy and add a Snake and some Apples! I’ll keep it simple and will make the Board exist of a few cells, all pretty large. On each cell you either have nothing, Snake or Apple - pretty simple right? JPanel has some nice-to-have methods built-in, such as repaint() which simply repaints the component, which in turns calls paintComponent(g: Graphics?) to paint/render it. . Disclaimer: the code might not be the most idiomatic, but I try to introduce a few concepts. . class Board: JPanel() { init { /** same code as before */ } override fun paintComponent(g: Graphics?) { super.paintComponent(g) val g2d: Graphics2D = g as? Graphics2D ?: return g2d.scale(20.0, 20.0) g2d.color = Color.GREEN g2d.fill(Rectangle(5, 6, 1, 1)) g2d.fill(Rectangle(5, 7, 1, 1)) g2d.fill(Rectangle(5, 8, 1, 1)) } } . Once again, what’s actually happening? First, we override the function paintComponent which renders Board layout. The input is a nullable Graphics, which is shown by the type having a ? at the end. This is a cool property of Kotlin, if something can be null it actually is a type. No Option/Maybe, just pure type. Then Graphics? is cast to non-null Graphics2D through a safe approach using as?, without ? the cast can crash, with ? the cast would return null if failing. Finally we use a elvis-expression ?: which is basically a wrapper for if (null) doThis else doThat, so if the left-hand-side is null it’ll give the right-hand-side. The right-hand-side in our case is a empty return statement, meaning that we just make a early-exit. If the value is not null it’ll give the non-null variant of the type! . Example use-case of elvis-operator ?: val a: Int = 1 ?: 0 // a = 1 val b: Int = null ?: 0 // b = 0 . Detailing the code further we now have g2d: Graphics2D where Graphics2D which gives us a few nice functions to draw components on the Board. . We set the scale to 20 This simplifies the behaviour, we can now use 20x30 grid where each cell is size 1, but it’s scaled into the 200x300 grid. | . | We use fill to draw Rectangle’s with set Color. | . Side-note: For those wondering how you safely execute on nulls by chaining, like you do with Monads (Option) . val nullableGraphics: Graphics? = null nullableGraphics?.scale(20.0, 20.0) // This is safe! No operation executed if null . Summing up, we now know how to render stuff on the Board and it’s all very static. The next step is to make the rendering less static and I believe the natural step from now is to create the data structures that’ll contain the game &amp; its state. Then we can make sure the data structures are able to update, so we can render new states. . Creating the data structures . Data structures are required to have a game state, that is the score and position of everything. . The natural state is Game which contains everything, let’s begin by creating a Game structure which contains the size of the Board. . data class Game(val width: Int, val height: Int) . Side-note: A data class is essentially the same as a case class from Scala. And for those who don’t know what a case class is it’s basically a class that simplifies a lot of stuff, mainly used as a data structure. You get equals, getters &amp; setters, and much more for free. Anyone from Java knows how awesome this is. . Moving on we need to define the cells mentioned, something like . data class Cell(val x: Int, val y: Int) . Wrapping up our current state we got most of what we need, Game which contains our game state &amp; Cell which is our co-ordinates. The next step is to actually draw the Cell’s and wrap the Cell in other classes such as Apple and Snake. Let’s add all the required code. . data class Apples(val cells: Set&lt;Cell&gt; = emptySet()) data class Snake(val cells: List&lt;Cell&gt;) { val head: Cell = TODO(&quot;Take the first cell.&quot;) val tail: List&lt;Cell&gt; = TODO(&quot;Drop one cell and return the rest.&quot;) } data class Game( val width: Int, val height: Int, val snake: Snake, // Adding snake and apples val apples: Apples ) class Board: JPanel() { private val game: Game = Game( 20, 30, Snake(listOf(Cell(2,3),Cell(2,4),Cell(2,5),)), Apples(setOf(Cell(4,5))) ) init { background = Color.black isFocusable = true preferredSize = Dimension(200, 300) } override fun paintComponent(g: Graphics?) { super.paintComponent(g) val g2d = g as? Graphics2D ?: return g2d.scale(20.0, 20.0) g2d.color = Color.GREEN game.snake.tail.forEach { cell -&gt; TODO(&quot;Render the cells using the previously used technique&quot;) } TODO(&quot;Render the head using the color YELLOW&quot;) TODO(&quot;Render the apples using the color RED&quot;) } } . Fixing the added TODOs and keeping the same GUI &amp; fun main we can now run the code. You should be seeing something like . . Pretty cool right!? We’ve got :white_check_mark: Rendering :white_check_mark: Data Structures . What’s left? . A game loop | Ability to actually move the data structures ( Snake) | Adding a Direction . To be able to move we need to know what directions to move in. In my humble opinion this is simplest done through a Enum. . enum class Direction { UP, DOWN, LEFT, RIGHT } . Simple enough. But let’s make it better, even though pre-optimization is the root of all evil it is sometimes fun :grinning:. Enums in Kotlin are pretty awesome, they can both keep values and have methods! Let’s add dx and dy. . enum class Direction(val dx: Int, val dy: Int) { // --&gt; // | // v UP(0, -1), DOWN(0, 1), LEFT(-1, 0), RIGHT(1, 0); } . Through dx and dy we can add it to the current cell to move in the direction which Direction is! . Updating Snake.kt &amp; Cell.kt to have Cell with Direction and some turn. . data class Cell(val x: Int, val y: Int) { fun move(direction: Direction) = TODO(&quot;Create new cell which moves in direction. OBS: Remember Direction now has dx, dy!&quot;) } data class Snake( val cells: List&lt;Cell&gt;, val direction: Direction, // new attribute val eatenApples: Int = 0 // new attribute ) { fun move(): Snake { val newHead = TODO(&quot;Move head&quot;) val newTail = TODO(&quot;Move tail!&quot;) return TODO(&quot;Create a new Snake with the updated position!&quot;) } fun turn(newDirection: Direction?) = TODO(&quot;Make sure to turn correctly&quot;) } . This is all fine &amp; dandy, but there is some improvements to be made that’ll clean up the code. I mentioned that Kotlin Enums can have methods, which is awesome. We can simplify the turn-logic by adding a method to Direction, namely isOppositeTo. See the code below. . enum class Direction(val dx: Int, val dy: Int) { /** Same code as previously */ fun isOppositeTo(that: Direction) = dx + that.dx == 0 &amp;&amp; dy + that.dy == 0 } . Right, we can now turn the snake and render the game. We need the Game-state to update to actually re-render the updated Snake, let’s add a update-function that does this. . fun update(direction: Direction?): Game { return TODO(&quot;&quot;&quot; Make sure to 1. Turn snake in direction 2. Move 3. Update the game state by returning Game &quot;&quot;&quot;) } . And our GUI . class Board: JPanel() { var dir: Direction = Direction.RIGHT var game: Game = Game( 20, 30, Snake(listOf(Cell(2,3),Cell(2,4),Cell(2,5),), dir), Apples(setOf(Cell(4,5))) ) init { addKeyListener(object : KeyAdapter() { override fun keyPressed(e: KeyEvent?) { dir = when (e?.keyCode) { VK_I, VK_UP, VK_W -&gt; Direction.UP else -&gt; TODO(&quot;Add the other key bindings. Reflect of how the object works and what is happening.&quot;) } game = game.update(dir) repaint() } }) } } . In our init (equal to a constructor) we add a keyListener which will listen on whenever we move. We moved game to be a var which allows us to change the reference. . Side-note: The difference between a val and var is not about immutability of the value, but rather that you cannot change the pointer to the object. By using val the compiler don’t allow you to change the reference. . val a = 1 a = 3 // CRASH -- This is not allowed var b = 1 b = 3 // b = 3, this is allowed. . Please note that this means that if your object is mutable, you can mutate the state of the object even though it’s a val. . Why put game on a var you might ask? Otherwise how would we update our Game as the data structure itself is “immutable”, i.e. cannot be changed, which would mean that we’d need to add a new Game object each time and save it on the stack (never cleaning it up) and that’d pretty fast make the application crash because of out of memory. . Finally, we update the game by calling our created update-method and then we use repaint() which draws the components! . Remember: paintComponent draws the canvas (game), so whenever repaint is called paintComponent draws the game again based on the game and cell’s in the game. . In conclusion this gives us an incredibly simple game, the snake moves whenever we press a key as we still don’t have a game loop based on time. So how do we add a game loop based on time? . The JVM got you covered! In the keyListener remove the update &amp; repaint, then add a timer . fixedRateTimer(TODO(&quot;Explore options to use for the timer and how they work&quot;)) { TODO(&quot;Insert a game loop here, essentially the same as done in the keyListener previously!&quot;) } . Run the game! …amazing right? We now move our snake, and it moves by itself if we don’t. But the game is still pretty boring… We never die, no apples can be eaten and finally no new apples appear. We have a few additions to make to make the game a bit challenging.. . Let’s start by fixing the apples. Update the Apples.kt to randomly add apples to the board when calling grow(). . To simplify the logic we use a set which means that all apples added are unique. . data class Apples( val width: Int, val height: Int, val cells: Set&lt;Cell&gt; = emptySet(), val growthSpeed: Int = 3, // this could actually be to only spawn apple when there is no other apple. Up to user val random: Random = Random // Once again, Kotlin provides a superb class, in this case a Random wrapper that works on JVM, JS &amp; Native - cool right? ) { fun grow(): Apples { return TODO(&quot;&quot;&quot; If we have a random number greater than growthSpeed, return no update. Otherwise add a new cell. &quot;&quot;&quot;) } } . Then we should allow the Snake to eat them, make sure to add eat(apples: Apples) method and implement it for Snake.kt. . fun eat(apples: Apples): Pair&lt;Snake, Apples&gt; { return TODO(&quot;&quot;&quot; If our head is on a Apple location, return a pair of Snake and Apple untouched. Otherwise make sure to remove the apple from apples and increase body size of snake! &quot;&quot;&quot;) } . At the end of all this we need the Game.kt to allow this logic to be used. This is done through updating update to allow the snake to eat apples and also grow apples to add new ones. . Great! We can eat apples, add new apples and all. But we’re still pretty invincible and we’ll just keep going forever. We need to make sure that the end can be lost, let’s do it by adding a new attribute isOver to Game.kt . val score: Int = TODO(&quot;Score based on snakes size, e.g. cell size&quot;) val isOver: Boolean = TODO(&quot;Game is over if snake head in tail or snake head not on the map!&quot;) fun update(dir: Direction): Game { if (isOver) return this { /** same code as was here before */ } } . Wrapping up the code with some minor refactoring / new functionality . Kotlin has a wonderful concept of extension functions, which simply is incredible. An extension function extends a class with new functionality. Did you ever wish Double had a rounding to string? fun Double.roundTo(n: Int): String = &quot;%.${decimals}f&quot;.format(this) solves this for you! Now your Double’s automatically gives you a hint to use .roundTo as one of Double’s built-in functions! . With these we can update our main-method to be a tiny bit cleaner. . g2d.color = Color.GREEN game.snake.tail.forEach { cell -&gt; g2d.fill(Rectangle(cell.x, cell.y, 1, 1)) } // Turns into --&gt; fun Graphics2D.renderCells(color: Color, cells: Iterable&lt;Cell&gt;) { this.color = color cells.forEach { cell -&gt; fill(Rectangle(cell.x, cell.y, 1, 1)) } } /** Which allows us to just call `g2d.renderCells(Color.GREEN, game.snake.tail)` etc. */ . What more improvements can be made? . Exercises left for the reader: . Add Score on the loosing screen | Add a win-condition (basically impossible, but taking all apples) | Reinforcement learning to train a bot (might be a future blog!) | Better &amp; cleaner code! | Part 3: True multiplatform (moving to JS &amp; Native) . First off, this part is more of a reader exercise. If you want the code please go to the GitHub repository. . All the snake-related code that isn’t in your main.kt-file should be moved into src/commonMain/kotlin which makes it multiplatform-code. This means that it can target JS, Native &amp; JVM instantly! . Side-note: because all the functionality for the Data Structures (e.g. take, List, Random) exists in Kotlin std-lib it’s automatically possible to use in multiplatform. . This is not true all the time, if we use platform-specific code. Our platform-specific code, tied with the JVM, is the timer and Swing which means that our whole GUI is tied to the JVM. . When the code has been migrated and import-paths are updated, run the JVM app again and validate that everything works. . JS/Browser target . Now create src/jsMain/kotlin/main.kt. . In this file we need to define how to draw the browser-based GUI. Some key methods, for the full code check out the git repository. . KeyListener document.onkeydown = { event -&gt; onkeydown(event).also { keyDir -&gt; dir = keyDir } } where onkeydown is your own method that handles key-events. . Timer window.setInterval({ game = game.update(dir); render(canvas, game) }, 200) . Canvas . val canvas = document.getElementById(&quot;snake-canvas&quot;) as HTMLCanvasElement val ctx = canvas.getContext(&quot;2d&quot;) as CanvasRenderingContext2D . On this ctx from the canvas you can use fillRect to draw rectangles, and fillStyle to set color. . HTML-Canvas &lt;canvas id=&quot;snake-canvas&quot; width=&quot;400px&quot; height=&quot;300px&quot;&gt;&lt;/canvas&gt; (put in index.html) . The game is run through ./gradlew jsBrowserRun, or selecting the gradle-icon at the top right (elephant) and typing jsBrowserRun. . And the code for the GUI using these components is pretty much exactly the same as in Swing to be honest. . Congratulations, you have now achieved creating a desktop game and a browser game! . Native target . And onto our final target, Native Binary, that runs completely without a browser or a virtual machine. . For the Native target the GUI will be supported through the library ncurses which unfortunately is only supported on Linux &amp; MacOS. If you’ve windows you can solve this through Windows Subsystem for Linux (WSL). . Begin by creating src/nativeMain/kotlin/main.kt. . To begin, in the main-function do the following: . fun maint(): Unit = memScoped { // insert code } . The memScoped part means that all memory allocated in the block is automatically disposed at the end, incredibly useful! :happy: . Then reading how to use ncurses we can figure out how to init this. The final end-goal being . fun main(): Unit = memScoped { initscr() defer { endwin() } noecho() curs_set(0) halfdelay(2) var game: Game = TODO() val window = newwin(game.height + 2, game.width + 2, 0, 0)!! defer { delwin(window) } var input = 0 while (input.toChar() != &#39;q&#39;) { window.draw(game) input = wgetch(window) val direction = when (input.toChar()) { &#39;i&#39; -&gt; TODO(&quot;&quot;) } game = game.update(direction) } } private fun CPointer&lt;WINDOW&gt;.draw(game: Game) { wclear(this) box(this, 0u, 0u) game.apples.cells.forEach { mvwprintw(this, it.y + 1, it.x + 1, &quot;.&quot;) } game.snake.tail.forEach { mvwprintw(this, it.y + 1, it.x + 1, &quot;o&quot;) } game.snake.head.let { mvwprintw(this, it.y + 1, it.x + 1, &quot;Q&quot;) } if (game.isOver) { mvwprintw(this, 0, 6, &quot;Game Over&quot;) mvwprintw(this, 1, 3, &quot;Your score is ${game.score}&quot;) } wrefresh(this) } . Try running it in the terminal. . This blog was created as a companion to a workshop I’m gonna do at AFRY, it has a bit more content including a presentation in person. . All the code is available here. . Thanks! . ~Hampus .",
            "url": "https://blog.londogard.com/gradle/kotlin/game/multiplatform/2020/11/07/snake-kotlin-multiplatform.html",
            "relUrl": "/gradle/kotlin/game/multiplatform/2020/11/07/snake-kotlin-multiplatform.html",
            "date": " • Nov 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "TIL: SDKMan - The Software Development Kit Manager",
            "content": "SDKMan - Swapping JDK made simple . I’ve decided to not only write blogs but also small snippets, here comes the first one. . What . SDKMan is a tool to install, set and swap JDK. SDKMan actually supports more than the Java JDK, among supported tooling is Java, Groovy, Scala, Kotlin and Ceylon. Ant, Gradle, Grails, Maven, SBT, Spark, Spring Boot, Vert.x and many others also supported. . It’s written in Bash, only requires curl &amp; zip/unzip. . So what SDKMan simplifies is . Installation of different JDKs, Gradle versions and so on | Swapping between JDKs | Allowing local (by folder basis) JDK-versions | How . We start by installation . Installation . If you need a more detailed guide go to this page. Downloading SDKMan $ curl -s &quot;https://get.sdkman.io &quot; | bash . Installing $ source &quot;$HOME/.sdkman/bin/sdkman-init.sh &quot; . Verification $sdk version - should return something along sdkman X.Y.Z . Usage . |What|Command|Comment| |—|—|—| |Install JDK|$sdk install java|Installs the latest stable version of Java JDK| |Install specific version|$sdk install scala 2.12.1|Install scala 2.12.1| |Install local version|$sdk install groovy 3.0.0-SNAPSHOT /path/to/groovy-3.0.0-SNAPSHOT|Installs a JDK you have locally to the SDKMan. The version name must be unique!| |Remove version|$sdk uninstall scala 2.11.6 |List candidates|$sdk list java|Lists all java candidates that are installable through SDKMan| |Use version|$sdk use scala 2.12.1|Use the version said, this only changes the current shell| |Default version|$sdk default scala 2.11.1|Changes version for all subsequent shells| |Current version|$sdk current|Lists all currently selected versions| . Remember to point your JDK to the ./sdkman/candidates/java/current path. Do the same for your IDE, such as IntelliJ-IDEA. . Why . I’ve got different projects where I need to use different java versions. In one project I need JDK 14 to include jpackage and another one I’m forced to use JDK 8 (legacy system), to swap between these has never been simpler! . Alternatives . jEnv is a great alternative. According to some more JDK versions exists (haven’t checked myself), but overall it seems that SDKMan is the preferred alternative. Looking at GitHub one can clearly see that SDKMan is more popular, both by stars, latest commit and forks - which should be a decent enough to make a choice. . One thing I’ve learned both through work and my personal projects is that often it’s better to make an non-optimal decision rather than trying to find the perfect solution, because diving into the pile of research to perfection will take much more time than just getting started. . -Hampus Londögård .",
            "url": "https://blog.londogard.com/jvm/jdk/til/2020/09/04/til-sdkman.html",
            "relUrl": "/jvm/jdk/til/2020/09/04/til-sdkman.html",
            "date": " • Sep 4, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "TIL: fastutil - fast & compact type-speciic collections for JVM (no autobox!)",
            "content": "fastutil - how to optimize your collections by primitives . fastutil extends the Java™ Collections Framework by providing type-specific maps, sets, lists and queues with a small memory footprint and fast access and insertion . Homepage of fastutil . What . So what does the quote above actually mean? First we need to dive into, what is a Java Collection, and why are they &quot;bad&quot; for performance and memory requirements? . Java Collections (Collection&lt;E&gt;) only works with objects, meaning that if we have a List&lt;E&gt; which we populate with int it’ll actually &quot;autobox&quot; the int into a Integer, i.e. the class, rather than the primitive type. What does this mean for you as a user? . It’s done through &quot;autoboxing&quot; which means automatic casting to the Integer-type, so nothing required for you | It allocates more memory than the primitive int. | . So how do you create an effective List that contains primitives, such as int, boolean and float? You can’t. What you can do is to create an array, int[], which will contain the actual primitives, no autoboxing applied. . But what if you want to have the methods from List, such as find and &quot;auto-resizing&quot;? Then you’ll have to research and find a library, fastutil to the rescue! . How . fastutil implements their own versions of List, HashMap and so on which actually use the raw primitives, thereby increasing throughput while lowering memory used (as we’re not allocating as many objects anymore, when using primitives). . These types of libraries are only required once you hit an enormous amount of data or very strict requirement. . Installation . Using gradle . implementation: &#39;it.unimi.dsi:fastutil:8.4.1&#39; (latest version as of Aug 2020, mvnrepository) . Usage . DoubleToDoubleMap . val d2dMap: Double2DoubleMap = Double2DoubleOpenHashMap().apply { put(2.0, 5.5) put(3.0, 6.6) } assertEquals(5.5, d2dMap.get(2.0)) . This map is not only less memory-hungry (because using double rather than Double) but is also faster with insertions &amp; get, than the Java Collections counterpart. . Why . Less space used &amp; faster - it is as simple as that! . No &quot;AutoBoxing&quot; | No Object allocations for primitives | . Side-note Something I noticed while working on my Language Model in Kotlin, with some strict requirements and a lot of data, was that even when using fastutil I wasn’t gaining that much as I was mainly using views of my Lists, further optimizing memory. Views are what the name implies, a view of the List. It never creates a copy but just the indices and make use of the original structure. Using immutable data this is very effective, but if you’d been using mutable data it could prove dangerous as someone can change the structure and data of your view (even if your view is immutable the underlying List might not be). . Alternatives . Goldman Sachs Collection - now Eclipse Collections - Probably the best alternative, in my opinion. HPPC - Carrot Search Labs Trove4j - Not as active as other alternatives, but who cares when it’s performant and &quot;done&quot;? . Find a 2015 benchmark of the libraries here At least both fastutil&amp; Eclipse Collections are updated for Java 8 streams! . -Hampus Londögård .",
            "url": "https://blog.londogard.com/jvm/til/optimization/2020/09/03/til-fastutil-primitive-structures.html",
            "relUrl": "/jvm/til/optimization/2020/09/03/til-fastutil-primitive-structures.html",
            "date": " • Sep 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "TIL: 'The Badass Runtime Plugin', jpackage & jlink - create a 'native' installable executable from your JVM-app that isn't huge",
            "content": "JPackage, JLink and how to pack a modern Java App . JPackage is a way to package a modern JVM-program as a installable binary, in a small format. . What . JPackage was finally included in the JDK by JDK-14, originally from the JavaFX-world (to bundle your desktop apps). JPackage combines itself with JLink, which builds upon ‘project jigsaw’, and together they form a way to create &quot;native&quot; binaries for JVM-projects. . What is JLink? . JLink is a way to assemble and optimize a set of modules and their dependencies into a custom runtime image (JRE). In other words we can take a ordinary JRE, ~200 MB, and chop it down to a total size of 25-40 MB for smaller project. . JLink is only possible thanks to ‘project jigsaw’ which introduced modules and modularized the whole JRE starting from JDK-9. The Java standard library (stdlib) was modularized into 75 modules. As you might guess it is even better if your own code is also modularized, but not enforced. . What is JPackage . JPackage is the packaging suite that allows you to package your code, dependencies and the JLink-created JRE. I ended up with installation files, with a natively executable file on 60 MB for one of my smaller projects, which is really good in comparison to Electron! In comparison to a C-program this might not be amazing, but you’ve to remember that this is completely cross-platform! . Side-note all sized discussed is without any major optimizations - and there exists a lot! Finally, if you exclude the JRE you can reach sizes of KB rather than MB! But excluding the JRE enforces the user to have it locally, which might not be good UX. . How . JPackage &amp; JLink is made easy thanks to The Badass Runtime Plugin or The Badass JLink Plugin where the latter require a modular project and the former works with any project! :happy: . Installation . Make sure you use &amp; target JDK 14 or higher, JPackage was first included in this version. I recommend SDKMan to install &amp; swap JDKs. . Then to add the Badass Runtime Plugin I recommend using gradle, which makes it as simple as the following. . plugins { ... id( &quot;org.beryx.runtime &quot;) version &quot;1.11.3 &quot; // latest version August 2020 ... } runtime { options.set(listOf( &quot;--strip-debug &quot;, &quot;--compress &quot;, &quot;2 &quot;, &quot;--no-header-files &quot;, &quot;--no-man-pages &quot;)) jpackage { installerType = &quot;deb &quot; // https://badass-runtime-plugin.beryx.org/releases/latest/ } } . This addition now creates the tasks required to build &amp; bundle your app. The options added make sure that you reduce the total size by a lot. I highly recommend reading the documentation, there’s so many incredibly useful options - I only provide the minimum! . Usage . By editing our building.gradle.kts to include everything from the Installation we can run the ./gradlew jpackage task to build our installer! . I want to note again, please make sure to read the homepage - a ton of optimizations and customization exist. There exists a lot of low hanging fruit for sure, so make sure to grab it! :wink: . Why . It’s really cool to see your JVM application installable using a .msi, .deb or even a .dmg while retaining a decent enough size. By using JPackage rather than GraalVM you make sure that you don’t loose anything in the form of performance or functionality. As a cherry on the top, it’s not just a executable file, but also includes a installer which is much better UX in my opinion. GraalVM will be discussed a bit more in Alternatives. . I want to re-iterate about the UX and size, which are the two main points of this. . We bundle a JRE with the JVM-app, allowing executables without requiring Java, of your version, to be installed on the user computer already. | The JRE is minified to only contain required modules, about 30-40 MB on a smaller project. | All required dependencies are bundled also | Installer which makes the whole JVM program really like any program on the computer | Basically a download, install run program that isn’t huge in size! | Alternatives . I see two alternatives that are worth mentioning . FAT-JAR / Uber-JAR / Shadow-JAR | GraalVM Native Image | &quot;Fat-JAR&quot; . A FAT-jar is a jar that bundles all dependencies and also includes a shell script, or .bat if Windows, to run the whole JVM-application. It’s pretty small in size, even though called FAT, as it doesn’t include a JRE to run the JVM. . This means that if your JVM-app requires Java 11 but the user only has Java 8 you need to have them download the JRE required, which sucks. . #### GraalVM The probably best alternative, it’s even smaller in size as SubstrateVM (their runtime) is really small and GraalVM allows AOT compile. . GraalVM has much faster startup-times than a JPackage program, but GraalVM is not as good when running for a long duration as there isn’t the incredibly good JIT from JVM. . I’d say something along the following - for long running apps choose JPackage, for lambda etc certainly choose GraalVM. . But GraalVM has further negatives, you can’t just code as you usually do. Reflection etc is not supported as usual, meaning there comes a lot of caveats using GraalVM. . Extra: I managed to end up with, after some minor trial-and-error, a binary file on ~ 12 MB for my file-sending program - pretty darn amazing! . I’ll write more about GraalVM and its SubstrateVM which is used to create the native binaries in a new TIL. . -Hampus Londögård .",
            "url": "https://blog.londogard.com/jvm/jdk/til/jpackage/jlink/2020/09/03/til-badass-runtime.html",
            "relUrl": "/jvm/jdk/til/jpackage/jlink/2020/09/03/til-badass-runtime.html",
            "date": " • Sep 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "CoViD-19 FAQ Search Engine 2.0",
            "content": "CoViD-19 FAQ Search Engine 2.0 . (Open in Google Colab here to run the code) . As promised here&#39;s a new improved (or is it?) FAQ Search Engine with some minor NLP-lessons added as we go, be ready to learn new (or old) things! Previously I added some requirements and I wish keep them, here they are as a refresher: . The end-product must be unsupervised No manually annotated data | No heuristic applied (i.e. understand the data and improve result by applying domain-specific knowledge on the task) | . | It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end) | Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!) | . These specifications adds a bit of spice, keep manual labour to a minimum at the same time as they prove a challenge that doesn&#39;t aim to achieve State of the Art but rather to be applicable and light! . With that in mind, let&#39;s move onwards! . Improvements to be done . In the previous blog &amp; notebook I first implemented a basic FAQ search based on finding the nearest neighbour from the embedded sentences, in the end I used Smooth Inverse Frequency Embeddings (A Simple but Tough-to-Beat Baseline for Sentence Embeddings) to embed the sentence which is an improvement from simply averaging the embeddings of the words in the sentence. . In the end I discussed some potential improvements which I wished to investigate. In this notebook I&#39;ll deliver these &quot;improvements&quot; based on grabbing some low hanging fruit. The total &quot;improvements&quot; to try out: . Lowercase | Better tokenization | Lemmatizing | Stop words | Ngram &amp; Custom Embeddings (will not be done because of time) | . To improve further I&#39;d say that either A) a lot of time to understand the data in depth and apply heuristics or B) a supervised approach, which in turn require labeled data (a.k.a sweet valued time). A larger dataset would also be helpful. All which I don&#39;t have currently. . Re-adding the old code . First I&#39;ll add the code from &quot;part one&quot; and it&#39;ll not be commented as it has been walked through. Further I&#39;ve removed the download &amp; parsing of FAQ, now the data is directly downloaded as a tsv-file allowing us to skip some libraries / code-cells. Some new dependencies are also added, e.g. stanza which is Stanfords new NLP-lib in Python (inspired by spaCy). . %%capture !pip install -U gensim !pip install -U fse !pip install stanza !pip install stop-words . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from tqdm import tqdm import matplotlib.pyplot as plt tqdm.pandas() from pathlib import Path import os import random import operator import regex as re # gensim + fasttext from gensim.models.fasttext import FastText, load_facebook_vectors from gensim.models import KeyedVectors from stop_words import get_stop_words # stop-words from basically all languages import stanza from fse import IndexedList from fse.models import uSIF from fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH print(MAX_WORDS_IN_BATCH) print(FAST_VERSION) . 10000 1 . stanza.download(&#39;sv&#39;, logging_level=&#39;ERROR&#39;) print(&quot;OBS!! nPlease download the Swe fastText model &amp; the CoViD FAQ data from links in this code cell!&quot;) # Swe fastText reduced dimensions --&gt; https://drive.google.com/open?id=1vaWtiSlRAZ3XCdtnSce_6dwQ0T5x0OEJ # CoViD FAQ data --&gt; https://github.com/londogard/nlp-projects/blob/master/datasets/covid.tsv . OBS!! Please download the Swe fastText model &amp; the CoViD FAQ data from links in this code cell! . Loading all the models . This might take a little while, even though the dimensions are reduced the model is pretty large. . ft_wv = load_facebook_vectors(&#39;~/git/nlp-projects/models/cc.sv.100.bin&#39;) df = pd.read_csv(&#39;~/git/nlp-projects/datasets/covid.tsv&#39;, sep=&#39; t&#39;) nlp = stanza.Pipeline(lang=&#39;sv&#39;, processors=&#39;tokenize&#39;, logging_level=&#39;ERROR&#39;) model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) flatten = lambda l: [item for sublist in l for item in sublist] # Helper function to flatten a list . Going forward . Let&#39;s get on to adding our improvements . 1. Tokenization &amp; lower-case . The first and forthmost improvement is to lowercase the text and then tokenize it using a better method of tokenization. Let&#39;s take a look at how stanza helps us out by applying a much better tokenization. . q = &quot;Hej där borta! Jag känner igen dig, Johan&#39;s kompis? Eller är det Johannas?&quot; stanza_tokenize = lambda x: [token.text for sentence in nlp(x).sentences for token in sentence.tokens] prev = q.split() new = stanza_tokenize(q) print(f&quot;Previously: t{prev[:12]}..&quot;) print(f&quot;After: t t{new[:12]}..&quot;) . Previously: [&#39;Hej&#39;, &#39;där&#39;, &#39;borta!&#39;, &#39;Jag&#39;, &#39;känner&#39;, &#39;igen&#39;, &#39;dig,&#39;, &#34;Johan&#39;s&#34;, &#39;kompis?&#39;, &#39;Eller&#39;, &#39;är&#39;, &#39;det&#39;].. After: [&#39;Hej&#39;, &#39;där&#39;, &#39;borta&#39;, &#39;!&#39;, &#39;Jag&#39;, &#39;känner&#39;, &#39;igen&#39;, &#39;dig&#39;, &#39;,&#39;, &#39;Johan&#39;, &#34;&#39;&#34;, &#39;s&#39;].. . So, what are we looking at? Stanza handled our tokenization and increased the number of tokens, can this really be good!? Yes! Keep calm and don&#39;t jump the ship yet, the increased number of tokens will be followed by a decrease of unique tokens, and indirectly out of vocobulary (OOV) tokens. Unlike what we set out to do we still don&#39;t lower-case the output, this will follow later, now let me explain what the tokenization helps us achieve: . Punctuation, e.g. [!,?..], is tokenized into its own token. | Some compound words are split up, e.g. Johan&#39;s is now Johan, &#39;, s which is three (3) separate tokens rather than one. | Because of the updated tokenization fredag and fredag! is now tokenized as [fredag] and [fredag, !], this in fact turns fredag into the same token in both thus achieving the same vector when embedded which is great, because it really means the same. The exclamation mark itself also applies the same meaning to all places it&#39;s applied, which in itself is an improvement now also as we embed it separately. . Why is this good? Even though we see a direct increase in number of tokens we see a decrease of number of unique tokens because we now tokenize borta, borta?, &amp; borta! as the same token, with one additional for the punctuation in the two latter cases rather than 3 separate tokens which would map to different data. The coverage of our Word Embeddings also increase because we now tokenize the text better. Perhaps borta! does not exist but borta surely do exist in the embedding dictionary / lookup. . def test_dimensions(preprocessing=[stanza_tokenize]): prev = flatten(df[&#39;question&#39;].apply(lambda x: x.split()).tolist()) post = flatten(df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocessing)).tolist()) print(f&quot;Previously: {len(prev)} tokens ({len(set(prev))} unique)&quot;) print(f&quot;Post: {len(post)} tokens ({len(set(post))} unique)&quot;) print(f&quot;Token reduction by ~{100 * (1- len(set(post))/len(set(prev))):.1f} %&quot;) labels = [&#39;#Tokens&#39;, &#39;#Unique Tokens&#39;] width = 0.35 x = np.arange(len(labels)) fig, ax = plt.subplots() rects1 = ax.bar(x - width/2, [len(prev), len(set(prev))], width, label=&#39;Before&#39;) rects2 = ax.bar(x + width/2, [len(post), len(set(post))], width, label=&#39;After&#39;) ax.set_ylabel(&#39;Tokens&#39;) ax.set_title(&#39;Tokens before and after&#39;) ax.set_xticklabels(labels) ax.set_xticks(x) ax.legend() fig.tight_layout() plt.show() # preprocessing is a list of lambda functions to apply def preprocess(text, preprocessing): for f in preprocessing: text = f(text) return text . Let&#39;s take a look how much this actually mattered! . test_dimensions() . Previously: 629 tokens (289 unique) Post: 713 tokens (273 unique) Token reduction by ~5.5 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; The expectations set up has been achieved and we can clearly see that the raw number of tokens grew while the unique token count shrinked. Applying lower-case to the text will further reduce the number of unique tokens, and obviously keep the number of tokens at the same count. . Let&#39;s add lower-casing and see what happens! . lowercase = lambda x: x.lower() preprocess_funcs = [lowercase, stanza_tokenize] test_dimensions(preprocessing=preprocess_funcs) . Previously: 629 tokens (289 unique) Post: 712 tokens (260 unique) Token reduction by ~10.0 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Lower-casing . Going from 5.5 to 10 % reduction is nothing to sneeze at, by applying these two simple techniques we now have the same data in a better format which allows us to have a lower number of unique tokens. Pretty awesome right? . Let&#39;s get on with this and apply the preprocessing to the questions and test it out with the FAQ-search! . df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocess_funcs)) df[&#39;X&#39;].head() . 0 [vad, är, coronavirus, ?] 1 [vad, är, covid-19, ?] 2 [vad, skiljer, covid-19, från, säsongsinfluens... 3 [vilka, är, symtomen, på, covid-19, ?] 4 [hur, vet, jag, om, mina, symtom, beror, på, p... Name: X, dtype: object . Testing the new input-data . Now that we&#39;ve created our input data we need to test our model on this! By applying the IndexedList which is the dataformat SFE wants as input we can train the model and then test it. . sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) . (75, 712) . def get_n_closest_questions(question, preprocessing, n=4): q_fixed = preprocess(question, preprocessing) resp = model.sv.similar_by_sentence(q_fixed, model=model, indexable=df[&#39;question&#39;].tolist()) # [([tokens], score)] resp = [f&#39;{result[2]:.2f}: {result[0]}&#39; for result in resp] print(&#39; n&#39;.join(resp[:n])) . get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocess_funcs) . 0.67: Kan man bli smittad av en person som har covid-19 men som inte har några symtom? 0.63: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset? 0.54: Kan viruset smitta till människa via post och paket? 0.42: Kan smitta överföras från mygg till människa? . get_n_closest_questions(&quot;Hur får jag min son att förstå?&quot;, preprocess_funcs) . 0.82: Hur pratar man med barn om det nya coronaviruset? 0.80: Vad är covid-19? 0.78: Hur sjuk blir man av covid-19? 0.77: Hur länge är man sjuk av covid-19? . 2. Lemmatization and Stop Words . Let&#39;s try to further improve this by actually lemmatizing and applying stop-words! . Lemmatization . So what is Lemmatization? Quoting Stanfords description: . For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. . The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: am, are, is =&gt; be car, cars, car&#39;s, cars&#39; =&gt; car . The result of this mapping of text will be something like: the boy&#39;s cars are different colors =&gt; the boy car be differ color . What is stop-words? . Stop-words are words we want to throw away as they add no real purpose. In older Machine Learning approaches it was way more important to add stop-words but in newer Deep Learning with Neural Networks stop-words often can be a negative thing, removing understanding of the sentence and perhaps minor differences which makes the world for understanding. . A example of a stop-word list could be [&quot;hej&quot;, &quot;vem&quot;, &quot;då&quot;, &quot;och&quot;, ...] which means that these words would be removed from a sentence. . In our case it makes sense to remove words like &#39;vad&#39;, &#39;varför&#39; and so on because the return of the FAQ seems to be very weighted towards these words. . nlp = stanza.Pipeline(lang=&#39;sv&#39;, processors=&#39;tokenize,mwt,pos,lemma&#39;, logging_level=&#39;ERROR&#39;) stanza_lemma = lambda x: [token.lemma for sentence in nlp(x).sentences for token in sentence.words] preprocess_funcs_lemma = [lowercase, stanza_lemma] print(f&#39;Previously: t{preprocess(&quot;hur förklarar jag för min dotter och son?&quot;, preprocess_funcs)}&#39;) print(f&#39;After: t t{preprocess(&quot;hur förklarar jag för min dotter och son?&quot;, preprocess_funcs_lemma)}&#39;) . Previously: [&#39;hur&#39;, &#39;förklarar&#39;, &#39;jag&#39;, &#39;för&#39;, &#39;min&#39;, &#39;dotter&#39;, &#39;och&#39;, &#39;son&#39;, &#39;?&#39;] After: [&#39;hur&#39;, &#39;förklara&#39;, &#39;jag&#39;, &#39;för&#39;, &#39;jag&#39;, &#39;dotter&#39;, &#39;och&#39;, &#39;son&#39;, &#39;?&#39;] . Some interesting notes Seeing &#39;min&#39; getting converted to &#39;jag&#39; is both good and bad, in this case we reduce dimensionality of the problem but we loose context and understanding. jag and min certainly does not mean the same thing. . Let&#39;s see how it pans out... . test_dimensions(preprocess_funcs_lemma) . Previously: 629 tokens (289 unique) Post: 712 tokens (228 unique) Token reduction by ~21.1 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; del model model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocess_funcs_lemma)) sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) . (75, 712) . get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocess_funcs_lemma) . 0.75: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset? 0.69: Hur smittar covid-19? 0.68: Kan man smittas flera gånger av det nya coronaviruset? 0.63: Smittar covid-19 via vatten och mat? . get_n_closest_questions(&quot;Hur får jag min son att förstå?&quot;, preprocess_funcs_lemma) . 0.79: Vad är covid-19? 0.75: Hur sjuk blir man av covid-19? 0.74: Hur länge är man sjuk av covid-19? 0.66: Om en person i familjen är sjuk - måste alla stanna hemma då? . Analyzing the results . Improvements? Not really, the model has an improved response to the &#39;hamster-question&#39; but it&#39;s way off when asking about the son. . Why? The most likely explanation is that even though we reduce the input dimensions an awful lot we remove dimensions that brings value, and removing value is bad - just as was touched upon previously. It might be helpful in some cases, perhaps this could prove helpful for a supervised approach such as TF-IDF + Support Vector Machine. . Any good parts? Yes, we can see some pretty hefty memory-requirement reductions when working with other types of models by applying this. Actually, in the case of this we could reduce the memory requirement by lemmatizing the dictionary of the embeddings and removing all non-lemmas. All in all, this could lead to a small performance loss but great memory win. . Stop words . As promised we shall apply stop-words, but as we saw no performance gain with lemmatization we&#39;ll keep the old tokenization. . stop_words = get_stop_words(&#39;sv&#39;) clean_stop = lambda x: [word for word in x if word not in stop_words] preprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop] del model model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don&#39;t need to remove stop-words in the sentences in our sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) preprocess(&quot;hur förklarar jag för min dotter och son?&quot;, preprocessing_func_stop) . [&#39;förklarar&#39;, &#39;dotter&#39;, &#39;son&#39;, &#39;?&#39;] . test_dimensions(preprocessing_func_stop) . Previously: 629 tokens (289 unique) Post: 417 tokens (206 unique) Token reduction by ~28.7 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocessing_func_stop) . 0.66: Kan man bli smittad av en person som har covid-19 men som inte har några symtom? 0.64: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset? 0.54: Kan viruset smitta till människa via post och paket? 0.41: Kan smitta överföras från mygg till människa? . get_n_closest_questions(&quot;Hur får jag min son att förstå?&quot;, preprocessing_func_stop) . 0.83: Vad är covid-19? 0.83: Hur pratar man med barn om det nya coronaviruset? 0.80: Hur sjuk blir man av covid-19? 0.80: Hur länge är man sjuk av covid-19? . Further analyzing . In my mind we&#39;ve some pretty good responses, in a way better and another way worse than lemmatizaton. Certainly not a set-back but neither a step forward. Testing different approaches and turning things on and off is a great way to increase data understanding and also gives a better sense of what different preprocessing functions actually does. In fact this is actually part of the most common Machine Learning development approach, working much like agile, which is iteratively circular and called CRISP-DM. I won&#39;t go deeply into CRISP-DM (already did once in my Master Thesis), but the following image gives you the gist. . Finally, as we see no great impact by applying either lemmatization nor stop-words we might just give up at the lower-case + stanza tokenization, but I&#39;d like to make one last shot in the dark - custom stop words! Let&#39;s see how it fares... . Custom Stop Words (breaking the rules) . So I decided to break the rules and create a small simple heuristic by applying custom stop words. Let&#39;s figure out which words we should remove using the following steps (which could in fact be automated)! . Find the most common words | Remove the ones which does not give any greater value | from collections import Counter df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocess_funcs)) counter = Counter(flatten(df[&#39;X&#39;].tolist())) . sorted(counter.items(), key=lambda item: item[1], reverse=True)[:15] . [(&#39;?&#39;, 75), (&#39;covid-19&#39;, 28), (&#39;vad&#39;, 25), (&#39;och&#39;, 22), (&#39;hur&#39;, 21), (&#39;för&#39;, 20), (&#39;det&#39;, 15), (&#39;kan&#39;, 14), (&#39;i&#39;, 14), (&#39;jag&#39;, 13), (&#39;av&#39;, 13), (&#39;gäller&#39;, 12), (&#39;som&#39;, 12), (&#39;är&#39;, 11), (&#39;en&#39;, 11)] . stop_words = [&#39;?&#39;, &#39;och&#39;, &#39;jag&#39;, &#39;i&#39;, &#39;är&#39;, &#39;en&#39;, &#39;min&#39;, &#39;?&#39;] clean_stop = lambda x: [word for word in x if word not in stop_words] preprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop] del model model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don&#39;t need to remove stop-words in the sentences in our sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) preprocess(&quot;hur förklarar jag för min dotter och son?&quot;, preprocessing_func_stop) . [&#39;hur&#39;, &#39;förklarar&#39;, &#39;för&#39;, &#39;dotter&#39;, &#39;son&#39;] . get_n_closest_questions(&quot;Hur får jag min son att förstå?&quot;, preprocessing=preprocess_funcs) . 0.83: Hur pratar man med barn om det nya coronaviruset? 0.83: Vad är covid-19? 0.80: Hur sjuk blir man av covid-19? 0.79: Hur länge är man sjuk av covid-19? . get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocessing=preprocess_funcs) . 0.66: Kan man bli smittad av en person som har covid-19 men som inte har några symtom? 0.63: Kan covid-19 smitta mellan djur och människa och kan mitt husdjur smittas av viruset? 0.54: Kan viruset smitta till människa via post och paket? 0.41: Kan smitta överföras från mygg till människa? . Not bad, not amazing - I feel pretty happy about this. . So what can be done from now on if time and resources where available? . Add a classifier + TF-IDF | BERT / ALBERT QA (the State-of-the-Art right now) | . Thanks for this time, - Hampus Londögård .",
            "url": "https://blog.londogard.com/jupyter/nlp/machine-learning/deep-learning/2020/08/01/faq-search-covid-2.html",
            "relUrl": "/jupyter/nlp/machine-learning/deep-learning/2020/08/01/faq-search-covid-2.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "SQL - Different Abstraction Levels (& how I came to love SQLDelight)",
            "content": "SQL - different abstraction levels and how I came to love SQLDelight . In this blog I’ll cover a few different abstraction levels of database access, focusing purely on SQL and not NoSQL / Reddis or anything like that. The purpose is to share the knowledge that there exist these types of abstractions and they do exist in all or at least most of the popular languages. . I’ll try to move from &quot;raw SQL&quot; to the modern &quot;Object-Relational Mapping&quot;-style, a.k.a ORM. . In the end I wish to make a short piece leaving out a lot of details but maintaining a feel of each style and some pros/cons. I bet you already guessed my preferred approach straight from the title :wink:. . How to interact with a SQL Database from a programming language . Structured Query Language (SQL) is as the name, once spelled out, a Domain Specific Language (DSL) just like regex. It’s basically a programming language written to facilitate and simplify the experience with the underlying engine. By using a DSL you gain capabilities that would be natural to integrate with most languages, and it also makes the engine do the same with the same code across languages. . I think that Regex and SQL are the most famous DSLs and for good reason, having regex work (almost) the same across languages simplifies the guides and the same applies to SQL. . Going forward let’s see how we communicate with a SQL-db from a programming language like Java using their famous jdbc (Java Database Connectivity) which is the driver that communicates with the db. . try { System.out.println( &quot;Connecting to database... &quot;); conn = DriverManager.getConnection(DB_URL,USER,PASS); //STEP 4: Execute a query System.out.println( &quot;Creating statement... &quot;); stmt = conn.createStatement(); String sql; sql = &quot;SELECT id, first, last, age FROM Employees &quot;; ResultSet rs = stmt.executeQuery(sql); //STEP 5: Extract data from result set while(rs.next()){ //Retrieve by column name int id = rs.getInt( &quot;id &quot;); int age = rs.getInt( &quot;age &quot;); String first = rs.getString( &quot;first &quot;); String last = rs.getString( &quot;last &quot;); //Display values System.out.print( &quot;ID: &quot; + id); System.out.print( &quot;, Age: &quot; + age); System.out.print( &quot;, First: &quot; + first); System.out.println( &quot;, Last: &quot; + last); } //STEP 6: Clean-up environment rs.close(); stmt.close(); conn.close(); } catch (SQLException se) { .... . Not very convenient right? Personally I think this looks horrible, it’s filled with horrible getters &amp; setters like we’re stuck in the Middle Ages or something. Personally my mind directly flows to serialization and how that must work somehow with databases, and that’s right - we can move into the future today! . Moving one abstraction level up . Welcome Room &amp; slick (two libraries I’ve experience with) to the room! Both of these libraries provide a type of serialization to classes and more convenient syntax to write the code. The first one heavily leans on annotation to make it work while the other one uses a more slick approach of &quot;copying&quot; the way you work with the standard Scala Collections (filter, map, flatMap, reduce etc). . I’d say that both do count as ORMs but they’re still not as abstract as other solutions such as peewee which we’ll discuss later. Let’s get into Room and how it works. First you define entities like a class with the added annotation @Entity and then you define a Data Access Object (DAO) to interact with the table / object. The DAO is where you define your queries, let’s take a look. . @Dao interface UserDao { @Query( &quot;SELECT * FROM user &quot;) fun getAll(): List&lt;User&gt; @Query( &quot;SELECT * FROM user WHERE uid IN (:userIds) &quot;) fun loadAllByIds(userIds: IntArray): List&lt;User&gt; ... } . In my opinion this approach strikes a really good balance between simple-to-use but still powerful and very configurable because you still use SQL, a bonus here is that it’s safe from SQL-injection as you’re making use of so-called prepared-statements (wikipedia). The biggest drawback is that it’s hard to write easy-to-read SQL in the annotation and for the annotation-haters we’ve a lot of annotations (which often slows down the compile-time noticeably among other things). . Moving on we’ve slick which is also a really cool approach! slick allows you to this but instead you write your queries in something that feels like using the normal Scala Collection library. This allows you to use map, filter, reduce etc to create queries, and even for-comprehension. Let’s see! . // Read all coffees and print them to the console println( &quot;Coffees: &quot;) db.run(coffees.result).map(_.foreach { case (name, supID, price, sales, total) =&gt; println( &quot; &quot; + name + &quot; t &quot; + supID + &quot; t &quot; + price + &quot; t &quot; + sales + &quot; t &quot; + total) }) // Read coffee with price lower than 9 and join with matching supplier using for-comprehension val q2 = for { c &lt;- coffees if c.price &lt; 9.0 s &lt;- suppliers if s.id === c.supID } yield (c.name, s.name) // A find using filter def find(id: Int) = db.run( users .filter(_.id === id) .result.headOption ) . Pretty slick right? . Moving another level up (Python + Peewee) . Ok, maybe it’s not actually moving one level up from slick but I’d say it’s still a little bit further away from raw SQL as we make more use of objects, in the case of slick you can more easily see the generated SQL-code. Let’s take a look at peewee which supports most databases (sqlite, mysql, postgresql and cockroachdb). . So where do we begin? Create the database and tables! It’s done by initiating a database and then creating different classes which each maps to their own tables automatically. . db = SqliteDatabase(&#39;people.db&#39;) # create the db class Person(Model): name = CharField() birthday = DateField() class Meta: database = db # This model uses the &quot;people.db &quot; database. class Pet(Model): owner = ForeignKeyField(Person, backref=&#39;pets&#39;) name = CharField() animal_type = CharField() class Meta: database = db # this model uses the &quot;people.db &quot; database . And how would one create entries and then query them? It’s simply done through object creation as in the following examples. . uncle_bob = Person(name=&#39;Bob&#39;, birthday=date(1960, 1, 15)) uncle_bob.save() # Sometimes the class already has a &quot;create method &quot; as in Person.create(name=&#39;Sarah&#39;, birthday=date(1980, 10, 20)) # And create a pet which belongs to uncle_bob bob_dog = Pet.create(owner=uncle_bob, name=&#39;Doggy&#39;, animal_type=&#39;dog&#39;) . And to query the tables we also make use of the object fully, as in the following small example. . bobby = Person.select().where(Person.name == &#39;Bob&#39;).get() # or all persons! for person in Person.select(): print(person.name) . Now we’ve gone through the different abstraction layers that you usually see available in most languages. Going forward I’d like to show SQLDelight which turns the abstraction a little bit upside down. . SQLDelight: Abstraction level left to the right . In SQLDelight I’d say we get the ideal balance of abstraction and configurability. We deal with raw SQL which is both a pro &amp; con, people will need to know SQL unlike in a abstracted ORM but you also get the full potential and it’s really simple to do complex joins (which is really messy in ORMs). . I was delighted at how simple it was to use from my Kotlin code while also providing a simple way to write my DB-interactions. No confusion and there’s a million guides out there showing how you write SQL code for complex joins if you ever need a hand. . Let’s begin with how you define a table and queries, through a so-called .sq-file. . -- .sq-file CREATE TABLE person ( name TEXT NOT NULL, birthday DATE NOT NULL ); -- You can actually also insert a Person directly in this file if you&#39;d like using the normal SQL insert statement. selectAll: SELECT * FROM person; insert: INSERT INTO person(name, birthday) VALUES (?, ?); insertPerson: INSERT INTO person(name, birthday) VALUES ?; . For those that don’t know SQL this does the following . Define the table | Create queries on the table These queries makes use of the custom format methodName: and then define the method using the SQL code beneath until it hits end ; . | | Now we have some SQL code defined in a .sq-file, how do we actually use this from our Kotlin-code? We build the project, while building the project the code is generated to our build project with the Kotlin-code. It’ll provide . Data Classes (like structs / objects / case classes) | Queries for each table | . And on top of this you’ll have full typing, which is pretty damn awesome! Let’s take a look at how we’d use this from Kotlin. . // Not optimal code, should use injection or something in reality for the db. val database = Database(driver) val personQueries: PersonQueries = database.personQueries println(personQueries.selectAll().executeAsList()) // Prints [] personQueries.insert(name = &quot;Bob &quot;, birthday = Date(2010, 1, 10)) println(personQueries.selectAll().executeAsList()) // Prints [Person.Impl( &quot;Bob &quot;, Date(2010, 1, 10))] val person = Person( &quot;Ronald McDonald &quot;, Date(2020, 1, 5)) personQueries.insertPerson(person) println(personQueries.selectAll().executeAsList()) // Prints [Person.Impl( &quot;Bob &quot;, Date(2010, 1, 10)), Person.Impl( &quot;Ronald McDonald &quot;, Date(2020, 1, 5))] . Let me just say, I’m amazed about this kind of reverse thinking of generating code from SQL. It gives us the convenience of a ORM but the flexibility of raw SQL :happy:. . Comparison Table . Database Simplicity Requires SQL knowledge Configurability (complex queries etc) Score (5) Comment . JDBC | I | III | III | 2 | To much overhead | . Room / Slick | II | II | II | 4 | Strikes a good balance between natural in normal code while configurable* | . Peewee | III | I | I | 3 | Really easy and fits into code great, but the complex queries becomes really hard and feels forced | . SQLDelight | II | III | III | 5 | Natural to use in the code, great customability &amp; little overhead* | . Both Room &amp; SQLDelight are enforcing SQLite right now which is a major con for those that needs postgresql etc. Personally I only use SQLite as was discussed in expensify’s blog SQLite can be squeezed to the extreme - expensify managed to handle up to 4 million queries per second! . Outro . In its essence today there’s a great variety of different kinds of wrappers for databases in almost all languages and it is all about finding one that strikes your balance of perfect. For a really simple database perhaps an ORM such as peewee where no SQL knowledge is really required could be enough. But be sure to know the trade-offs, once your database grows complex so does peewee grow complex fast, same applies to slick and others. Raw SQL as a fall-back is always good to have and a lot of the libraries are starting to add it (e.g. slick), but it never feels natural and always is a bit like a bandaid, ugly right? . Anyhow, I hope this was interesting and perhaps someone learned about a new abstraction-level for databases or was inspired to pick up their own. . ~Hampus .",
            "url": "https://blog.londogard.com/jvm/kotlin/sql/multiplatform/2020/06/01/sqldelight-kotlin.html",
            "relUrl": "/jvm/kotlin/sql/multiplatform/2020/06/01/sqldelight-kotlin.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "A simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency",
            "content": "CoViD-19 Swedish QA . I decided to scratch a small itch I’ve had for a while now - creating a search engine using an unsupervised approach. The final product, or the first iteration rather, ended up pretty good and I wanted to share what I’ve done so far. . Introduction to the problem and requirements . An unsupervised approach where we never edit the data nor supply any manually annotated data? Every Data Scientist dream I suppose. There’s a reason as of why supervised approaches generally result in better performance but there is some light at the end of the tunnel for unsupervised approaches too. . Let’s begin with my own requirements, which are mainly created to only keep the fun problem-solving left. . The end-product must be unsupervised No manually annotated data | No heuristic applied (at least in first iteration) | . | It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end) | Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!) | . With this in mind I set out to build my own FAQ search engine. . What is required to answer questions using a FAQ? We need to find the most relevant Q/A to the question posed. . How do we do this? There is numerous types of ways to do this unsupervised. I’ll account for a few here: . Latent Dirichlet Allocation (LDA) which is a way to find topics through clever statistical analysis (basically soft clusters of documents) | Embedding and Cosine Similarity, find the distance between the two arrays of numbers in the embedded space. One can also apply Euclidean Distance which isn’t especially good because of Curse of Dimensionality. Other possible approaches includes Word Mover Distance. | Simple word counting and Bag of Words | Tools Chosen . After a little research I found a few tools which fit my need. . fastText . fastText that came out of Facebook AI Research (FAIR) and this paper. It’s a type of Word Embeddings where also subwords are embedded through ngrams of characters, this means that we are able to embedd words that are out of vocabulary, which can be the reason because of either misspelling or just a missing word. On their homepage they have a plethora of models including a Swedish one that has been derived from Wikipedia, pretty awesome! . Smooth Inverse Frequency . Smooth Inverse Frequency (SIF) is an algorithm to embed sentences which was proposed in &quot;A Simple but Tough-To-Beat Baseline for Sentence Embeddings&quot; in 2017. In its essence they propose to embed the sentence using a weighted average and thereafter modify them a bit using PCA/SVD. . Folkhälsomyndigheten FAQ . Finally I need the FAQ to use, in my case it’s Covid-19 FAQ from Folkhälsomyndigheten. It was parsed into pandas dataframes using requests &amp; BeautifulSoup4 (bs4). . Final Result . So after all this was figured out I sat down an afternoon and cooked some code together, the result ended up more impressive than I had imagined. The questions posed are being responded with pretty good results. I’m especially impressed by question about astma, son and regler. Here’s a few of them: . &gt; Hur sjuk blir jag? Hur sjuk blir man av covid-19? - 0.98 Hur länge är man sjuk av covid-19? - 0.97 Hur lång är inkubationstiden? - 0.81 . &gt; Hur vet jag om det är astma? Hur vet jag om mina symtom beror på pollenallergi eller på covid-19? - 0.63 Hur sjuk blir man av covid-19? - 0.53 Hur länge är man sjuk av covid-19? - 0.53 . &gt; Hur förklarar jag corona för min son? Hur pratar man med barn om det nya coronaviruset? - 0.58 Hur lång är inkubationstiden? - 0.53 Hur sjuk blir man av covid-19? - 0.49 . &gt; Hur minskar vi spridningen i sverige? Hur gör ni för att mäta förekomsten av covid-19 i samhället? - 0.65 Hur övervakar ni på Folkhälsomyndigheten spridningen av covid-19? - 0.57 Hur stor är dödligheten till följd av covid-19? - 0.56 . &gt; Vad för regler finns? Vad gäller för olika verksamheter? - 0.76 Vad gäller för handeln? - 0.75 Vad är covid-19? - 0.71 . One can directly note the correlation of the beginning. It seems like the first word has a high correlation with the most similar question. Weird. Removing stop words could probably improve this, but that’d be for the second implementation. . Further improvements for iteration 2, 3 and beyond! . Pre-processing . As mentioned right above we can apply some basic pre-processing such as removing stop words. In reality this should be handled by SIF but looking at our similarity scores there’s a 1-1 relation between the first word of the sentence. . Other improvements worth trying out is lemmatizing or stemming the words (&quot;cutting them to the root&quot; in simple terms) and further using a better tokenization is worth trying out (currently splitting on whitespace). spaCy offers a strong tokenizer, but I haven’t tried it out for Swedish yet. Once again fastText should handle this but it’s worth trying out if it improves or keep the result at the same level. . Different Embedding Techniques . There exist a certain Sentence Embedding that’s basically made for this task - MULE (Multimodal Universal Language Embeddings). MULE is even multilingual but unfortunately they’re not able to embed Swedish so we’d require a translation from Swedish to one of the 16 languages supported by MULE. This means that it is out of the question because of my requirements, but could still be fun to check out. . Other embeddings such as FLAIR (by Zalando), BERT (using BERT-as-a-service) or even training my own embeddings (perhaps using StarSpace) could prove interesting also. . Completely other technique . I mentioned first of all LDA, and I think LDA could be interesting. Most often LDA is applied to larger documents but as with everything it is never wrong to try out and verify the results. . Supervised approaches would certainly be able to show us some good performance but that requires annotating data in one way or another which is a boring task - but very important. Perhaps I’ll revisit and label some data, with todays Transfer Learning we can achieve higher accuracy with less data using other pre-trained Language Models such as BERT or Multifit (from Ulmfit). . Ending words . This was a really fun task and I’m happy that I tried it out. I’m sure I’ll revisit and improve it further by applying some of the possible improvements. Further I think I might actually try to do this for all FAQs available by our authorities to create a &quot;Multi FAQ&quot; which could prove pretty cool. With more data the results should also be better. . And as an ending note my model ended up using 2.5-3 GB of memory during run-time which means it’s possible to run on my Raspberry Pi 4! Further reduction of size can be done by removing the most uncommon words in the vocabulary (vocab is 2M words, which is very large). I applied a dimension reduction using the built in version of fastText (ending up using d=100 and still achieving good search results). . The implementation is available at my GitHub (Londogard) or directly launched in Google Colaboratory. . Thanks for this time, I’ll be back with more! Hampus Londögård .",
            "url": "https://blog.londogard.com/faq/nlp/machine-learning/2020/05/13/faq-search-covid-1.html",
            "relUrl": "/faq/nlp/machine-learning/2020/05/13/faq-search-covid-1.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "How I created a email generator in Kotlin (for Afry Tipsrundan)",
            "content": "Email Generation - Tipsrundan - . At AFRY IT South I’m co-responsible with Hassan Ftouni at driving the competence. One of my initiatives that we both now drive is to have a biweekly, every second week for all you picky readers out there, newsletter called &quot;Tipsrundan&quot;. . Tipsrundan has lately gathered some fuss around and Afry IT West now wants to join in. This means new challenges to somehow build an email together with more people, collaborating on what to have and what to keep regional. Let me say that this is a fun challenge! . In this post I’ll go through how I built our completely new &quot;Email Generator&quot; in Kotlin that I built a Sunday afternoon. This includes a few things such as . Learning how CSS works in emails (in comparison to browsers) | Kotlin | . Let me start at how Tipsrundan has evolved since we initiated it in October (crazy how time flies). . The evolution of Tipsrundan . When we sent the first Tipsrundan, called TL;DR back then, I used a &quot;email templating language&quot; called MJML and a pre-built template found on their homepage. With this we got a responsive email using their &quot;homemade&quot; templating language. I enjoyed it at the same time as I hated it, there was way to much manual labour copying the sections and inserting my own code, the indentation in the web-editor wasn’t great and so on. I bet it’s a great tool but it didn’t cut it for me, after two or three issues and a ton of research I found a new tool I liked, with a good free variant, called Stripo. Stripo is a really good tool which has excellent support with its drag n’ drop editor where you can save modules and much more. We got a good looking &amp; responsive email that worked out great, everything good right? . It was really good until I realized we had to share the template with new people and Stripo requires premium for this (can’t blame them, they need their cut) which honestly I was to lazy to fix through management. . With this knowledge I set out to create a tool which we could use internally that is simple and keeps simple. Forward comes a solution I built over a Sunday afternoon where we generate emails from JSON. . Tipsrundan generation . I had some pretty simple requirement: . JSON or yaml as the filetype which we’d generate Tipsrundan-email from. | Have different sections and easily extendable | Decent looking &amp; responsive (i.e. work on a phone and desktop) | By having these requirements I knew that it’ll be easy to sync over git or whatever tool we need and that we can potentially create themed Tipsrundan editions in the future. . I also knew that I wanted to do this in Kotlin, mainly because I really enjoy coding in Kotlin. . Step 1: Defining the format . The first step was to decide what format to use, or at least begin with. Both yaml and JSON was considered and in the end JSON felt like the best fit. . { &quot;title &quot;: string, &quot;issue &quot;: number, &quot;regional &quot;: [item], .. more categories } . where item is . { &quot;title &quot;: string, &quot;description &quot;: string, &quot;url &quot;: string } . Pretty straight-forward, as I said I prefer to keep it simple. . Step 2: Reading the data . Now that we have a definition of the data we need to read it, this is really a solved problem in most languages through some kind of library. In my case I choose the serialization library provided by Kotlin in the kotlinx-library. As a FYI this library can serialize using CBOR (Concise Binary Object Representation) and other formats. The name of the library is kotlinx-serialization and can be found here. It’s easiest installed through gradle (using the Kotlin DSL): . plugins { kotlin( &quot;plugin.serialization &quot;) version &quot;1.3.70 &quot; // same version as kotlin } dependencies { .. other dependencies implementation( &quot;org.jetbrains.kotlinx:kotlinx-serialization-runtime:0.20.0 &quot;) // Requires jcenter() as a repository } . kotlinx-serialization is actually cross-platform compatible meaning that it exists for Kotlin targeting JVM, Native &amp; JS (yes we can target all these platforms through Kotlin!). . Once installed it’s pretty easy to serialize &amp; deserialize, like any other library really. Create the data classes, which is the equivalent of a case class in Scala. . data class Item(val title: String, val description: String, val url: String) . Currently kotlinx-serialization can do the serialization through two different methods, either add an annotation to the class that we’ll use Reflection - this does not work for native. Or we mark the data class as Serializable, the latter being preferred as it’s truly cross-platform and is more performant. If anyone is wondering a data class is basically a class that provides setters, getters, equality, toString and more! It’s really awesome. Adding the annotation we end up with the following: . @Serializable data class Item(val title: String, val description: String, val url: String) . Step 3: How to write html in Kotlin? . We all probably know about html-templating that’s available in most languages, I decided against that and went for a DSL. Kotlin is the language for DSL (Domain Specific Language), for good and bad. Through yet another kotlinx library we got kotlinx-html which provides this DSL. . It looks something like this . fun BODY.createFooter() = footer { hr { } section { p { b { + &quot;Thank you for this time see you in two weeks &quot; } br { } + &quot;Hampus &amp; Hassan &quot; } } } . By using a DSL we get types (as you can see on the BODY) and other bonuses. Although this DSL is pretty verbose it works pretty good. In the end using a DSL or html-template engine does not matter that much in my opinion. By the way, the way this function is typed is called a extension function in Kotlin and is one of my favorite tools. It means that we extend the class, BODY with a new method which is usable on a object of the class. Cool right? . Let’s move on to the styling and how CSS can be annoying. . Step 4: Styling . There was some important parts going into this, we want the email to look at least decent and also be responsive so that it’s viewable on both a phone and computer. . CSS and emails are not as simple as with a webpage I learned rather fast. I had great issues actually getting the HTML to look good in gmail/outlook. In the end I found this awesome post from Litmus which is one of the leading Email Marketing providers. I learned that . External CSS is a no-go for emails (a lot of the providers turned it off because of security concerns) | Embedded CSS (using style-tag in the header) works on most places today (not true a few years ago) | Inline CSS is the best | Because I want to keep it simple I went with the second approach, this mean that I can keep the code a bit cleaner and not write as many wrappers for the styled elements. . So knowing how I should implement my styling I needed to find a good style, in the end I remembered an old Reddit-post where I found &quot;MVP.css&quot; which is a small CSS that gives cards, buttons and more. Really brilliant in my opinion, made by Andy Brewer and can be found here. I’ve personally tweaked it a bit to keep the email a bit more compact and informative as this is really made for webpages, but the essentials are the same. . Step 5: Wrapping it all up . Combining all this into a few files in a git repo we can now generate emails from a JSON easily and have multiple categories. . The JSON is used as a data structure | Kotlin used as language | kotlinx-serialization used as a JSON deserializer | kotlinx-html used to build the HTML directly in Kotlin with types | Embedded CSS used as it’s widely usable by today in email clients | . The repository can be found here. . I hope this was somewhat interesting &amp; something learned. If you’ve any comments please reach out to me through any of the available channels! . Hampus Londögård .",
            "url": "https://blog.londogard.com/email/kotlin/html/css/serialization/2020/03/31/email-generator-kotlin-tipsrundan.html",
            "relUrl": "/email/kotlin/html/css/serialization/2020/03/31/email-generator-kotlin-tipsrundan.html",
            "date": " • Mar 31, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "[2019-02-04] AFRY NLP Competence Meeting: Text Classification IMDB",
            "content": "2019-02-04 AFRY NLP Competence Meeting: Text Classification IMDB . I’ve set a goal to create one blog post per Competence Meeting I’ve held at AFRY to spread the knowledge further. This goal will also grab all the older meetings, my hope is that I’ll be finished before summer 2020, but we’ll see. . . Introduction . Most of my Competence Meetings take place in the form of Jupyter Notebooks (.ipynb). Notebooks are awesome as they allow us to: . Mix and match markdown &amp; code-blocks | Keep the state of the program, i.e. very explorative | This is really good in combination with the workshop-format that we usually have. Using services such as Google Colab one can take the file and open it in the browser and run it there. This means that we don’t need any downloads and pretty often we also have a speed gain because the node used is faster than a laptop with its GPU. . Let’s get on to the competence evening. . . Text Classification . Today we’ll go through text classification, what it is, how it is used and how to make it yourself while trying to keep have a great mix of both theory and practical use. Text classification is just what the name suggest, a way to classify texts. Let it be spam or reviews, you train it and it’ll predict what class the text belongs to. . . A good baseline . To have a good baseline is incredibly important in Machine Learning. In summary you want the following . Simple model to predict outcome | Use this model to compare your new, more complex model to | . This is to be able to know what progress you’re making. You don’t want to do anything more complex without any gains. . One pretty common simple baseline is just to pick a random class as prediction. . Classes &amp; Features . What is a class and feature? . Features are the input to the model, you can see a machine learning system as a &quot;consumer&quot; of features. You can view this as a cookie monster consuming cookies and then he says if they taste good or bad. He has the input, cookie, that can be a feature. He then has a output, class, that is good/bad. Repeat this a lot of times and you can retrieve statistics if Cookie Y is good or bad. . To generalize this system we would divide the feature into multiple feature, like what ingredients the cookie contains. So instead of saying this is a &quot;Chocolate Chip Cookie&quot; we know tell the system the features are: . chocolate: yes sugar:yes honey:no oat:no cinnamon: no sweet: yes sour: no &quot; . . In numerical input it would translate to something as [1,1,0,0,0,1,0]. . One-Hot-Encoding - how we represent features &amp; classes . As shown in the translation to numerical vectors we don’t represent words as actual words. We always use numbers, often we even use something called One-Hot-Encoding. . One-Hot-Encoding means that we have an array of one 1 and the rest is 0s. This is to optimize math performed by the GPU (or CPU). . Using the example of Good &amp; Bad cookies with the extension of Decent we will One-Hot-Encode these as the following . Good = [1,0,0] Bad = [0,1,0] Decent = [0,0,1] . The same is applied to our features. If you’re using a framework (such as Keras) it is pretty common that they include an method to do this, or even that it is done automatically for you. . Back to text classification . To classify a text we do what is called an sentiment analysis meaning that we try to estimate the sentiment polarity of a text body. In the first part of this workshop we’ll be assuming that there’s only two sentiments, Negative and Positive. Then we can express this as the following classification problem: . Feature: String body Class: Bad|Good . The output, Classes, are easy to One-Hot-Encode but how do we succesfully One-Hot-Encode a string? A character can be seen as a class but is that really something we can learn from? To solve this we need to preprocess our input somehow. . Preprocessing . Preprocessing is an incredibly important part of Machine Learning. Combining preprocessing with Data Mining is actually around 70% of the workload (IBM) when developing models through the CRISP-DM. From my experience this is true. . Having good data and finding the most important features is incredibly important to have a competent system. In this task we need to preprocess the text to simplify the learning process for our system. We will do the following: . Clean the text | Vectorize the texts into numerical vectors | . Cleaning the text . Why do we need to clean the text? It is to remove weird stuff &amp; outliers. If we have the text I&#39;m a cat.we want to simplify this into [i&#39;m, a, cat] or even [im, a, cat]. . Removing data such as non-alphabetical characters and the letter case makes more data look a like and reduces the dimension of our input – this simplifies the learning of the system. But removing features can be bad also, if someone writes in all CAPS we can guess that they’re angry. But let’s take that later. . import regex as re def clean_text(text): &quot; &quot; &quot; Applies some pre-processing on the given text. Steps : - Removing punctuation - Lowering text &quot; &quot; &quot; # remove the characters [ ], [&#39;] and [ &quot;] text = re.sub(r &quot; &quot;, &quot; &quot;, text) text = re.sub(r &quot; &#39; &quot;, &quot; &quot;, text) # Extra: Is regex needed? Other ways to accomplish this. text = re.sub(r &quot; &quot; &quot;, &quot; &quot;, text) # replace all non alphanumeric with space text = re.sub(r &quot; W+ &quot;, &quot; &quot;, text) # text = re.sub(r &quot;&lt;.+?&gt; &quot;, &quot; &quot;, text) # &lt;br&gt;&lt;/br&gt;hej&lt;br&gt;&lt;/br&gt; # Extra: How would we go ahead and remove HTML? Time to learn some Regex! return text.strip().lower() clean_text( &quot;Wow, we can clean text now. Isn&#39;t that amazing!? &quot;).split() . Vectorization . Now that we can extract text we need to be able to input it to the system. We have to vectorize it. In this part we’ll vectorize each word as a number. The simplest approach to this is using Bag of Words (BOW). . Bag of Words creates a list of words which is called the Dictionary. The Dictionary is just a list of the words from the training data. . Training data: [ &quot;ÅF is a big company &quot;, &quot;ÅF making future &quot;] --&gt; Dictionary: [ÅF, is, a, big, company, making, future] New text: &quot;ÅF company is a future company &quot; --&gt; [1,1,1,0,2,0,1] . Our new text is vectorized on top of the dictionary. You take the dictionary and replace the words position with the count of it that is found in the new text. . Finalizing the preprocessing . We can actually do some more things to improve the system which I won’t go into detail about (read the code). We remove stop-words and so on. . from sklearn.feature_extraction.text import CountVectorizer training_texts = [ &quot;ÅF is a big company &quot;, &quot;ÅF making future &quot; ] test_texts = [ &quot;ÅF company is a future company &quot; ] # this is the vectorizer vectorizer = CountVectorizer( stop_words= &quot;english &quot;, # Removes english stop words (such as &#39;a&#39;, &#39;is&#39; and so on.) preprocessor=clean_text # Customized preprocessor ) # fit the vectorizer on the training text vectorizer.fit(training_texts) # get the vectorizer&#39;s vocabulary inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()} vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))] # vectorization example pd.DataFrame( data=vectorizer.transform(test_texts).toarray(), index=[ &quot;Test sentence &quot;], columns=vocabulary ) . Let’s do something fun out of this! . To begin with we need data. Luckily I know a perfect dataset for this – the IMDB movie reviews from stanford. This is a widely used dataset throughout Sentiment Analysis. The data contains 50 000 reviews where 50 % is positive and the rest negative. First we fetch a dataset. Download this file and unpack it (into aclImdb) if the first code-snippet was unsuccessful. . import os import numpy as np def load_train_test_imdb_data(data_dir): &quot; &quot; &quot; Loads the IMDB train/test datasets from a folder path. Input: data_dir: path to the &quot;aclImdb &quot; folder. Returns: train/test datasets as pandas dataframes. &quot; &quot; &quot; data = {} for split in [ &quot;train &quot;, &quot;test &quot;]: data[split] = [] for sentiment in [ &quot;neg &quot;, &quot;pos &quot;]: score = 1 if sentiment == &quot;pos &quot; else 0 path = os.path.join(data_dir, split, sentiment) file_names = os.listdir(path) for f_name in file_names: with open(os.path.join(path, f_name), &quot;r &quot;) as f: review = f.read() data[split].append([review, score]) # We shuffle the data to make sure we don&#39;t train on sorted data. This results in some bad training. np.random.shuffle(data[ &quot;train &quot;]) data[ &quot;train &quot;] = pd.DataFrame(data[ &quot;train &quot;], columns=[&#39;text&#39;, &#39;sentiment&#39;]) np.random.shuffle(data[ &quot;test &quot;]) data[ &quot;test &quot;] = pd.DataFrame(data[ &quot;test &quot;], columns=[&#39;text&#39;, &#39;sentiment&#39;]) return data[ &quot;train &quot;], data[ &quot;test &quot;] train_data, test_data = load_train_test_imdb_data( data_dir= &quot;aclImdb/ &quot;) . Let’s create our classifier . We now have a dataset that we have successfully partitioned into a dictionary so that we can use it for our classifier. . Do you see an issue with our baseline right now? . …As mentioned we want to only have important features to simplify training. Right now we have an enormous amount of features, our BOW-approach result in an 80 000-dimensional vector. Because of this we must use simple algorithms that learn fast &amp; easy, e.g. Linear SVM, Naive Bayes or Logistic Regression. . Let’s create some code that actually let’s us train a Linear SVM! . from sklearn.metrics import accuracy_score from sklearn.svm import LinearSVC # Transform each text into a vector of word counts vectorizer = CountVectorizer(stop_words= &quot;english &quot;, preprocessor=clean_text) training_features = vectorizer.fit_transform(train_data[ &quot;text &quot;]) test_features = vectorizer.transform(test_data[ &quot;text &quot;]) # Training model = LinearSVC() model.fit(training_features, train_data[ &quot;sentiment &quot;]) y_pred = model.predict(test_features) # Evaluation acc = accuracy_score(test_data[ &quot;sentiment &quot;], y_pred) print( &quot;Accuracy on the IMDB dataset: {:.2f} &quot;.format(acc*100)) . Comparison to state-of-the-art . Our accuracy is somewhere around 83.5-84 % which is really good! With this simple model and incredibly simplistic feature extraction we achieve a really high amount of correct answer! Comparing this to state-of-the-art we’re around 11 percent units beneat (~95% accuracy achieved here). . Incredible right? Exciting!? For me it is at least! . How do we improve from here? . Improving the model . We have some huge improvements to make outside of fine-tuning, so we’ll skip the fine-tuning from now. . The first step is to improve our vectorization. . TF-IDF . If you were at first friday (@ÅF) you have heard about TF-IDF earlier. TF-IDF stands for Term Frequence-Inverse Document Frequency and is a measurement that aims to fight imbalances in texts. . In our vectorization step we look at the word-count meaning that we’ll have some biases to how much a word is present, the longer the text the more the bias. To reduce this we can take the word-count divided by the total amount of words in the text (TF). We also want to downscale the words that are incredibly frequent such as stop words and topic-related words, and upscale unusual words somewhat, e.g.glamorous might not be frequent but it is important to the text most likely. We use IDF for this. We then take these two and combine. . . Implementation details . This is actually really easy to do as sklearn already has a finished TfIdfVectorizer so all we have to do is to replace the CountVectorizer. Let’s see how it goes! . from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from sklearn.feature_extraction.text import TfidfVectorizer # Transform each text into a vector of word counts vectorizer = TfidfVectorizer(stop_words= &quot;english &quot;, preprocessor=clean_text) training_features = vectorizer.fit_transform(train_data[ &quot;text &quot;]) test_features = vectorizer.transform(test_data[ &quot;text &quot;]) # Training model = LinearSVC() model.fit(training_features, train_data[ &quot;sentiment &quot;]) y_pred = model.predict(test_features) # Evaluation acc = accuracy_score(test_data[ &quot;sentiment &quot;], y_pred) print( &quot;Accuracy on the IMDB dataset: {:.2f} &quot;.format(acc*100)) # Extra: Implement our own TfIdfVectorizer. . Conclusion of TF-IDF . The TfIdVectorizer improved our scoring with 2 percent units, that’s incredible for such an easy improvement! . This for me shows how important it is to understand the data and what is important. You really need to grasp how to extract the important and what tools are available. . But let’s not stop here, lets reiterate and improve further. . What is the next natural step? Context I believe. During my master-thesis on spell correction of Street Names it was very obvious how important context is to increase the models understanding. Unfortunately we couldn’t use the context of a sentence in the thesis (as of the nature of street names) but here we can! . Use of context . Words by themself prove some meaning but sometimes they’re used in a negated sense, e.g. not good. Good in itself would most likely be positive but if we can get the context around the word we can be more sure about in what manner it is applied. . We call this N-grams where N is equal to the amount of words taken into consideration for each word. Using bigrams (N=2) we get the following: . companies often use corporate bs =&gt; [companies, often, use, slogans, (companies, often), (often,use), (use,slogans)] . Sometimes you include a start &amp; ending word so that it would be ( t, companies) and (slogans, r) or such. In this case as we are not finetuning we won’t go into that. We’ll keep it simple. . The all-mighty sklearn TfIdfVectorizer actually already have included N-gram support using the parameter ngram_range=(1, N). So let’s make it simple for us and make use of that! . from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from sklearn.feature_extraction.text import TfidfVectorizer # Transform each text into a vector of word counts vectorizer = TfidfVectorizer(ngram_range=(1, 2), strip_accents=&#39;ascii&#39;, max_df=0.98) training_features = vectorizer.fit_transform(train_data[ &quot;text &quot;]) test_features = vectorizer.transform(test_data[ &quot;text &quot;]) # Training model = LinearSVC() model.fit(training_features, train_data[ &quot;sentiment &quot;]) y_pred = model.predict(test_features) # Evaluation acc = accuracy_score(test_data[ &quot;sentiment &quot;], y_pred) print( &quot;Accuracy on the IMDB dataset: {:.2f} &quot;.format(acc*100)) . Conclusion of N-gram . Once again we see a massive improvement. We’re almost touching 89 % now! That’s just a mere 6 percent units below state-of-the-art. What can we do to improve now? . Some possible improvements for you to try! . Use a custom threshold to reduce the dimensions | Play around with the ngram_range (don’t forget a threshold if you do this) | Improve the preprocessing | . # Try some fun things here if you want too :) . Conclusion of phase 1 . We have created a strong baseline for text classification with great accuracy for its simplicity. The following steps has been done . First a simple preprocessing step which is of great importance. We have to remember to not make it to complex, the complexity of preprocessing is like an evil circle in the end. In our case we remove punctuations, stopwords and lower the case. | Secondly we vectorize the data to make it readable by the system. A classifier requires numerical features. For this we had a TfIdfVectorizer that computes frequency of words while downsampling words that are to common &amp; upsampling unusual words. | Finally we added N-gram to the model to increase the understanding of the sentence by supplying context. | . Phase 2 . How do we improve from here? TF-IDF has its cons and pros. Some of the cons are that they: . Don’t account for any kind of positioning at all | The dimensions are ridiculous large | They can’t capture semantics. | . Improvements upon this is made by using neural networks and word embeddings. . Word Embeddings . Word Embeddings &amp; Neural Networks are where we left off. By change our model to instead utilize these two concepts we can improve the accuracy once again. . Word Embeddings . Word Embeddings (WE) are actually a type of Neural Network. It uses embedding to create the model. I quickly explained WE during my presentation on Summarization and how to build a great summarizer. Today we’ll go a little more into depth. . To begin with I’ll take the most common example, WE lets us do the following arithmetiric with words: . King - Man + Woman = Queen . This is, in my opinion, completely amazing and fascinating. How does this work? Where do I learn more? Those are my first thoughts. In fact the theory is pretty basic until you get to the nittygritty details, as with most things. . WE is built on the concept ot learn how words are related to eachother. What company do a word have? To make the example more complex we can redefine this too the following: A is to B what C is to D. . Currently there is three &quot;big&quot; models that are widely used. The first one Word2Vec (Mikolov et al 2013), the second is GloVe (MIT MIT, Pennington et al 2014) and the final one is fastText (facebook). . We will look into how you can achieve this without Deep Learning / Neural Networks unlike the models mentioned. . Step 1: How to represent words in a numerical vector . The first thing we have to do to actually understand/achieve word embeddings is to represent words in a numerical vector. In relation to this a quick explanation of sparse &amp; dense representations would be great. Read more in detail at Wikipedia: Sparse Matrix . Sparse representation is when we represent something very sparsely. It tells us that the points in the space is very few in regards to the dimensions and that most elements are empty. Think one-hot-encoding. . A Dense representation in comparison has few dimensions in comparison to possible values and most elements are filled. Think of something continuous. . The most simple way to represent words in a numerical vector is something we touched earlier, by one-hot-encoding them, i.e. a sparse representation. . (Source: Marco Bonzanini, 2017) . Because of how languages are structured having one-hot-encoding means that we will have an incredibly sparse matrix (can be good) but it will have an enormous amount of dimensions (bad). . On top of this how would we go ahead and measure the distance between words? Normally one would use the cosine similarity but if we have a one-hot-encoding all the words would be orthogonal against eachother meaning that the dot-product will be zero. . Creating a dense representation however would indeed capture similarity as we could make use of cosine-similarity and more. Introducing Word2Vec. . Step 2: Word2Vec, representing data densely . The goal of Word2Vec, at least to my understanding, is to actually predict the context of a word. Or in other words we learn embeddings by prediciting the context of the word. The context here being the same definition as in N-grams. Word2Vec uses shallow neural network to learn word vectors so that each word is good at predicting its own contexts (more about his in Skip-Grams) and how to predict a word given a context (more about this in CBOW). . Skip-gram . Skip-gram very simplified is when you train on the N-grams but without the real word. . As of now we have empirical results showing how this technique is very successful at learning the meaning of the words. On top of this the embedding that we get has both direction of semantic and syntatic meaning that are exposed in example such as King - Man.... . Another example would be: Vector(Madrid) - Vector(Spain) + Vector(Sweden) ~ Vector(Stockholm) . So how do the arithmetic of words actually work? . I won’t go into details (some complicated math, see Gittens et al) but if we assume the following to be true: . All words are distributed uniformly | The embedding model is linear | The conditional distributions of words are indepedent | . Then we can prove that the embedding of the paraphrase of a set of words is obtained by taking the sum over the embeddings of all of the individual words. . Using this result it’s easy to show how the famous man-woman, king-queen relationship works. . Extra note: You can show this then by havingn King and Queen having the same Male-Femalerelationship as the King then is the paraphrase of the set of words {Queen, X} . I want to note that these assumptions are not 100 percent accurate. In reality word distributions are thought to follow Zipf’s law. . GloVe . A year after Word2Vec was a fact to the world the scientist decided to reiterate again. This time we got GloVe. GloVe tried to improve upon Word2Vec by that given a word its relationship(s) can be recovered from co-occurence statistics of a large corpus. GloVe is expensive and memory hungry, but it’s only one load so the issue isn’t that big. Nitty bitty details . fastText . With fastText one of the biggest problems is solved, both GloVe and Word2Vec only learn embeddings of word of the vocabulary. Because of this we can’t find an embedding for a word that isn’t in the dictionary. . Bojanowski et al solved this by learning the word embeddings using subword information. To summarize fastText learns embeddings of character n-grams instead. . The simple way . A simple approach to create your own word embeddings without a neural network is by factorizing a co-occurence matrix using SVD (singular-value-decomposition). As mentioned Word2Vec is barely a neural network as it has no hidden layers nor an y non-linearities. GloVe factorizes a co-occurense matrix while gaining even better results. . I highly recommend you to go check this blog out: https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ by Stitch Fix. An awesome read and we can go implement this too! .",
            "url": "https://blog.londogard.com/python/competence/machine-learning/nlp/text-classification/2020/02/23/competence-meeting-imdb-text-classification.html",
            "relUrl": "/python/competence/machine-learning/nlp/text-classification/2020/02/23/competence-meeting-imdb-text-classification.html",
            "date": " • Feb 23, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Gradle, JVM and GitHub Packages",
            "content": "Gradle, JVM and GitHub Packages . Initial comment this is mainly done as a reminder to myself. . So about 6 months ago GitHub launched a new exciting service; GitHub Package Registry. This service lets you as a GitHub-user upload your Open Source code for free on GitHubs registry supporting a wide array of languagues and build systems - JavaScript (npm), Java/JVM-languages (Maven/Gradle), Ruby (RubyGems), .NET (NuGet), and Docker images. Perhaps more have been added since I last verified. . In this post I’ll try to keep to the point and give clear easy instruction that you’ll be able to bookmark and go back to whenever you need to set this up. . I’ll give the instruction directly beneath with some comments afterwards for the one who’d like to read some extra. . . 1. Add Maven Publish Plugin . build.gradle.kts . plugins { `maven-publish` // Add this kotlin( &quot;jvm &quot;) version &quot;1.3.60 &quot; } . build.gradle . plugins { id( &quot;maven-publish &quot;) } . build.gradle (using the old apply way of things) . apply plugin: &#39;maven-publish&#39; . 2. Add Publication Part . build.gradle.kts . publishing { repositories { maven { name = &quot;GitHubPackages &quot; url = uri( &quot;https://maven.pkg.github.com/OWNER/REPO &quot;) credentials { username = project.findProperty( &quot;gpr.user &quot;) as String? ?: System.getenv( &quot;GITHUB_ACTOR &quot;) password = project.findProperty( &quot;gpr.key &quot;) as String? ?: System.getenv( &quot;GITHUB_TOKEN &quot;) } } } publications { register&lt;MavenPublication&gt;( &quot;gpr &quot;){ from(components[ &quot;java &quot;]) } } } . build.gradle . publishing { repositories { maven { name = &quot;GitHubPackages &quot; url = uri( &quot;https://maven.pkg.github.com/OWNER/REPOSITORY &quot;) credentials { username = project.findProperty( &quot;gpr.user &quot;) ?: System.getenv( &quot;GITHUB_ACTOR &quot;) password = project.findProperty( &quot;gpr.key &quot;) ?: System.getenv( &quot;GITHUB_TOKEN &quot;) } } } publications { gpr(MavenPublication) { from(components.java) } } } . 3. Automating Release Workflow . To simplify our lifes further; . JitPack is already automated and tracking your repository automatically adding the new releases ones a release is created. | GitHub is not automated and we need to upload our assets | . Automating GitHub packages upload through release &amp; GitHub Actions . We’ll use GitHub Actions to create a workflow where once a release passes stage ‘published’ the assets will be uploaded to the repository/artifactory of GitHub Packages. This integration is really awesome as once we’ve set it up we only need to press &quot;Create Release&quot; on the GitHub page to deploy our library to both GitHub Packages &amp; JitPack! . Create the directory .github/workflows in your root-folder of the project if it doesn’t exist yet. Add the following file: . name: Release &amp; Publish Build on: release: types: [published] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 1.8 uses: actions/setup-java@v1 with: java-version: 1.8 - name: Clean Build run: ./gradlew clean build - name: Publish Build env: GITHUB_TOKEN: $ run: ./gradlew publish . The secrets.GITHUB_TOKEN is automatically supplied by GitHub itself during the run of the GitHub Actions-script. . Pretty awesome right? Go build your libraries and deploy! .",
            "url": "https://blog.londogard.com/gradle/jvm/library/2020/02/10/gradle-github-packages.html",
            "relUrl": "/gradle/jvm/library/2020/02/10/gradle-github-packages.html",
            "date": " • Feb 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.londogard.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.londogard.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}