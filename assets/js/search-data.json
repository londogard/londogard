{
  
    
        "post0": {
            "title": "Transformers From Scratch (by Hampus Lond√∂g√•rd)",
            "content": ". A visual illustration of basic self-attention. Note that the softmax operation over the weights is not illustrated. . Obviously we need to apply a few more things to create a Transformer, but we&#39;ll get to that. . Understanding how it works . As shown previously we&#39;ll use Movie Recommender system to show why the dot-product works. Say you created your manually annotated data . . As you see, negative + negative = positive, positive + positive = positive. The magnitude of the score increases the final outcome. . So combining these two vectors together will work out real nice! . This might not be so practical in reality and as such we make the movie and user features parameters of the model. Then we ask the user for for a small number of movies they like and optimize the features so that their dot product matches the known likes. Even when not manually giving any features meaningful data is extracted. . Even though we don‚Äôt tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content. . . This is in essence the same as self-attention. . Using word embeddings on a sentence as . $v_{the}, v_{cat}, v_{walk}$ . and feed it into self attention we get the output-vectors y, . $y_{the}, y_{cat}, y_{walk}$ . where y-vectors are the weighted sum over all embedding vectors in the first sequence, weighted by their (normalized) dot-product with $v_{cat}$. E.g. $y_{the}$ = $v_{the} * v_{the} + v_{the} * v_{cat} + v_{the} * v_{walk}$. . Because v is learned this will continually be updated to work better. While updating how v is created most likely walk and cat will have a high dot-product as those are correlated. . This is the basic intuition. Please note: . No parameters (yet) so the upstream mechanism creating the embeddings fully drives the self-attention by learning representations with particular dot-products | Self-attention see the input as a bag, i.e. not a continuous input as it is, e.g. it is permutation equivariant. | . In Pytorch: basic self-attention . What I cannot create, I do not understand - Feynman . The first thing we need to implement is matrix multiplications, which will be done through torch as native python loops are too slow. . Input:$t$ vectors, dimension $k$ and $b$ mini-batches (tensor: $b, t, k$) . We‚Äôll represent the input, a sequence of t vectors of dimension k as a t by k matrix ùêó. Including a minibatch dimension b, gives us an input tensor of size (b,t,k). . The set of all raw dot products w‚Ä≤ij forms a matrix, which we can compute simply by multiplying ùêó by its transpose: . Let&#39;s code! . class TODO(Exception): &quot;&quot;&quot;Raised when there is something TODO&quot;&quot;&quot; pass . import torch import torch.nn.functional as F # assume we have some tensor x with size (b=1, t=3, k=3) x = torch.array([[1.,2.,3., 1.,2.,3., 1.,2.,3.]]) # batch matrix multiplication; does not broadcast (!!!!) # could use matmul to broadcast raw_weights = raise TODO(&quot;Use torch batch-matrix-multiply (bmm) to do x*x^T. Remember there&#39;s 3 dimensions!&quot;) # Apply row-wise Softmax weights = raise TODO(&quot;Use functional softmax to apply softmax row-wise (along which dim?)&quot;) # creating y y = torch.bmm(weights, x) . And that&#39;s it. We have created basic self-attention which is the basis of Transformers (state-of-the-art). . Additional Tricks . To implement Transformers fully we need to add a few tricks. . 1) Queries, keys &amp; values . Each input-vector $x_i$ is used 3 times . Creating its own weight for $y_i$ | Creating others weight for $y$ ($y_j$) | Used as a part of the weighted sum for each output $y$ | This is often called the query, the key, and the value (explained later). We update the network to instead use three weight-matrices, one for each task, making it more controllable. Adding $W_k, W_q, W_v$ of size $k*k$, we&#39;ve got . $q_i = W_qx_i, k_i = W_kx_i, v_i = W_vx_i$ $w_{ij}^{&#39;} = q_i^Tk_j$ $w_ {ij} = softmax(w_{ij}^{&#39;})$ $y_i = sum_j w_{ij}v_j$ . We&#39;ve now given the Self Attention some controllable parameters &amp; allows modification to the input vector to fit the task at hands. . . Illustration of the self-attention with key (red), query (query) and value (green) transformations. Remember old image and compare! . 2) Scaling the Dot Product . Softmax can be very sensitive to large values, exploding gradient or making it slow to learn. I don&#39;t recall where but ~ 8 years ago someone figured out that scaling the value by $ frac{1}{ sqrt{k}}$ where $k$ is the embedding dimension. . $w_{ij}^{&#39;} = frac{q_i^Tk_j}{ sqrt{k}}$ . Why $ sqrt{k}$? Imagine a vector in $‚Ñùk$ with values all $c$. Its Euclidean length is $ sqrt{k}c$. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors. . 3) Multi-head Attention . The final improvement is to allow word to have different meanings with different neighbours (basically what ngram achieves). . By adding multiple, indexed r, self attention mechanisms with different matrices, $W_q^r$ etc. These are called attention heads. . For input $x_i$ each attention head produces a different output vector $y_i^r$. We concatenate these, and pass them through a linear transformation to reduce the dimension back to $k$. Remember what is a linear transformation? . Narrow and wide self-attention. There&#39;s two ways to apply Multi-Head Self-Attention. . (narrow) Cut embedding vector into chunks 8 heads &amp; $k=256$ --&gt; 8 chunks of size 32 | Each chunk gets Q, K, V matrices ($W_q^r$,...) ($32 times32$) | . | (wide) Make matrices $256 times256$ and apply each head to the whole 256-vector First (narrow) = faster &amp; less memory | Second (wider) = better result | . | Only second (wider) variant described. . In Pytorch: complete self-attention . Let&#39;s make the implementation with bells &amp; whistles. . import torch from torch import nn import torch.nn.functional as F class SelfAttention(): def __init__(self, k, heads=8): raise TODO(&quot;Make SelfAttention a torch module (nn.Module)&quot;) super().__init__() self.k, self.heads = k, heads . . Combining three attention heads into one matrix multiplication (for the queries). . h attention-heads considered as h separate sets of $W_q^r,W_k^r,W_v^r$, but as shown in image above a more efficient approach is possible. . Combine all heads into three single $k times hk$ matrices. This means that we can compute the concatenated queries, keys &amp; values in a single matrix multiplication. . class SelfAttention(nn.Module): def __init__(self, k, heads=8): super().__init__() self.k, self.heads = k, heads # These compute the queries, keys and values for all # heads (as a single concatenated vector) self.tokeys = raise TODO(&quot;Create a linear layer of k * hk size, no bias&quot;) self.toqueries = raise TODO(&quot;Create a linear layer of k * hk size, no bias&quot;) self.tovalues = raise TODO(&quot;Create a linear layer of k * hk size, no bias&quot;) # This unifies the outputs of the different heads into a single k-vector self.unifyheads = raise TODO(&quot;Create a linear layer of k * hk size, WITH bias&quot;) . From here we can implement the computation of the self-attention (forward function). Where we first calculate queries, keys &amp; values. . class SelfAttention(nn.Module): def __init__(self, k, heads=8): super().__init__() self.k, self.heads = k, heads # These compute the queries, keys and values for all # heads (as a single concatenated vector) self.tokeys = nn.Linear(k, k * heads, bias=False) self.toqueries = nn.Linear(k, k * heads, bias=False) self.tovalues = nn.Linear(k, k * heads, bias=False) # This unifies the outputs of the different heads into # a single k-vector self.unifyheads = nn.Linear(heads * k, k) def forward(self, x): b, t, k = x.size() h = self.heads queries = raise TODO(&quot;Create queries and then reshape into h separate matrices, using `view`&quot;) keys = raise TODO(&quot;Create keys and then reshape into h separate matrices, using `view`&quot;) values = raise TODO(&quot;Create values and then reshape into h separate matrices, using `view`&quot;) . Having reshaped queries etc from $(b,t, h*k)$ into $(b,t,h,k)$ each head has its own dimension. . The step now is to compute dot-product for each head. We can batch this if we reshape the matrices into something that&#39;s possible to batch (transposing; as head/batch is not next to each-other). costly . TODO !! Fix !! class SelfAttention(nn.Module): def __init__(self, k, heads=8): super().__init__() self.k, self.heads = k, heads # These compute the queries, keys and values for all # heads (as a single concatenated vector) self.tokeys = nn.Linear(k, k * heads, bias=False) self.toqueries = nn.Linear(k, k * heads, bias=False) self.tovalues = nn.Linear(k, k * heads, bias=False) # This unifies the outputs of the different heads into # a single k-vector self.unifyheads = nn.Linear(heads * k, k) def forward(self, x): b, t, k = x.size() h = self.heads queries = self.toqueries(x).view(b, t, h, k) keys = self.tokeys(x) .view(b, t, h, k) values = self.tovalues(x) .view(b, t, h, k) # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory keys = keys.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries.transpose(1, 2).contiguous().view(b * h, t, k) values = values.transpose(1, 2).contiguous().view(b * h, t, k) queries = raise TODO(&quot;Scale queries by k^(1/4)&quot;) keys = raise TODO(&quot;Scale keys by k^(1/4)&quot;) # - get dot product of queries and keys, and scale dot = torch.bmm(queries, keys.transpose(1, 2)) # - dot has size (b*h, t, k) containing raw weights dot = raise TODO(&quot;Normalize row-wise using F.softmax&quot;) # - dot now contains row-wise normalized weights # apply the self attention to the values out = torch.bmm(dot, values).view(b, h, t, k) # swap h, t back, unify heads out = out.transpose(1, 2).contiguous().view(b, t, h * k) return raise TODO(&quot;Unify the heads into the classes again using unifyheads&quot;) . And there you have it: multi-head, scaled dot-product self attention. . Building Transformers . A transformer is not just a self-attention layer, it is an architecture. . Any architecture designed to process a connected set of units‚Äîsuch as the tokens in a sequence or the pixels in an image‚Äîwhere the only interaction between units is through self-attention. . Like most mechanism, e.g. convolutions, a standard approach as emerged. The first step is to wrap the self-attention into a block that we can repeat. . The transformer block . There are some variations on how to build a basic transformer block, but most of them are structured roughly like this: . . MultiLayer Perceptron = MLP = Basic Feed-forward Neural Network | Blue lines = Residual Connection (allow the gradient to flow through the network on a kind of &quot;highway&quot;, making the training faster and reducing &quot;blown up gradients&quot;) | . class TransformerBlock(nn.Module): def __init__(self, k, heads): super().__init__() self.attention = raise TODO(&quot;make a SelfAttention&quot;) self.norm1 = raise TODO(&quot;make a nn.LayerNorm of size k&quot;) self.norm2 = raise TODO(&quot;make a nn.LayerNorm of size k&quot;) # k * 4 is arbitrary, but needs to be larger than input/output layer (k) self.ff = raise TODO(&quot;create a MLP: Sequential(Linear(k, 4*k), ReLU, Linear(4*k, k))&quot;) def forward(self, x): attended = raise TODO(&quot;call your created attention on x&quot;) x = raise TODO(&quot;Use the first normalizer to normalizer attented+x&quot;) fedforward = raise TODO(&quot;Call feedforward (ff) on new x&quot;) return raise TODO(&quot;Finally normalize with 2nd on the addition of feedforward &amp; x&quot;) . And that is really it! We have now built a Transformer Block &amp; Self Attention. . Now we want to use it :) . Classification transformer . The simplest classification task is a Sequence Classifier. The IMDB Review dataset is a great contender to try things out with, where each review is positive or negative. . Essentially we&#39;ll create a chain of Transformers Block and input our embedded vectors of words, and in the end transforming the output into a single value (true/false). . Output: producing a classifier . Most common way: Global Average Pooling on final output sequence and map result to a softmaxed class vector. . . Overview of a simple sequence classification transformer. The output sequence is averaged to produce a single vector representing the whole sequence. This vector is projected down to a vector with one element per class and softmaxed to produce probabilities. . Input:Using Positions (think: ngram)We&#39;ve already discussed that the current model uses embedding layer but has no sense of sequence time slotting (~&#160;ngram). . We want our State-of-the-Art model to have a sense of order so we need to fix it. . Add a second vector of the same length as word embedding, that represent the position of the sentence+word, and add it to the word embedding. There&#39;s two ways to do this. . Position Embedding We simply embed the positions like we did the words. Easy to implement | Works pretty good | Drawback is that we have to see sequences of every length during training, otherwise the position is not trained! | . | Position Encoding Position encodings work in the same way as embeddings, except that we don&#39;t learn the position vectors, we just choose some function $f:‚Ñï‚Üí‚Ñùk$ to map the positions to real valued vectors, and let the network figure out how to interpret these encodings. E.g. sin/cos | Works with longer sequences than seen (might not work well, but it works!) | Drawback: choice of encoding function is a complicated hyperparameter, and more complicated implementation. | . | For this tutorial the Position Embedding is used. . PyTorch . Let&#39;s implement this! . class Transformer(nn.Module): def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes): super().__init__() self.num_tokens = num_tokens self.token_emb = raise TODO(&quot;Create a Embedding layer (nn.X) with num_tokens &amp; k&quot;) self.pos_emb = raise TODO(&quot;Create Embedding Layer with seq_length &amp; k&quot;) # The sequence of transformer blocks that does all the # heavy lifting blocks = [] for i in range(depth): raise TODO(&quot;Append a TransformerBlock we recently created for each loop; and why not use list-comprehension?&quot;) self.t_blocks = raise TODO(&quot;Now make them a Sequential layer (*list = spread)&quot;) # Maps the final output sequence to class logits self.to_probs = raise TODO(&quot;To get class logits we simply use a Linear layer of k * num_classes&quot;) def forward(self, x): &quot;&quot;&quot; :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary). :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes). &quot;&quot;&quot; # generate token embeddings tokens = raise TODO(&quot;Embedd the tokens&quot;) b, t, k = tokens.size() # generate position embeddings positions = torch.arange(t) positions = self.pos_emb(positions)[None, :, :].expand(b, t, k) x = tokens + positions x = raise TODO(&quot;Run the network through the tranformer blocks&quot;) # Average-pool over the t dimension and project to class # probabilities x = raise TODO(&quot;Use the probability function, but first take the mean over dim=1 to average&quot;) return raise TODO(&quot;Use the F.log_softmax on dim=1 to normalize output!&quot;) . At depth 6, with a maximum sequence length of 512, this transformer achieves an accuracy of about 85%, competitive with results from RNN models, and much faster to train. To see the real near-human performance of transformers, we‚Äôd need to train a much deeper model on much more data. More about how to do that later. . Text generation transformer . Let&#39;s move on! . In Text Generation we are not allowed to know the future during training, how else are we going to predict it? This means that we need to use an autoregressive model. . For Text Generation we&#39;ll train a character-to-character prediction, the input is a sequence of characters and output is the input shifted one charafter to the left. . . For the usual RNN this is all that is needed, but as mentioned now we need to make our model autoregressive, meaning that it can&#39;t look ahead. This is done by applying a mask which disables all elements that are ahead of current index, as in image below. . . Masking the self attention, to ensure that elements can only attend to input elements that precede them in the sequence. Note that the multiplication symbol is slightly misleading:we actually set the masked out elements (the white squares) to ‚àí‚àû . In PyTorch . Implementing this in PyTorch . class SelfAttentionAutoRegressive(nn.Module): def __init__(self, k, heads=8): super().__init__() self.k, self.heads = k, heads # These compute the queries, keys and values for all # heads (as a single concatenated vector) self.tokeys = nn.Linear(k, k * heads, bias=False) self.toqueries = nn.Linear(k, k * heads, bias=False) self.tovalues = nn.Linear(k, k * heads, bias=False) # This unifies the outputs of the different heads into # a single k-vector self.unifyheads = nn.Linear(heads * k, k) def forward(self, x): b, t, k = x.size() h = self.heads queries = self.toqueries(x).view(b, t, h, k) keys = self.tokeys(x) .view(b, t, h, k) values = self.tovalues(x) .view(b, t, h, k) # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory keys = keys.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries.transpose(1, 2).contiguous().view(b * h, t, k) values = values.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries / (k ** (1/4)) keys = keys / (k ** (1/4)) # - get dot product of queries and keys, and scale dot = torch.bmm(queries, keys.transpose(1, 2)) # === START OF CHANGES === indices = torch.triu_indices(t, t, offset=1) dot[:, indices[0], indices[1]] = raise TODO(&quot;-inf; and think off what we are doing. triu_indices is shown below&quot;) # === END OF CHANGES === # - dot has size (b*h, t, t) containing raw weights dot = F.softmax(dot, dim=2) # - dot now contains row-wise normalized weights # apply the self attention to the values out = torch.bmm(dot, values).view(b, h, t, k) # swap h, t back, unify heads out = out.transpose(1, 2).contiguous().view(b, t, h * k) return self.unifyheads(out) . ??torch.triu_indices . Once the model has been &#39;handicapped&#39; as this we&#39;re ready to go! . If you&#39;d like to learn the historical aspect of Transformers, some design consideration &amp; more details please visit the original blog by Peter Bloem and finish that one! . I hope you found this helpful! Please take a good look in Appendix for demos with Text Classification &amp; Text Generation. Do note that the depth should probably be increased as should the amount of data. Transformers improve a lot with time, depth &amp; data (especially data!). . Thanks ~Hampus . Appendix: Demos (Text Classification &amp; Text Generation) . This appendix includes one demo of each type where I&#39;ve integrated the PyTorch with fast.ai to smoothen the process from model to actually using it. . Dependencies . Simply run this block to . Upgrade fastai (2.0+) | Import fastai | . %%capture !pip install -U fastai from fastai.text.all import * from functools import partial # Can use partial to preload a transformer block . Text Classification . The Text Classification is done on the IMDB challenge. There&#39;s code included for both IMDB_SAMPLE and IMDB where the sample is a little too small to actually get good stats on. . I think adding some more depth (perhaps 6 blocks?) will increase the accuracy even further. . Model (Classifier) . Code we already wrote for the classifier . class SelfAttention(Module): def __init__(self, k, heads=8): self.k, self.heads = k, heads # These compute the queries, keys and values for all # heads (as a single concatenated vector) self.tokeys = nn.Linear(k, k * heads, bias=False) self.toqueries = nn.Linear(k, k * heads, bias=False) self.tovalues = nn.Linear(k, k * heads, bias=False) # This unifies the outputs of the different heads into # a single k-vector self.unifyheads = nn.Linear(heads * k, k) def forward(self, x): b, t, k = x.size() h = self.heads assert self.k == k, f&#39;Input embedding dim ({e}) should match layer embedding dim ({self.emb})&#39; queries = self.toqueries(x).view(b, t, h, k) keys = self.tokeys(x) .view(b, t, h, k) values = self.tovalues(x) .view(b, t, h, k) # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory keys = keys.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries.transpose(1, 2).contiguous().view(b * h, t, k) values = values.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries / (k ** (1/4)) keys = keys / (k ** (1/4)) # - get dot product of queries and keys, and scale dot = torch.bmm(queries, keys.transpose(1, 2)) assert dot.size() == (b*h, t, t) # - dot has size (b*h, t, t) containing raw weights dot = F.softmax(dot, dim=2) # - dot now contains row-wise normalized weights # apply the self attention to the values out = torch.bmm(dot, values).view(b, h, t, k) # swap h, t back, unify heads out = out.transpose(1, 2).contiguous().view(b, t, h * k) return self.unifyheads(out) . class TransformerBlock(Module): def __init__(self, k, heads): super().__init__() self.attention = SelfAttention(k, heads=heads) self.norm1 = nn.LayerNorm(k) self.norm2 = nn.LayerNorm(k) self.ff = nn.Sequential( nn.Linear(k, 4 * k), nn.ReLU(), nn.Linear(4 * k, k) ) def forward(self, x): attended = self.attention(x) x = self.norm1(attended + x) fedforward = self.ff(x) return self.norm2(fedforward + x) . class Transformer(Module): def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes, device): super().__init__() self.device = device self.seq_length = seq_length self.num_tokens = num_tokens self.token_emb = nn.Embedding(num_tokens, k) self.pos_emb = nn.Embedding(seq_length, k) # The sequence of transformer blocks that does all the # heavy lifting tblocks = [TransformerBlock(k=k, heads=heads) for x in range(depth)] self.tblocks = nn.Sequential(*tblocks) # Maps the final output sequence to class logits self.toprobs = nn.Linear(k, num_classes) def forward(self, x): &quot;&quot;&quot; :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary). :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes). &quot;&quot;&quot; # generate token embeddings # x = x.split(&#39; &#39;) x = x[:,:self.seq_length,] tokens = self.token_emb(x) b, t, k = tokens.size() # print(b, t, k) # generate position embeddings positions = torch.arange(t, device=self.device) positions = self.pos_emb(positions)[None, :, :].expand(b, t, k) x = tokens + positions x = self.tblocks(x) # Average-pool over the t dimension and project to class # probabilities x = self.toprobs(x.mean(dim=1)) return F.log_softmax(x, dim=1) . The Text Classification . Data Collection | Training Loop | Validation | . path = untar_data(URLs.IMDB) path.ls() . (#7) [Path(&#39;/root/.fastai/data/imdb/README&#39;),Path(&#39;/root/.fastai/data/imdb/unsup&#39;),Path(&#39;/root/.fastai/data/imdb/train&#39;),Path(&#39;/root/.fastai/data/imdb/test&#39;),Path(&#39;/root/.fastai/data/imdb/tmp_lm&#39;),Path(&#39;/root/.fastai/data/imdb/tmp_clas&#39;),Path(&#39;/root/.fastai/data/imdb/imdb.vocab&#39;)] . # df = pd.read_csv(path/&#39;texts.csv&#39;);df.head(2) # dls = TextDataLoaders.from_df(df, path=path, text_col=&#39;text&#39;, label_col=&#39;label&#39;, valid_col=&#39;is_valid&#39;) . /usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . db = DataBlock( blocks = (TextBlock.from_df(df, max_vocab=10_000, seq_len=256), CategoryBlock), get_items=get_text_files, splitter=GrandparentSplitter(valid_name=&#39;test&#39;), get_y=parent_label ) dls = db.dataloaders(path, path = path) . dls.show_batch(max_n=2) . text category . 0 xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review n n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , xxunk bowl of xxunk . xxmaj it &#39;s warm and gooey , but you &#39;re not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n&#39;t quite feel right . xxmaj victor xxmaj vargas suffers from a certain xxunk on the director &#39;s part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an xxunk storyline would make the film critic proof . xxmaj he was right , but it did n&#39;t fool me . xxmaj raising xxmaj victor xxmaj vargas is | negative | . 1 xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there &#39;s just no getting around that , and it &#39;s hard to actually put one &#39;s feeling for this film into words . xxmaj it &#39;s not one of those films that tries too hard , nor does it come up with the xxunk possible scenarios to get the two protagonists together in the end . xxmaj in fact , all its charm is xxunk , contained within the characters and the setting and the plot ‚Ä¶ which is highly believable to xxunk . xxmaj it &#39;s easy to think that such a love story , as beautiful as any other ever told , * could * happen to you ‚Ä¶ a feeling you do n&#39;t often get from other romantic comedies | positive | . len(dls.vocab[0]),dls.device,dls.one_batch()[0].size() . (10008, device(type=&#39;cuda&#39;, index=0), (64, 3345)) . learn = Learner(dls, Transformer(k=256, heads=8, depth=4, seq_length=256, num_tokens=len(dls.vocab[0]), num_classes=2, device=dls.device), metrics=[accuracy]) . learn.lr_find() . SuggestedLRs(lr_min=5.754399462603033e-05, lr_steep=0.6309573650360107) . learn.fit_one_cycle(6, 5.7e-5) # We can increase depth &amp; more to improve the result . epoch train_loss valid_loss accuracy time . 0 | 0.652948 | 0.627165 | 0.656120 | 04:00 | . 1 | 0.580878 | 0.548993 | 0.722400 | 04:08 | . 2 | 0.481650 | 0.492818 | 0.760320 | 04:09 | . 3 | 0.458933 | 0.472389 | 0.770200 | 04:09 | . 4 | 0.432385 | 0.461327 | 0.778800 | 04:10 | . 5 | 0.409607 | 0.460612 | 0.780320 | 04:08 | . Fast.AI approach . learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) learn.fine_tune(4, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.624490 | 0.643156 | 0.625000 | 00:12 | . epoch train_loss valid_loss accuracy time . 0 | 0.441161 | 0.550412 | 0.755000 | 00:27 | . 1 | 0.380404 | 0.602693 | 0.650000 | 00:27 | . 2 | 0.316619 | 0.483822 | 0.750000 | 00:27 | . 3 | 0.256229 | 0.535840 | 0.750000 | 00:27 | . learn.show_results() . Text Generation . First of we generate text based on the IMDB dataset, but then we&#39;ve also got the shakespeare txt file afterwards for personal testing :) . The Model . Code we&#39;ve done, the essential model . class SelfAttentionAutoRegressive(Module): def __init__(self, k, heads=8): self.k, self.heads = k, heads # These compute the queries, keys and values for all # heads (as a single concatenated vector) self.tokeys = nn.Linear(k, k * heads, bias=False) self.toqueries = nn.Linear(k, k * heads, bias=False) self.tovalues = nn.Linear(k, k * heads, bias=False) # This unifies the outputs of the different heads into # a single k-vector self.unifyheads = nn.Linear(heads * k, k) def forward(self, x): b, t, k = x.size() h = self.heads queries = self.toqueries(x).view(b, t, h, k) keys = self.tokeys(x) .view(b, t, h, k) values = self.tovalues(x) .view(b, t, h, k) # - fold heads into the batch dimension ... contiguous = reshapes matrix in memory keys = keys.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries.transpose(1, 2).contiguous().view(b * h, t, k) values = values.transpose(1, 2).contiguous().view(b * h, t, k) queries = queries / (k ** (1/4)) keys = keys / (k ** (1/4)) # - get dot product of queries and keys, and scale dot = torch.bmm(queries, keys.transpose(1, 2)) indices = torch.triu_indices(t, t, offset=1, device=&#39;cuda&#39;) # OBS this also changed dot[:, indices[0], indices[1]] = float(&#39;-inf&#39;) # - dot has size (b*h, t, t) containing raw weights dot = F.softmax(dot, dim=2) # - dot now contains row-wise normalized weights # apply the self attention to the values out = torch.bmm(dot, values).view(b, h, t, k) # swap h, t back, unify heads out = out.transpose(1, 2).contiguous().view(b, t, h * k) return self.unifyheads(out) . class TransformerBlock(Module): def __init__(self, k, heads): super().__init__() self.attention = SelfAttentionAutoRegressive(k, heads=heads) self.norm1 = nn.LayerNorm(k) self.norm2 = nn.LayerNorm(k) self.ff = nn.Sequential( nn.Linear(k, 4 * k), nn.ReLU(), nn.Linear(4 * k, k) ) def forward(self, x): attended = self.attention(x) x = self.norm1(attended + x) fedforward = self.ff(x) return self.norm2(fedforward + x) . class Transformer(Module): def __init__(self, k, heads, depth, seq_length, num_tokens, device): super().__init__() self.device = device self.seq_length = seq_length self.num_tokens = num_tokens self.token_emb = nn.Embedding(num_tokens, k) self.pos_emb = nn.Embedding(seq_length, k) # The sequence of transformer blocks that does all the # heavy lifting tblocks = [TransformerBlock(k=k, heads=heads) for x in range(depth)] self.tblocks = nn.Sequential(*tblocks) # Maps the final output sequence to class logits self.toprobs = nn.Linear(k, num_tokens) def forward(self, x): &quot;&quot;&quot; :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary). :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes). &quot;&quot;&quot; # generate token embeddings # print(x.size()) tokens = self.token_emb(x) b, t, k = tokens.size() # print(b, t, k) # generate position embeddings positions = torch.arange(t, device=self.device) positions = self.pos_emb(positions)[None, :, :].expand(b, t, k) x = tokens + positions x = self.tblocks(x) # Average-pool over the t dimension and project to class # probabilities x = self.toprobs(x.view(b*t, k)).view(b, t, self.num_tokens) # print(x.size()) return F.log_softmax(x, dim=2) . The Text Generation . Data Collection | Training Loop | Validation | . path = untar_data(URLs.IMDB_SAMPLE) path.ls() . (#2) [Path(&#39;/root/.fastai/data/imdb_sample/texts.csv&#39;),Path(&#39;/root/.fastai/data/imdb_sample/models&#39;)] . df = pd.read_csv(path/&#39;texts.csv&#39;) . df.head(1) . label text is_valid . 0 negative | Un-bleeping-believable! Meg Ryan doesn&#39;t even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff! | False | . dls = TextDataLoaders.from_df(df, path=path, text_col=&#39;text&#39;, is_lm=True, valid_col=&#39;is_valid&#39;, seq_len=256) . /usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . dls.show_batch(max_n=1) . text text_ . 0 xxbos i enjoyed this movie . xxmaj unlike like some of the xxunk up , xxunk trash that is passed off as action movies , xxmaj playing xxmaj god is simple and realistic , with characters that are believable , action that is not over the top and enough twists and turns to keep you interested until the end . n n xxmaj well directed , well acted and a good story . xxbos xxmaj for those fans of xxmaj laurel and xxmaj hardy , the 1940s and beyond were a very sad time for the team . xxmaj their xxunk with xxmaj xxunk xxmaj xxunk xxmaj studios had xxunk and now they were &quot; free xxunk to work for any studio who xxunk them a job . xxmaj unfortunately , xxmaj fox , xxup xxunk , xxup xxunk ( without xxmaj xxunk ) and even a xxmaj french film company | i enjoyed this movie . xxmaj unlike like some of the xxunk up , xxunk trash that is passed off as action movies , xxmaj playing xxmaj god is simple and realistic , with characters that are believable , action that is not over the top and enough twists and turns to keep you interested until the end . n n xxmaj well directed , well acted and a good story . xxbos xxmaj for those fans of xxmaj laurel and xxmaj hardy , the 1940s and beyond were a very sad time for the team . xxmaj their xxunk with xxmaj xxunk xxmaj xxunk xxmaj studios had xxunk and now they were &quot; free xxunk to work for any studio who xxunk them a job . xxmaj unfortunately , xxmaj fox , xxup xxunk , xxup xxunk ( without xxmaj xxunk ) and even a xxmaj french film company who | . len(dls.vocab),dls.one_batch()[0].size() . (7080, (64, 256)) . learn = Learner(dls, Transformer(k=256, heads=8, depth=3, seq_length=256, num_tokens=len(dls.vocab), device=dls.device), loss_func=CrossEntropyLossFlat()) . learn.freeze() learn.lr_find() . SuggestedLRs(lr_min=0.02089296132326126, lr_steep=0.0014454397605732083) . learn.unfreeze() . learn.fit_one_cycle(4) # Add suggested LR if you&#39;d like learn.fit_one_cycle(4) learn.fit_one_cycle(4) . epoch train_loss valid_loss time . 0 | 5.952718 | 6.005916 | 00:09 | . 1 | 5.943147 | 5.950939 | 00:09 | . 2 | 5.907956 | 5.901570 | 00:09 | . 3 | 5.869049 | 5.870370 | 00:09 | . epoch train_loss valid_loss time . 0 | 5.782402 | 5.828668 | 00:09 | . 1 | 5.687460 | 5.575956 | 00:09 | . 2 | 5.571561 | 5.469945 | 00:09 | . 3 | 5.484975 | 5.450408 | 00:09 | . epoch train_loss valid_loss time . 0 | 5.322245 | 5.439701 | 00:09 | . 1 | 5.280037 | 5.311460 | 00:09 | . 2 | 5.215474 | 5.259928 | 00:09 | . 3 | 5.167902 | 5.247015 | 00:09 | . def predict(self, text, n_words=1, no_unk=True, temperature=1., min_p=None, no_bar=False, decoder=decode_spec_tokens, only_last_word=False): &quot;Return `text` and the `n_words` that come after&quot; idxs = idxs_all = self.dls.test_dl([text]).items[0].to(self.dls.device) if no_unk: unk_idx = self.dls.vocab.index(UNK) for _ in (range(n_words) if no_bar else progress_bar(range(n_words), leave=False)): with self.no_bar(): preds,_ = self.get_preds(dl=[(idxs[None],)]) # print(preds.size()) res = preds[0][-1] if no_unk: res[unk_idx] = 0. if min_p is not None: if (res &gt;= min_p).float().sum() == 0: warn(f&quot;There is no item with probability &gt;= {min_p}, try a lower value.&quot;) else: res[res &lt; min_p] = 0. if temperature != 1.: res.pow_(1 / temperature) idx = torch.multinomial(res, 1).item() idxs = idxs_all = torch.cat([idxs_all, idxs.new([idx])]) if only_last_word: idxs = idxs[-1][None] num = self.dls.train_ds.numericalize tokens = [num.vocab[i] for i in idxs_all if num.vocab[i] not in [BOS, PAD]] sep = self.dls.train_ds.tokenizer.sep return sep.join(decoder(tokens)) @delegates(Learner.get_preds) def get_preds(self, concat_dim=1, **kwargs): return super().get_preds(concat_dim=concat_dim, **kwargs) . predict(learn, &quot;I think its a very&quot;, n_words=20, temperature=1.) . &#39;i think its a very Who spoiler to sort with a flat in clumsy and my world to sit to american to were even&#39; . learn.save(&#39;gen.pkl&#39;) . !wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt . FastAI way . predict(language_model_learner(dls, AWD_LSTM), &quot;I think its&quot;, n_words=2, temperature=1.) .",
            "url": "https://blog.londogard.com/2021/02/17/_02_18_transformers_explained.html",
            "relUrl": "/2021/02/17/_02_18_transformers_explained.html",
            "date": " ‚Ä¢ Feb 17, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "TIL: GitPod - your editor in the cloud",
            "content": "TIL: GitPod - your editor in the cloud . GitPod.io is a editor in the cloud based on Theia which is a IDE built for the cloud &amp; desktop. It implements the same language servers and allow you to use the same extensions as in VS Code - which is great. . . For more about GitPod and how to combine it with languages such as Kotlin, Java &amp; more, read üëá. . What . GitPod.io from the start worked for the normal ‚Äúlow-barrier‚Äù languages such as JavaScript &amp; Python and still works great, and it really is where it excels. Lately though they‚Äôve stepped up and added more &amp; more targets with further customization. . You can now: . Customize your docker build by installing new packages (through brew or apt install). | Customize your extensions (majority, if not all, from VS Code available) | Add VNC (see native desktop programs) | Prebuilt containers (save a lot of time to not rebuild everything each time) | ‚Ä¶ &amp; more | . With a free account you get 50h / month of time to use this (as of 2021-01-22), which is very kind of GitPod! . How . First of all you need either a GitHub, GitLab or Bitbucket account. Once that is done you need to find a repository that you want to work on, it can be both a private or public repository. . Once you‚Äôve got your account &amp; repository set up then head to gitpod.io/#{Repository: e.g. https://github.com/londogard/snake-js-jvm-native} and it will open up. This will ask you to create a PR with some new config adding a new file called .gitpod.yml and further you can add .gitpod.Dockerfile which configures your Docker setup. . Setting up .gitpod.yml: . image: file: .gitpod.Dockerfile # OBS: only use this if you have a custom image # List the ports you want to expose and what to do when they are served. See https://www.gitpod.io/docs/config-ports/ ports: - port: 3000 onOpen: open-preview # List the start up tasks. You can start them in parallel in multiple terminals. See https://www.gitpod.io/docs/config-start-tasks/ tasks: - init: ./gradlew clean build vscode: extensions: - random.extension # (automatically added by adding extensions through interface). . Setting up .gitpod.Docker: . FROM gitpod/workspace-full-vnc # For NoVNC to work USER gitpod RUN sudo apt-get -q update &amp;&amp; sudo apt-get install -y libgtk-3-dev &amp;&amp; # For NoVNC to work sudo apt-get install -yq gcc-multilib &amp;&amp; sudo apt install -y libtinfo5 &amp;&amp; # For Kotlin Native to work sudo rm -rf /var/lib/apt/lists/* . With this setup you can push the changes to your repo and re-open the GitPod to reload a brand new Docker image with the updated packages. . This configuration allows you to do all I‚Äôve mentioned such as running Native Kotlin, run with VNC to view native application windows and more! By default you‚Äôre able to view a in-place/preview browser window so doing web development is working from the get-go and as such also Kotlin/JS worked pretty well without any configuration. . If you‚Äôre wondering, the NoVNC is on port 6080. . Why . For me there‚Äôs actually not just one reason to why I‚Äôd like to do this. . Be able to do some quick coding / reviewing on my tablet. | Workshops at AFRY (no installation required for users) | Just in a few days (2021-01-26) I‚Äôll do a workshop at AFRY with Kotlin Multiplatform where GitPod.io will be a great alternative for colleagues who have computers where they‚Äôre not allowed to install their own applications. . Alternatives . GitHub Codespaces is coming, but is still in a private BETA. | VS Code Remote (obs this actually require your own computer somewhere to host the vs code server) | . I think for now GitPod.io is the simplest and best out of them, unless you have your own computer to use as a server for VS Code Remote! . Thanks ~Hampus .",
            "url": "https://blog.londogard.com/jvm/native/kotlin/js/2021/01/21/til-gitpod-kotlin-jvm.html",
            "relUrl": "/jvm/native/kotlin/js/2021/01/21/til-gitpod-kotlin-jvm.html",
            "date": " ‚Ä¢ Jan 21, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "How to build and play Snake via Native Binary, JVM and JS/Browser (Kotlin)",
            "content": "A three part blog (all included in this one) that goes through . How Kotlin Multiplatform works (compiler and everything) | How to build a game (Snake) and finally | how to make it multiplatform. | All the code is available here. . It‚Äôs highly recommended using IntelliJ, a free (community) edition can be downloaded from jetbrains.com. It‚Äôs also required that your Kotlin version is above 1.4 (some huge Multiplatform changes was added in release 1.4). . Disclaimer: this post is pretty long and I recommend reading one part at a time (it‚Äôs 3 parts). . Personally I hate unfinished blogs that are multiple parts, hence I uploaded all at once. So be assured, you‚Äôre getting all parts - right here, right now! :happy: . Finally, the first part is purely informational about how everything works and the second part is how to actually code the game. The second part is interactive and contains a lot of TODOs. The third, and final, part covers how we are able to use the same code on JS/Browser &amp; Native . All the finished code is available here (open master if you wish to have the unfinished code). . Introduction . First off, what is Kotlin? . . Image from kotlinlang.org . By my own account it‚Äôs a language that has learned from many mistakes done in the past and tries to extend and embrace the good ones! . The most obvious one solved by Kotlin is ‚ÄúThe Billion Dollar Mistake‚Äù as the inventor Tony Hoare calls it himself, namely null. Kotlin is not alone about this, but certainly off to a good start! . Some mentionable features on top of this is . Coroutines - A more efficient (lightweight) threading model, also called ‚Äúgreen threads‚Äù sometimes. Feels very natural and easy. | Data-Science &amp; Jupyter support | Extension functions - Perhaps my favourite feature, do you feel a class is missing a function? No problem, you‚Äôre free to do so! | Excellent typing - Perhaps not Scala, but still very good. | &amp; more! | . All of this is available through Kotlins Multiplatform effort, where Multiplatform does not mean Mac/Windows/Linux but rather that we can compile into different platforms such as Java Bytecode (JVM), Native and JS/Browser. . Enough praises, let‚Äôs get onto how the multiplatform solution actually works through Part 1. . Part 1: How does Kotlin Multiplatform work? . Let‚Äôs begin by understanding exactly what Native is? From the landing page of kotlinlang.org/native. . Kotlin/Native is a technology for compiling Kotlin code to native binaries, which can run without a virtual machine. It is an LLVM based backend for the Kotlin compiler and native implementation of the Kotlin standard library. . This statement tells us a few things, such as native refering to binary executables that can run on a OS (natively) using no virtual machine or browser! But in practice? . ‚úîÔ∏è Small file size ‚úîÔ∏è No overhead ‚úîÔ∏è Incredibly fast starting-time . As usual it isn‚Äôt a win-win situation but you loose some . ‚ùåDevelopment speed . LLVM . LLVM is probably the biggest project (compiler) that exists to build native binaries. Languages such as C, C++, Haskell, Rust &amp; Swift compile into native binaries through LLVM. . From the info-box previously, . It is an LLVM based backend for the Kotlin compiler and native implementation of the Kotlin standard library. . So‚Ä¶ What is a backend? More specifically, what is a backend for a compiler? . How the Kotlin Compiler works, Frontend to Backend . A compiler is like a translator, just as you‚Äôd translate Swedish into English a compiler instead translates computer code written in one programming language into another one of lower level, e.g. assembly. . In general all compilers follow the same pattern, and Kotlin is no different. Even though it‚Äôs a similar path it‚Äôs interesting to learn about, even more if you don‚Äôt know how it usually works! The Kotlin Compiler first compiles Kotlin code into a Intermediate Representation, or IR, which it later turns into Java Bytecode, when targeting the Java Virtual Machine (JVM). . . The first part is called the Compiler Frontend . The Kotlin Compiler Frontend . . The Compiler Frontend turns the Kotlin code into a Intermediate Representation of the code which is represented by a abstract syntax tree. The abstract syntax tree in turn is built from concrete syntax, e.g. strings. The process involves lexical analysis which creates tokens and pass it forward to the parser that finally builds the abstract syntax tree. For those interested this could be a really fun challenge and learning lesson to implement yourself! . Moving on, the second and final part is called the Compiler Backend. . The Kotlin Compiler Backend . . The Compiler Backend turns this abstract syntax tree, or IR, into computer output language. In the image that is Java Bytecode which is understood by the JVM. The backend is the part that actually optimize code to remove for-loops where applicable, exchange variables into constants and so on. Just as with the frontend it‚Äôs a really good challenge to either implement a backend without optimizations, or focus on a existing one and adding a optimization yourself! . What is interesting about Kotlin is that it has different compiler backends, which means that the IR compile not only into Java Bytecode but also JS/Browser &amp; Native binaries. . . Side-note: For Native Backend there‚Äôs in fact two Intermediate Representations, first Kotlin IR which then compiles into LLVM IR. LLVM finally turns this into a native binary through its own Compiler Backend. During the final step in LLVM all the optimizations applied to C, C++, Swift &amp; many more is also applied to Kotlin Native code! . How Kotlin keeps multiplatform clean . It might sound messy to target multiple platforms like this, and how could it possibly end up clean? . By using the standard libraries that are included with Kotlin, which includes almost everything you need, and multiplatform-developed community libraries, e.g. SQLDelight, you get code that looks the same and works the same irregardless of target (JS/Browser, Native or the JVM). . To give an example of how Kotlin std-lib works, let‚Äôs take one of the most common types - String. . . By using Kotlin.String rather than the usual Java.lang.String you do when programming Java you get a type that works on multiple platforms, including some awesome convenience functions. Imagine, you can write native code using .substring, .take(n) &amp; .replace - amazing compared to c right? :happy: . To put this in context of the compiler, this means that the Compiler Backend automatically maps the IR of a Kotlin.String into the correct type. . You can take this concept and apply to anything such as IO , network &amp; more - all which are included in the std-lib! . Summarizing how Multiplatform works . Let‚Äôs recollect what we‚Äôve gone through . Kotlin Compiler compiles Kotlin code into a Intermediate Representation (IR) through the Compiler Frontend. IR is a abstract syntax tree. | | Kotlin Compiler then goes the Compiler Backend which turns the code into the lower level language, e.g. Java Bytecode, and applies multiple optimisations. | Kotlin has a std-lib which has functionalities as Kotlin.String, Kotlin.List, networking and much more. Kotlin.String turns into KString in the case of targeting Native. KString is Kotlins own native strings with some cool helper methods. | | Native is usually considered ‚Äúdangerous‚Äù and ‚Äúhard‚Äù because of all the quirks like pointers, address space and other. . Kotlin Native deals with memory allocation in the same way as Swift, namely reference counting which deallocates objects once they‚Äôve got no references. There‚Äôs some advantages such as being really fast, but also downsides such as reference cycles which it handles poorly when compared to the JVM Garbace Collector (GC). . Kotlin also has some really nice convenience syntax such as the memScope-block. . Outro: Kotlin Multiplatform and why it matters . ‚úîÔ∏è One code-base for common logic . Serialization logic, e.g. parsing JSON into a data class | Networking | Database | . ‚úîÔ∏è Development speed ‚úîÔ∏è Required Knowledge . ‚ùå Still requires some code in said language . Especially for UI | . So all in all we can share our code between platforms which improves development speed &amp; quality in multiple ways. . The biggest ‚Äúdownside‚Äù is that even though we share the code we most likely will need some kind of specific code for the platform, for the GUI on iOS as an example. Perhaps compose can help us get closer to that reality soon - who knows. The final, and perhaps obvious, one I‚Äôd like to mention straight away is that platform specific libraries of course are not usable on multiplatform. This includes libraries such as React (JS) &amp; ncurses (native). . Personally I see Kotlin Multiplatform as a great way to share core logic between different targets, but one must use it with care and not try to force it into being used everywhere in every way. . Part 2: How to set up Multiplatform and build Snake . OBS: Make sure to have Kotlin 1.4 or higher before starting the project. You can verify version &amp; update through File &gt; Settings &gt; Languages &amp; Frameworks &gt; Kotlin. There you see ‚Äúcurrent version‚Äù and if a newer one is available, make sure it‚Äôs at least 1.4 (any higher will work too)! . First we‚Äôll have to set up a Multiplatform project. The official guide is actually really good, and if you‚Äôre using IntelliJ it‚Äôs a breeze to setup! Just as in the guide make sure to select Library. OBS: For each target (JVM, JS &amp; Native) select ‚ÄúTemplate‚Äù and choose Application for each. . . Run the build. . Side-note: gradle is a really good build-tool that I‚Äôd like to discuss more. But for now let‚Äôs just enjoy the simplicity of how our whole project ‚Äòautomagically‚Äô just works and builds as expected while targeting different platforms. . Building a JVM App (Snake) . Let‚Äôs start simple, Keep It Simple Stupid (KISS) principle applied, and create a JVM app. JVM has multiple advantages while developing such as . Easy to run on all OS:es | Great debugging! | A ton of resources | First off, we need to draw something. This is easiest done through the Swing library which is included in the default jdk, some might call it old but hey - it does the job.Create a file called main.kt in src/jvmMain/kotlin. . Swing has a built-in threading solution (almost too bad, because Coroutines are awesome in Kotlin!) and the best way to start the GUI is by using the existing EventQueue class and its invokeLater function. invokeLater makes sure the code runs last in the EventQueue if you add more methods, which makes sense - you want to draw the UI as the final thing. . fun main() { EventQueue.invokeLater { JFrame().apply { title = &quot;Snake&quot; isVisible = true } } } . Where the apply is a context wrapper that takes the object and uses it as context (this) inside of the block/scope ({}). See its signature: . inline fun &lt;T&gt; T.apply(block: T.() -&gt; Unit): T . This would equate to . val jframe = JFrame() jframe.title = &quot;Snake&quot; jframe.isVisible = true . in more Java-like syntax. Why use apply? It allows us to achieve some interesting chaining concepts which I really enjoy. . Now run the main-function, there should be a green ‚Äúrun‚Äù-button at the left, press that. Hopefully it compiles and a window will appear, with the title set to ‚ÄúSnake‚Äù. . Awesome! We need to render something inside of the box, a game soon enough, let‚Äôs see how we can achieve that. . Adding some minor refactoring and some new classes we can draw something . class Board: JPanel() { init { TODO(&quot;&quot;&quot; - Set background to black - Allow focus - Set preferredSize to some 200x300 &quot;&quot;&quot;) } } class GUI: JFrame() { init { title = &quot;Snake&quot; isVisible = true isResizable = false setLocationRelativeTo(null) defaultCloseOperation = EXIT_ON_CLOSE TODO(&quot;Add the Board to the JFrame, through add()&quot;) } } fun main() { EventQueue.invokeLater { GUI() } } . What are we doing? JFrame was refactored GUI which then is a subclass of JFrame, with a few extra attributes added such as defaultCloseOperation = EXIT_ON_CLOSE that makes sure the program exits if we close the window, feel free to test it out! Further a Board was added which extends JPanel, it‚Äôs in the Board the game will be rendered. Finally, add(Board()) allows us to add our Board to the JFrame. . Run! Something is not right.. The background seems black enough, but the size is most likely not correct. We can‚Äôt even resize as isResizable=false was set. Make sure to add pack() at the end, as in . class GUI: JFrame() { init { { /** same code as before */ } add(Board()) pack() } } . What pack() does is that it packs and resizes the JFrame to include all its component(s) and their current size(s). . Super! We‚Äôre now able to render our Board and see the whole board. . Drawing the snake &amp; apple . We‚Äôve got the canvas (Board), now we just need to get artsy and add a Snake and some Apples! I‚Äôll keep it simple and will make the Board exist of a few cells, all pretty large. On each cell you either have nothing, Snake or Apple - pretty simple right? JPanel has some nice-to-have methods built-in, such as repaint() which simply repaints the component, which in turns calls paintComponent(g: Graphics?) to paint/render it. . Disclaimer: the code might not be the most idiomatic, but I try to introduce a few concepts. . class Board: JPanel() { init { /** same code as before */ } override fun paintComponent(g: Graphics?) { super.paintComponent(g) val g2d: Graphics2D = g as? Graphics2D ?: return g2d.scale(20.0, 20.0) g2d.color = Color.GREEN g2d.fill(Rectangle(5, 6, 1, 1)) g2d.fill(Rectangle(5, 7, 1, 1)) g2d.fill(Rectangle(5, 8, 1, 1)) } } . Once again, let‚Äôs dive into what‚Äôs happening ‚Äòunder the hood‚Äô. . First, we override the function paintComponent which renders Board layout. The input is a nullable Graphics, which is shown by the type having a ? at the end. This is a cool property of Kotlin, if something can be null it actually is a type. No Option/Maybe, just pure type. . Then Graphics? is cast to non-null Graphics2D through a safe approach using as?, without ? the cast can crash, with ? the cast would return null if failing. . Finally we use a elvis-expression ?: which is basically a wrapper for if (null) doThis else doThat, so if the left-hand-side is null it‚Äôll give the right-hand-side. The right-hand-side in our case is a empty return statement, meaning that we just make a early-exit. If the value is not null it‚Äôll give the non-null variant of the type! . Example use-case of elvis-operator ?: val a: Int = 1 ?: 0 // a = 1 val b: Int = null ?: 0 // b = 0 . Detailing the code further we now have g2d: Graphics2D where Graphics2D which gives us a few nice functions to draw components on the Board. . We set the scale to 20 This simplifies the behaviour, we can now use 20x30 grid where each cell is size 1, but it‚Äôs scaled into the 200x300 grid. | . | We use fill to draw Rectangle‚Äôs with set Color. | . Side-note: For those wondering how you safely execute on nulls by chaining, like you do with Monads (Option) . val nullableGraphics: Graphics? = null nullableGraphics?.scale(20.0, 20.0) // This is safe! No operation executed if null . Summing up, we now know how to render stuff on the Board and it‚Äôs all very static. The next step is to make the rendering less static and I believe the natural step from now is to create the data structures that‚Äôll contain the game &amp; its state. Then we can make sure the data structures are able to update, so we can render new states. . Creating the data structures . Data structures are required to have a game state, that is the score and position of everything. . The natural state is Game which contains everything, let‚Äôs begin by creating a Game structure which contains the size of the Board. . data class Game(val width: Int, val height: Int) . Side-note: A data class is essentially the same as a case class from Scala. And for those who don‚Äôt know what a case class is it‚Äôs basically a class that simplifies a lot of stuff, mainly used as a data structure. You get equals, getters &amp; setters, and much more for free. Anyone from Java knows how awesome this is. . Moving on we need to define the cells mentioned, something like . data class Cell(val x: Int, val y: Int) . Wrapping up our current state we got most of what we need, Game which contains our game state &amp; Cell which is our co-ordinates. The next step is to actually draw the Cell‚Äôs and wrap the Cell in other classes such as Apple and Snake. . Let‚Äôs add all the required code. . data class Apples(val cells: Set&lt;Cell&gt; = emptySet()) data class Snake(val cells: List&lt;Cell&gt;) { val head: Cell = TODO(&quot;Take the first cell.&quot;) val tail: List&lt;Cell&gt; = TODO(&quot;Drop one cell and return the rest.&quot;) } data class Game( val width: Int, val height: Int, val snake: Snake, // Adding snake and apples val apples: Apples ) class Board: JPanel() { private val game: Game = Game( 20, 30, Snake(listOf(Cell(2,3),Cell(2,4),Cell(2,5),)), Apples(setOf(Cell(4,5))) ) init { background = Color.black isFocusable = true preferredSize = Dimension(200, 300) } override fun paintComponent(g: Graphics?) { super.paintComponent(g) val g2d = g as? Graphics2D ?: return g2d.scale(20.0, 20.0) g2d.color = Color.GREEN game.snake.tail.forEach { cell -&gt; TODO(&quot;Render the cells using the previously used technique&quot;) } TODO(&quot;Render the head using the color YELLOW&quot;) TODO(&quot;Render the apples using the color RED&quot;) } } . Fixing the added TODOs and keeping the same GUI &amp; fun main we can now run the code. You should be seeing something like . . Pretty cool right!? We‚Äôve got ‚úîÔ∏è Rendering ‚úîÔ∏è Data Structures . What‚Äôs left? . A game loop | Ability to actually move the data structures ( Snake) | Adding a Direction . To be able to move we need to know what directions to move in. In my humble opinion this is simplest done through a Enum. . enum class Direction { UP, DOWN, LEFT, RIGHT } . Simple enough. But let‚Äôs make it better, even though pre-optimization is the root of all evil it is sometimes fun :grinning:. Enums in Kotlin are pretty awesome, they can both keep values and have methods! Let‚Äôs add dx and dy. . enum class Direction(val dx: Int, val dy: Int) { // --&gt; // | // v UP(0, -1), DOWN(0, 1), LEFT(-1, 0), RIGHT(1, 0); } . Through dx and dy we can add it to the current cell to move in the direction which Direction is! . Updating Snake.kt &amp; Cell.kt to have Cell with Direction and some turn. . data class Cell(val x: Int, val y: Int) { fun move(direction: Direction) = TODO(&quot;Create new cell which moves in direction. OBS: Remember Direction now has dx, dy!&quot;) } data class Snake( val cells: List&lt;Cell&gt;, val direction: Direction, // new attribute val eatenApples: Int = 0 // new attribute ) { fun move(): Snake { val newHead = TODO(&quot;Move head&quot;) val newTail = TODO(&quot;Move tail!&quot;) return TODO(&quot;Create a new Snake with the updated position!&quot;) } fun turn(newDirection: Direction?) = TODO(&quot;Make sure to turn correctly&quot;) } . This is all fine &amp; dandy, but there is some improvements to be made that‚Äôll clean up the code. I mentioned that Kotlin Enums can have methods, which is awesome. We can simplify the turn-logic by adding a method to Direction, namely isOppositeTo. See the code below. . enum class Direction(val dx: Int, val dy: Int) { /** Same code as previously */ fun isOppositeTo(that: Direction) = dx + that.dx == 0 &amp;&amp; dy + that.dy == 0 } . Right, we can now turn the snake and render the game. We need the Game-state to update to actually re-render the updated Snake, let‚Äôs add a update-function that does this. . fun update(direction: Direction?): Game { return TODO(&quot;&quot;&quot; Make sure to 1. Turn snake in direction 2. Move 3. Update the game state by returning Game &quot;&quot;&quot;) } . And our GUI . class Board: JPanel() { var dir: Direction = Direction.RIGHT var game: Game = Game( 20, 30, Snake(listOf(Cell(2,3),Cell(2,4),Cell(2,5),), dir), Apples(setOf(Cell(4,5))) ) init { addKeyListener(object : KeyAdapter() { override fun keyPressed(e: KeyEvent?) { dir = when (e?.keyCode) { VK_I, VK_UP, VK_W -&gt; Direction.UP else -&gt; TODO(&quot;Add the other key bindings. Reflect of how the object works and what is happening.&quot;) } game = game.update(dir) repaint() } }) } } . In our init (equal to a constructor) we add a keyListener which will listen on whenever we move. We moved game to be a var which allows us to change the reference. . Side-note: The difference between a val and var is not about immutability of the value, but rather that you cannot change the pointer to the object. By using val the compiler don‚Äôt allow you to change the reference. . val a = 1 a = 3 // CRASH -- This is not allowed var b = 1 b = 3 // b = 3, this is allowed. . Please note that this means that if your object is mutable, you can mutate the state of the object even though it‚Äôs a val. . Why put game on a var you might ask? Otherwise how would we update our Game as the data structure itself is ‚Äúimmutable‚Äù, i.e. cannot be changed, which would mean that we‚Äôd need to add a new Game object each time and save it on the stack (never cleaning it up) and that‚Äôd pretty fast make the application crash because of out of memory. . Finally, we update the game by calling our created update-method and then we use repaint() which draws the components! . Remember: paintComponent draws the canvas (game), so whenever repaint is called paintComponent draws the game again based on the game and cell‚Äôs in the game. . In conclusion this gives us an incredibly simple game, the snake moves whenever we press a key as we still don‚Äôt have a game loop based on time. So how do we add a game loop based on time? . The JVM got you covered! In the keyListener remove the update &amp; repaint, then add a timer . fixedRateTimer(TODO(&quot;Explore options to use for the timer and how they work&quot;)) { TODO(&quot;Insert a game loop here, essentially the same as done in the keyListener previously!&quot;) } . Run the game! ‚Ä¶amazing right? . We now move our snake, and it moves by itself if we don‚Äôt. But the game is still pretty boring‚Ä¶ We never die, no apples can be eaten and finally no new apples appear. We have a few additions to make to make the game a bit challenging.. . Let‚Äôs start by fixing the apples. Update the Apples.kt to randomly add apples to the board when calling grow(). . To simplify the logic we use a set which means that all apples added are unique. . data class Apples( val width: Int, val height: Int, val cells: Set&lt;Cell&gt; = emptySet(), val growthSpeed: Int = 3, // this could actually be to only spawn apple when there is no other apple. Up to user val random: Random = Random // Once again, Kotlin provides a superb class, in this case a Random wrapper that works on JVM, JS &amp; Native - cool right? ) { fun grow(): Apples { return TODO(&quot;&quot;&quot; If we have a random number greater than growthSpeed, return no update. Otherwise add a new cell. &quot;&quot;&quot;) } } . Then we should allow the Snake to eat them, make sure to add eat(apples: Apples) method and implement it for Snake.kt. . fun eat(apples: Apples): Pair&lt;Snake, Apples&gt; { return TODO(&quot;&quot;&quot; If our head is on a Apple location, return a pair of Snake and Apple untouched. Otherwise make sure to remove the apple from apples and increase body size of snake! &quot;&quot;&quot;) } . At the end of all this we need the Game.kt to allow this logic to be used. This is done through updating update to allow the snake to eat apples and also grow apples to add new ones. . Great! We can eat apples, add new apples and all. But we‚Äôre still pretty invincible and we‚Äôll just keep going forever. We need to make sure that the end can be lost, let‚Äôs do it by adding a new attribute isOver to Game.kt . val score: Int = TODO(&quot;Score based on snakes size, e.g. cell size&quot;) val isOver: Boolean = TODO(&quot;Game is over if snake head in tail or snake head not on the map!&quot;) fun update(dir: Direction): Game { if (isOver) return this { /** same code as was here before */ } } . Wrapping up the code with some minor refactoring / new functionality . Kotlin has a wonderful concept of extension functions, which simply is incredible. An extension function extends a class with new functionality. Did you ever wish Double had a rounding to string? fun Double.roundTo(n: Int): String = &quot;%.${decimals}f&quot;.format(this) solves this for you! Now your Double‚Äôs automatically gives you a hint to use .roundTo as one of Double‚Äôs built-in functions! . With these we can update our main-method to be a tiny bit cleaner. . g2d.color = Color.GREEN game.snake.tail.forEach { cell -&gt; g2d.fill(Rectangle(cell.x, cell.y, 1, 1)) } // Turns into --&gt; fun Graphics2D.renderCells(color: Color, cells: Iterable&lt;Cell&gt;) { this.color = color cells.forEach { cell -&gt; fill(Rectangle(cell.x, cell.y, 1, 1)) } } /** Which allows us to just call `g2d.renderCells(Color.GREEN, game.snake.tail)` etc. */ . What more improvements can be made? . Exercises left for the reader: . Add Score on the loosing screen | Add a win-condition (basically impossible, but taking all apples) | Reinforcement learning to train a bot (might be a future blog!) | Better &amp; cleaner code! | Part 3: True multiplatform (moving to JS &amp; Native) . I‚Äôll begin by saying that this part is more of a reader exercise. If you want the finished code please go to the GitHub repository. The idea is that this part will be solved by yourself during the workshop this is intended for. . All the snake-related code that isn‚Äôt in your main.kt-file should be moved into src/commonMain/kotlin which makes it multiplatform-code. This means that it can target JS, Native &amp; JVM instantly! . Side-note: because all the functionality for the Data Structures (e.g. take, List, Random) exists in Kotlin std-lib it‚Äôs automatically possible to use in multiplatform. . This is not true all the time, if we use platform-specific code. Our platform-specific code, tied with the JVM, is the timer and Swing which means that our whole GUI is tied to the JVM. . When the code has been migrated and import-paths are updated, run the JVM app again and validate that everything works. . JS/Browser target . Now create src/jsMain/kotlin/main.kt. . In this file we need to define how to draw the browser-based GUI. Some key methods, for the full code check out the git repository. . KeyListener document.onkeydown = { event -&gt; onkeydown(event).also { keyDir -&gt; dir = keyDir } } where onkeydown is your own method that handles key-events. . Timer window.setInterval({ game = game.update(dir); render(canvas, game) }, 200) . Canvas . val canvas = document.getElementById(&quot;snake-canvas&quot;) as HTMLCanvasElement val ctx = canvas.getContext(&quot;2d&quot;) as CanvasRenderingContext2D . On this ctx from the canvas you can use fillRect to draw rectangles, and fillStyle to set color. . HTML-Canvas &lt;canvas id=&quot;snake-canvas&quot; width=&quot;400px&quot; height=&quot;300px&quot;&gt;&lt;/canvas&gt; (put in index.html) . The game is run through ./gradlew jsBrowserRun, or selecting the gradle-icon at the top right (elephant) and typing jsBrowserRun. . And the code for the GUI using these components is pretty much exactly the same as in Swing to be honest. . Congratulations, you have now achieved creating a desktop game and a browser game! . Native target . And onto our final target, Native Binary, that runs completely without a browser or a virtual machine. . For the Native target the GUI will be supported through the library ncurses which unfortunately is only supported on Linux &amp; MacOS. If you‚Äôve windows you can solve this through Windows Subsystem for Linux (WSL). . Begin by creating src/nativeMain/kotlin/main.kt. . To begin, in the main-function do the following: . fun maint(): Unit = memScoped { // insert code } . The memScoped part means that all memory allocated in the block is automatically disposed at the end, incredibly useful! :happy: . Then reading how to use ncurses we can figure out how to init this. The final end-goal being . fun main(): Unit = memScoped { initscr() defer { endwin() } noecho() curs_set(0) halfdelay(2) var game: Game = TODO() val window = newwin(game.height + 2, game.width + 2, 0, 0)!! defer { delwin(window) } var input = 0 while (input.toChar() != &#39;q&#39;) { window.draw(game) input = wgetch(window) val direction = when (input.toChar()) { &#39;i&#39; -&gt; TODO(&quot;&quot;) } game = game.update(direction) } } private fun CPointer&lt;WINDOW&gt;.draw(game: Game) { wclear(this) box(this, 0u, 0u) game.apples.cells.forEach { mvwprintw(this, it.y + 1, it.x + 1, &quot;.&quot;) } game.snake.tail.forEach { mvwprintw(this, it.y + 1, it.x + 1, &quot;o&quot;) } game.snake.head.let { mvwprintw(this, it.y + 1, it.x + 1, &quot;Q&quot;) } if (game.isOver) { mvwprintw(this, 0, 6, &quot;Game Over&quot;) mvwprintw(this, 1, 3, &quot;Your score is ${game.score}&quot;) } wrefresh(this) } . Try running it in the terminal. . This blog was created as a companion to a workshop I‚Äôm gonna do at AFRY, it has a bit more content including a presentation in person. . All the finished code is available here, if you prefer the unfinished code head to master-branch. . Thanks! . ~Hampus .",
            "url": "https://blog.londogard.com/gradle/kotlin/game/multiplatform/2020/11/07/snake-kotlin-multiplatform.html",
            "relUrl": "/gradle/kotlin/game/multiplatform/2020/11/07/snake-kotlin-multiplatform.html",
            "date": " ‚Ä¢ Nov 7, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "TIL: SDKMan - The Software Development Kit Manager",
            "content": "SDKMan - Swapping JDK made simple . I‚Äôve decided to not only write blogs but also small snippets, here comes the first one. . What . SDKMan is a tool to install, set and swap JDK. SDKMan actually supports more than the Java JDK, among supported tooling is Java, Groovy, Scala, Kotlin and Ceylon. Ant, Gradle, Grails, Maven, SBT, Spark, Spring Boot, Vert.x and many others also supported. . It‚Äôs written in Bash, only requires curl &amp; zip/unzip. . So what SDKMan simplifies is . Installation of different JDKs, Gradle versions and so on | Swapping between JDKs | Allowing local (by folder basis) JDK-versions | How . We start by installation . Installation . If you need a more detailed guide go to this page. Downloading SDKMan $ curl -s &quot;https://get.sdkman.io &quot; | bash . Installing $ source &quot;$HOME/.sdkman/bin/sdkman-init.sh &quot; . Verification $sdk version - should return something along sdkman X.Y.Z . Usage . |What|Command|Comment| |‚Äî|‚Äî|‚Äî| |Install JDK|$sdk install java|Installs the latest stable version of Java JDK| |Install specific version|$sdk install scala 2.12.1|Install scala 2.12.1| |Install local version|$sdk install groovy 3.0.0-SNAPSHOT /path/to/groovy-3.0.0-SNAPSHOT|Installs a JDK you have locally to the SDKMan. The version name must be unique!| |Remove version|$sdk uninstall scala 2.11.6 |List candidates|$sdk list java|Lists all java candidates that are installable through SDKMan| |Use version|$sdk use scala 2.12.1|Use the version said, this only changes the current shell| |Default version|$sdk default scala 2.11.1|Changes version for all subsequent shells| |Current version|$sdk current|Lists all currently selected versions| . Remember to point your JDK to the ./sdkman/candidates/java/current path. Do the same for your IDE, such as IntelliJ-IDEA. . Why . I‚Äôve got different projects where I need to use different java versions. In one project I need JDK 14 to include jpackage and another one I‚Äôm forced to use JDK 8 (legacy system), to swap between these has never been simpler! . Alternatives . jEnv is a great alternative. According to some more JDK versions exists (haven‚Äôt checked myself), but overall it seems that SDKMan is the preferred alternative. Looking at GitHub one can clearly see that SDKMan is more popular, both by stars, latest commit and forks - which should be a decent enough to make a choice. . One thing I‚Äôve learned both through work and my personal projects is that often it‚Äôs better to make an non-optimal decision rather than trying to find the perfect solution, because diving into the pile of research to perfection will take much more time than just getting started. . -Hampus Lond√∂g√•rd .",
            "url": "https://blog.londogard.com/jvm/jdk/til/2020/09/04/til-sdkman.html",
            "relUrl": "/jvm/jdk/til/2020/09/04/til-sdkman.html",
            "date": " ‚Ä¢ Sep 4, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "TIL: fastutil - fast & compact type-speciic collections for JVM (no autobox!)",
            "content": "fastutil - how to optimize your collections by primitives . fastutil¬†extends the¬†Java‚Ñ¢ Collections Framework¬†by providing type-specific maps, sets, lists and queues with a small memory footprint and fast access and insertion . Homepage of fastutil . What . So what does the quote above actually mean? First we need to dive into, what is a Java Collection, and why are they &quot;bad&quot; for performance and memory requirements? . Java Collections (Collection&lt;E&gt;) only works with objects, meaning that if we have a List&lt;E&gt; which we populate with int it‚Äôll actually &quot;autobox&quot; the int into a Integer, i.e. the class, rather than the primitive type. What does this mean for you as a user? . It‚Äôs done through &quot;autoboxing&quot; which means automatic casting to the Integer-type, so nothing required for you | It allocates more memory than the primitive int. | . So how do you create an effective List that contains primitives, such as int, boolean and float? You can‚Äôt. What you can do is to create an array, int[], which will contain the actual primitives, no autoboxing applied. . But what if you want to have the methods from List, such as find and &quot;auto-resizing&quot;? Then you‚Äôll have to research and find a library, fastutil to the rescue! . How . fastutil implements their own versions of List, HashMap and so on which actually use the raw primitives, thereby increasing throughput while lowering memory used (as we‚Äôre not allocating as many objects anymore, when using primitives). . These types of libraries are only required once you hit an enormous amount of data or very strict requirement. . Installation . Using gradle . implementation: &#39;it.unimi.dsi:fastutil:8.4.1&#39; (latest version as of Aug 2020, mvnrepository) . Usage . DoubleToDoubleMap . val d2dMap: Double2DoubleMap = Double2DoubleOpenHashMap().apply { put(2.0, 5.5) put(3.0, 6.6) } assertEquals(5.5, d2dMap.get(2.0)) . This map is not only less memory-hungry (because using double rather than Double) but is also faster with insertions &amp; get, than the Java Collections counterpart. . Why . Less space used &amp; faster - it is as simple as that! . No &quot;AutoBoxing&quot; | No Object allocations for primitives | . Side-note Something I noticed while working on my Language Model in Kotlin, with some strict requirements and a lot of data, was that even when using fastutil I wasn‚Äôt gaining that much as I was mainly using views of my Lists, further optimizing memory. Views are what the name implies, a view of the List. It never creates a copy but just the indices and make use of the original structure. Using immutable data this is very effective, but if you‚Äôd been using mutable data it could prove dangerous as someone can change the structure and data of your view (even if your view is immutable the underlying List might not be). . Alternatives . Goldman Sachs Collection - now Eclipse Collections - Probably the best alternative, in my opinion. HPPC - Carrot Search Labs Trove4j - Not as active as other alternatives, but who cares when it‚Äôs performant and &quot;done&quot;? . Find a 2015 benchmark of the libraries here At least both fastutil&amp; Eclipse Collections are updated for Java 8 streams! . -Hampus Lond√∂g√•rd .",
            "url": "https://blog.londogard.com/jvm/til/optimization/2020/09/03/til-fastutil-primitive-structures.html",
            "relUrl": "/jvm/til/optimization/2020/09/03/til-fastutil-primitive-structures.html",
            "date": " ‚Ä¢ Sep 3, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "TIL: 'The Badass Runtime Plugin', jpackage & jlink - create a 'native' installable executable from your JVM-app that isn't huge",
            "content": "JPackage, JLink and how to pack a modern Java App . JPackage is a way to package a modern JVM-program as a installable binary, in a small format. . What . JPackage was finally included in the JDK by JDK-14, originally from the JavaFX-world (to bundle your desktop apps). JPackage combines itself with JLink, which builds upon ‚Äòproject jigsaw‚Äô, and together they form a way to create &quot;native&quot; binaries for JVM-projects. . What is JLink? . JLink is a way to assemble and optimize a set of modules and their dependencies into a custom runtime image (JRE). In other words we can take a ordinary JRE, ~200 MB, and chop it down to a total size of 25-40 MB for smaller project. . JLink is only possible thanks to ‚Äòproject jigsaw‚Äô which introduced modules and modularized the whole JRE starting from JDK-9. The Java standard library (stdlib) was modularized into 75 modules. As you might guess it is even better if your own code is also modularized, but not enforced. . What is JPackage . JPackage is the packaging suite that allows you to package your code, dependencies and the JLink-created JRE. I ended up with installation files, with a natively executable file on 60 MB for one of my smaller projects, which is really good in comparison to Electron! In comparison to a C-program this might not be amazing, but you‚Äôve to remember that this is completely cross-platform! . Side-note all sized discussed is without any major optimizations - and there exists a lot! Finally, if you exclude the JRE you can reach sizes of KB rather than MB! But excluding the JRE enforces the user to have it locally, which might not be good UX. . How . JPackage &amp; JLink is made easy thanks to The Badass Runtime Plugin or The Badass JLink Plugin where the latter require a modular project and the former works with any project! :happy: . Installation . Make sure you use &amp; target JDK 14 or higher, JPackage was first included in this version. I recommend SDKMan to install &amp; swap JDKs. . Then to add the Badass Runtime Plugin I recommend using gradle, which makes it as simple as the following. . plugins { ... id( &quot;org.beryx.runtime &quot;) version &quot;1.11.3 &quot; // latest version August 2020 ... } runtime { options.set(listOf( &quot;--strip-debug &quot;, &quot;--compress &quot;, &quot;2 &quot;, &quot;--no-header-files &quot;, &quot;--no-man-pages &quot;)) jpackage { installerType = &quot;deb &quot; // https://badass-runtime-plugin.beryx.org/releases/latest/ } } . This addition now creates the tasks required to build &amp; bundle your app. The options added make sure that you reduce the total size by a lot. I highly recommend reading the documentation, there‚Äôs so many incredibly useful options - I only provide the minimum! . Usage . By editing our building.gradle.kts to include everything from the Installation we can run the ./gradlew jpackage task to build our installer! . I want to note again, please make sure to read the homepage - a ton of optimizations and customization exist. There exists a lot of low hanging fruit for sure, so make sure to grab it! :wink: . Why . It‚Äôs really cool to see your JVM application installable using a .msi, .deb or even a .dmg while retaining a decent enough size. By using JPackage rather than GraalVM you make sure that you don‚Äôt loose anything in the form of performance or functionality. As a cherry on the top, it‚Äôs not just a executable file, but also includes a installer which is much better UX in my opinion. GraalVM will be discussed a bit more in Alternatives. . I want to re-iterate about the UX and size, which are the two main points of this. . We bundle a JRE with the JVM-app, allowing executables without requiring Java, of your version, to be installed on the user computer already. | The JRE is minified to only contain required modules, about 30-40 MB on a smaller project. | All required dependencies are bundled also | Installer which makes the whole JVM program really like any program on the computer | Basically a download, install run program that isn‚Äôt huge in size! | Alternatives . I see two alternatives that are worth mentioning . FAT-JAR / Uber-JAR / Shadow-JAR | GraalVM Native Image | &quot;Fat-JAR&quot; . A FAT-jar is a jar that bundles all dependencies and also includes a shell script, or .bat if Windows, to run the whole JVM-application. It‚Äôs pretty small in size, even though called FAT, as it doesn‚Äôt include a JRE to run the JVM. . This means that if your JVM-app requires Java 11 but the user only has Java 8 you need to have them download the JRE required, which sucks. . #### GraalVM The probably best alternative, it‚Äôs even smaller in size as SubstrateVM (their runtime) is really small and GraalVM allows AOT compile. . GraalVM has much faster startup-times than a JPackage program, but GraalVM is not as good when running for a long duration as there isn‚Äôt the incredibly good JIT from JVM. . I‚Äôd say something along the following - for long running apps choose JPackage, for lambda etc certainly choose GraalVM. . But GraalVM has further negatives, you can‚Äôt just code as you usually do. Reflection etc is not supported as usual, meaning there comes a lot of caveats using GraalVM. . Extra: I managed to end up with, after some minor trial-and-error, a binary file on ~ 12 MB for my file-sending program - pretty darn amazing! . I‚Äôll write more about GraalVM and its SubstrateVM which is used to create the native binaries in a new TIL. . -Hampus Lond√∂g√•rd .",
            "url": "https://blog.londogard.com/jvm/jdk/til/jpackage/jlink/2020/09/03/til-badass-runtime.html",
            "relUrl": "/jvm/jdk/til/jpackage/jlink/2020/09/03/til-badass-runtime.html",
            "date": " ‚Ä¢ Sep 3, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "CoViD-19 FAQ Search Engine 2.0",
            "content": "CoViD-19 FAQ Search Engine 2.0 . (Open in Google Colab here to run the code) . As promised here&#39;s a new improved (or is it?) FAQ Search Engine with some minor NLP-lessons added as we go, be ready to learn new (or old) things! Previously I added some requirements and I wish keep them, here they are as a refresher: . The end-product must be unsupervised No manually annotated data | No heuristic applied (i.e. understand the data and improve result by applying domain-specific knowledge on the task) | . | It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end) | Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!) | . These specifications adds a bit of spice, keep manual labour to a minimum at the same time as they prove a challenge that doesn&#39;t aim to achieve State of the Art but rather to be applicable and light! . With that in mind, let&#39;s move onwards! . Improvements to be done . In the previous blog &amp; notebook I first implemented a basic FAQ search based on finding the nearest neighbour from the embedded sentences, in the end I used Smooth Inverse Frequency Embeddings (A Simple but Tough-to-Beat Baseline for Sentence Embeddings) to embed the sentence which is an improvement from simply averaging the embeddings of the words in the sentence. . In the end I discussed some potential improvements which I wished to investigate. In this notebook I&#39;ll deliver these &quot;improvements&quot; based on grabbing some low hanging fruit. The total &quot;improvements&quot; to try out: . Lowercase | Better tokenization | Lemmatizing | Stop words | Ngram &amp; Custom Embeddings (will not be done because of time) | . To improve further I&#39;d say that either A) a lot of time to understand the data in depth and apply heuristics or B) a supervised approach, which in turn require labeled data (a.k.a sweet valued time). A larger dataset would also be helpful. All which I don&#39;t have currently. . Re-adding the old code . First I&#39;ll add the code from &quot;part one&quot; and it&#39;ll not be commented as it has been walked through. Further I&#39;ve removed the download &amp; parsing of FAQ, now the data is directly downloaded as a tsv-file allowing us to skip some libraries / code-cells. Some new dependencies are also added, e.g. stanza which is Stanfords new NLP-lib in Python (inspired by spaCy). . %%capture !pip install -U gensim !pip install -U fse !pip install stanza !pip install stop-words . import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from tqdm import tqdm import matplotlib.pyplot as plt tqdm.pandas() from pathlib import Path import os import random import operator import regex as re # gensim + fasttext from gensim.models.fasttext import FastText, load_facebook_vectors from gensim.models import KeyedVectors from stop_words import get_stop_words # stop-words from basically all languages import stanza from fse import IndexedList from fse.models import uSIF from fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH print(MAX_WORDS_IN_BATCH) print(FAST_VERSION) . 10000 1 . stanza.download(&#39;sv&#39;, logging_level=&#39;ERROR&#39;) print(&quot;OBS!! nPlease download the Swe fastText model &amp; the CoViD FAQ data from links in this code cell!&quot;) # Swe fastText reduced dimensions --&gt; https://drive.google.com/open?id=1vaWtiSlRAZ3XCdtnSce_6dwQ0T5x0OEJ # CoViD FAQ data --&gt; https://github.com/londogard/nlp-projects/blob/master/datasets/covid.tsv . OBS!! Please download the Swe fastText model &amp; the CoViD FAQ data from links in this code cell! . Loading all the models . This might take a little while, even though the dimensions are reduced the model is pretty large. . ft_wv = load_facebook_vectors(&#39;~/git/nlp-projects/models/cc.sv.100.bin&#39;) df = pd.read_csv(&#39;~/git/nlp-projects/datasets/covid.tsv&#39;, sep=&#39; t&#39;) nlp = stanza.Pipeline(lang=&#39;sv&#39;, processors=&#39;tokenize&#39;, logging_level=&#39;ERROR&#39;) model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) flatten = lambda l: [item for sublist in l for item in sublist] # Helper function to flatten a list . Going forward . Let&#39;s get on to adding our improvements . 1. Tokenization &amp; lower-case . The first and forthmost improvement is to lowercase the text and then tokenize it using a better method of tokenization. Let&#39;s take a look at how stanza helps us out by applying a much better tokenization. . q = &quot;Hej d√§r borta! Jag k√§nner igen dig, Johan&#39;s kompis? Eller √§r det Johannas?&quot; stanza_tokenize = lambda x: [token.text for sentence in nlp(x).sentences for token in sentence.tokens] prev = q.split() new = stanza_tokenize(q) print(f&quot;Previously: t{prev[:12]}..&quot;) print(f&quot;After: t t{new[:12]}..&quot;) . Previously: [&#39;Hej&#39;, &#39;d√§r&#39;, &#39;borta!&#39;, &#39;Jag&#39;, &#39;k√§nner&#39;, &#39;igen&#39;, &#39;dig,&#39;, &#34;Johan&#39;s&#34;, &#39;kompis?&#39;, &#39;Eller&#39;, &#39;√§r&#39;, &#39;det&#39;].. After: [&#39;Hej&#39;, &#39;d√§r&#39;, &#39;borta&#39;, &#39;!&#39;, &#39;Jag&#39;, &#39;k√§nner&#39;, &#39;igen&#39;, &#39;dig&#39;, &#39;,&#39;, &#39;Johan&#39;, &#34;&#39;&#34;, &#39;s&#39;].. . So, what are we looking at? Stanza handled our tokenization and increased the number of tokens, can this really be good!? Yes! Keep calm and don&#39;t jump the ship yet, the increased number of tokens will be followed by a decrease of unique tokens, and indirectly out of vocobulary (OOV) tokens. Unlike what we set out to do we still don&#39;t lower-case the output, this will follow later, now let me explain what the tokenization helps us achieve: . Punctuation, e.g. [!,?..], is tokenized into its own token. | Some compound words are split up, e.g. Johan&#39;s is now Johan, &#39;, s which is three (3) separate tokens rather than one. | Because of the updated tokenization fredag and fredag! is now tokenized as [fredag] and [fredag, !], this in fact turns fredag into the same token in both thus achieving the same vector when embedded which is great, because it really means the same. The exclamation mark itself also applies the same meaning to all places it&#39;s applied, which in itself is an improvement now also as we embed it separately. . Why is this good? Even though we see a direct increase in number of tokens we see a decrease of number of unique tokens because we now tokenize borta, borta?, &amp; borta! as the same token, with one additional for the punctuation in the two latter cases rather than 3 separate tokens which would map to different data. The coverage of our Word Embeddings also increase because we now tokenize the text better. Perhaps borta! does not exist but borta surely do exist in the embedding dictionary / lookup. . def test_dimensions(preprocessing=[stanza_tokenize]): prev = flatten(df[&#39;question&#39;].apply(lambda x: x.split()).tolist()) post = flatten(df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocessing)).tolist()) print(f&quot;Previously: {len(prev)} tokens ({len(set(prev))} unique)&quot;) print(f&quot;Post: {len(post)} tokens ({len(set(post))} unique)&quot;) print(f&quot;Token reduction by ~{100 * (1- len(set(post))/len(set(prev))):.1f} %&quot;) labels = [&#39;#Tokens&#39;, &#39;#Unique Tokens&#39;] width = 0.35 x = np.arange(len(labels)) fig, ax = plt.subplots() rects1 = ax.bar(x - width/2, [len(prev), len(set(prev))], width, label=&#39;Before&#39;) rects2 = ax.bar(x + width/2, [len(post), len(set(post))], width, label=&#39;After&#39;) ax.set_ylabel(&#39;Tokens&#39;) ax.set_title(&#39;Tokens before and after&#39;) ax.set_xticklabels(labels) ax.set_xticks(x) ax.legend() fig.tight_layout() plt.show() # preprocessing is a list of lambda functions to apply def preprocess(text, preprocessing): for f in preprocessing: text = f(text) return text . Let&#39;s take a look how much this actually mattered! . test_dimensions() . Previously: 629 tokens (289 unique) Post: 713 tokens (273 unique) Token reduction by ~5.5 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; The expectations set up has been achieved and we can clearly see that the raw number of tokens grew while the unique token count shrinked. Applying lower-case to the text will further reduce the number of unique tokens, and obviously keep the number of tokens at the same count. . Let&#39;s add lower-casing and see what happens! . lowercase = lambda x: x.lower() preprocess_funcs = [lowercase, stanza_tokenize] test_dimensions(preprocessing=preprocess_funcs) . Previously: 629 tokens (289 unique) Post: 712 tokens (260 unique) Token reduction by ~10.0 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Lower-casing . Going from 5.5 to 10 % reduction is nothing to sneeze at, by applying these two simple techniques we now have the same data in a better format which allows us to have a lower number of unique tokens. Pretty awesome right? . Let&#39;s get on with this and apply the preprocessing to the questions and test it out with the FAQ-search! . df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocess_funcs)) df[&#39;X&#39;].head() . 0 [vad, √§r, coronavirus, ?] 1 [vad, √§r, covid-19, ?] 2 [vad, skiljer, covid-19, fr√•n, s√§songsinfluens... 3 [vilka, √§r, symtomen, p√•, covid-19, ?] 4 [hur, vet, jag, om, mina, symtom, beror, p√•, p... Name: X, dtype: object . Testing the new input-data . Now that we&#39;ve created our input data we need to test our model on this! By applying the IndexedList which is the dataformat SFE wants as input we can train the model and then test it. . sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) . (75, 712) . def get_n_closest_questions(question, preprocessing, n=4): q_fixed = preprocess(question, preprocessing) resp = model.sv.similar_by_sentence(q_fixed, model=model, indexable=df[&#39;question&#39;].tolist()) # [([tokens], score)] resp = [f&#39;{result[2]:.2f}: {result[0]}&#39; for result in resp] print(&#39; n&#39;.join(resp[:n])) . get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocess_funcs) . 0.67: Kan man bli smittad av en person som har covid-19 men som inte har n√•gra symtom? 0.63: Kan covid-19 smitta mellan djur och m√§nniska och kan mitt husdjur smittas av viruset? 0.54: Kan viruset smitta till m√§nniska via post och paket? 0.42: Kan smitta √∂verf√∂ras fr√•n mygg till m√§nniska? . get_n_closest_questions(&quot;Hur f√•r jag min son att f√∂rst√•?&quot;, preprocess_funcs) . 0.82: Hur pratar man med barn om det nya coronaviruset? 0.80: Vad √§r covid-19? 0.78: Hur sjuk blir man av covid-19? 0.77: Hur l√§nge √§r man sjuk av covid-19? . 2. Lemmatization and Stop Words . Let&#39;s try to further improve this by actually lemmatizing and applying stop-words! . Lemmatization . So what is Lemmatization? Quoting Stanfords description: . For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set. . The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance: am, are, is =&gt; be car, cars, car&#39;s, cars&#39; =&gt; car . The result of this mapping of text will be something like: the boy&#39;s cars are different colors =&gt; the boy car be differ color . What is stop-words? . Stop-words are words we want to throw away as they add no real purpose. In older Machine Learning approaches it was way more important to add stop-words but in newer Deep Learning with Neural Networks stop-words often can be a negative thing, removing understanding of the sentence and perhaps minor differences which makes the world for understanding. . A example of a stop-word list could be [&quot;hej&quot;, &quot;vem&quot;, &quot;d√•&quot;, &quot;och&quot;, ...] which means that these words would be removed from a sentence. . In our case it makes sense to remove words like &#39;vad&#39;, &#39;varf√∂r&#39; and so on because the return of the FAQ seems to be very weighted towards these words. . nlp = stanza.Pipeline(lang=&#39;sv&#39;, processors=&#39;tokenize,mwt,pos,lemma&#39;, logging_level=&#39;ERROR&#39;) stanza_lemma = lambda x: [token.lemma for sentence in nlp(x).sentences for token in sentence.words] preprocess_funcs_lemma = [lowercase, stanza_lemma] print(f&#39;Previously: t{preprocess(&quot;hur f√∂rklarar jag f√∂r min dotter och son?&quot;, preprocess_funcs)}&#39;) print(f&#39;After: t t{preprocess(&quot;hur f√∂rklarar jag f√∂r min dotter och son?&quot;, preprocess_funcs_lemma)}&#39;) . Previously: [&#39;hur&#39;, &#39;f√∂rklarar&#39;, &#39;jag&#39;, &#39;f√∂r&#39;, &#39;min&#39;, &#39;dotter&#39;, &#39;och&#39;, &#39;son&#39;, &#39;?&#39;] After: [&#39;hur&#39;, &#39;f√∂rklara&#39;, &#39;jag&#39;, &#39;f√∂r&#39;, &#39;jag&#39;, &#39;dotter&#39;, &#39;och&#39;, &#39;son&#39;, &#39;?&#39;] . Some interesting notes Seeing &#39;min&#39; getting converted to &#39;jag&#39; is both good and bad, in this case we reduce dimensionality of the problem but we loose context and understanding. jag and min certainly does not mean the same thing. . Let&#39;s see how it pans out... . test_dimensions(preprocess_funcs_lemma) . Previously: 629 tokens (289 unique) Post: 712 tokens (228 unique) Token reduction by ~21.1 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; del model model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocess_funcs_lemma)) sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) . (75, 712) . get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocess_funcs_lemma) . 0.75: Kan covid-19 smitta mellan djur och m√§nniska och kan mitt husdjur smittas av viruset? 0.69: Hur smittar covid-19? 0.68: Kan man smittas flera g√•nger av det nya coronaviruset? 0.63: Smittar covid-19 via vatten och mat? . get_n_closest_questions(&quot;Hur f√•r jag min son att f√∂rst√•?&quot;, preprocess_funcs_lemma) . 0.79: Vad √§r covid-19? 0.75: Hur sjuk blir man av covid-19? 0.74: Hur l√§nge √§r man sjuk av covid-19? 0.66: Om en person i familjen √§r sjuk - m√•ste alla stanna hemma d√•? . Analyzing the results . Improvements? Not really, the model has an improved response to the &#39;hamster-question&#39; but it&#39;s way off when asking about the son. . Why? The most likely explanation is that even though we reduce the input dimensions an awful lot we remove dimensions that brings value, and removing value is bad - just as was touched upon previously. It might be helpful in some cases, perhaps this could prove helpful for a supervised approach such as TF-IDF + Support Vector Machine. . Any good parts? Yes, we can see some pretty hefty memory-requirement reductions when working with other types of models by applying this. Actually, in the case of this we could reduce the memory requirement by lemmatizing the dictionary of the embeddings and removing all non-lemmas. All in all, this could lead to a small performance loss but great memory win. . Stop words . As promised we shall apply stop-words, but as we saw no performance gain with lemmatization we&#39;ll keep the old tokenization. . stop_words = get_stop_words(&#39;sv&#39;) clean_stop = lambda x: [word for word in x if word not in stop_words] preprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop] del model model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don&#39;t need to remove stop-words in the sentences in our sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) preprocess(&quot;hur f√∂rklarar jag f√∂r min dotter och son?&quot;, preprocessing_func_stop) . [&#39;f√∂rklarar&#39;, &#39;dotter&#39;, &#39;son&#39;, &#39;?&#39;] . test_dimensions(preprocessing_func_stop) . Previously: 629 tokens (289 unique) Post: 417 tokens (206 unique) Token reduction by ~28.7 % . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocessing_func_stop) . 0.66: Kan man bli smittad av en person som har covid-19 men som inte har n√•gra symtom? 0.64: Kan covid-19 smitta mellan djur och m√§nniska och kan mitt husdjur smittas av viruset? 0.54: Kan viruset smitta till m√§nniska via post och paket? 0.41: Kan smitta √∂verf√∂ras fr√•n mygg till m√§nniska? . get_n_closest_questions(&quot;Hur f√•r jag min son att f√∂rst√•?&quot;, preprocessing_func_stop) . 0.83: Vad √§r covid-19? 0.83: Hur pratar man med barn om det nya coronaviruset? 0.80: Hur sjuk blir man av covid-19? 0.80: Hur l√§nge √§r man sjuk av covid-19? . Further analyzing . In my mind we&#39;ve some pretty good responses, in a way better and another way worse than lemmatizaton. Certainly not a set-back but neither a step forward. Testing different approaches and turning things on and off is a great way to increase data understanding and also gives a better sense of what different preprocessing functions actually does. In fact this is actually part of the most common Machine Learning development approach, working much like agile, which is iteratively circular and called CRISP-DM. I won&#39;t go deeply into CRISP-DM (already did once in my Master Thesis), but the following image gives you the gist. . Finally, as we see no great impact by applying either lemmatization nor stop-words we might just give up at the lower-case + stanza tokenization, but I&#39;d like to make one last shot in the dark - custom stop words! Let&#39;s see how it fares... . Custom Stop Words (breaking the rules) . So I decided to break the rules and create a small simple heuristic by applying custom stop words. Let&#39;s figure out which words we should remove using the following steps (which could in fact be automated)! . Find the most common words | Remove the ones which does not give any greater value | from collections import Counter df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocess_funcs)) counter = Counter(flatten(df[&#39;X&#39;].tolist())) . sorted(counter.items(), key=lambda item: item[1], reverse=True)[:15] . [(&#39;?&#39;, 75), (&#39;covid-19&#39;, 28), (&#39;vad&#39;, 25), (&#39;och&#39;, 22), (&#39;hur&#39;, 21), (&#39;f√∂r&#39;, 20), (&#39;det&#39;, 15), (&#39;kan&#39;, 14), (&#39;i&#39;, 14), (&#39;jag&#39;, 13), (&#39;av&#39;, 13), (&#39;g√§ller&#39;, 12), (&#39;som&#39;, 12), (&#39;√§r&#39;, 11), (&#39;en&#39;, 11)] . stop_words = [&#39;?&#39;, &#39;och&#39;, &#39;jag&#39;, &#39;i&#39;, &#39;√§r&#39;, &#39;en&#39;, &#39;min&#39;, &#39;?&#39;] clean_stop = lambda x: [word for word in x if word not in stop_words] preprocessing_func_stop = [lowercase, stanza_tokenize, clean_stop] del model model = uSIF(ft_wv, workers=4, lang_freq=&quot;sv&quot;) df[&#39;X&#39;] = df[&#39;question&#39;].apply(lambda x: preprocess(x, preprocessing_func_stop)) # We don&#39;t need to remove stop-words in the sentences in our sfe_format = IndexedList(df[&#39;X&#39;].tolist()) model.train(sfe_format) preprocess(&quot;hur f√∂rklarar jag f√∂r min dotter och son?&quot;, preprocessing_func_stop) . [&#39;hur&#39;, &#39;f√∂rklarar&#39;, &#39;f√∂r&#39;, &#39;dotter&#39;, &#39;son&#39;] . get_n_closest_questions(&quot;Hur f√•r jag min son att f√∂rst√•?&quot;, preprocessing=preprocess_funcs) . 0.83: Hur pratar man med barn om det nya coronaviruset? 0.83: Vad √§r covid-19? 0.80: Hur sjuk blir man av covid-19? 0.79: Hur l√§nge √§r man sjuk av covid-19? . get_n_closest_questions(&quot;kan min hamster bli smittad?&quot;, preprocessing=preprocess_funcs) . 0.66: Kan man bli smittad av en person som har covid-19 men som inte har n√•gra symtom? 0.63: Kan covid-19 smitta mellan djur och m√§nniska och kan mitt husdjur smittas av viruset? 0.54: Kan viruset smitta till m√§nniska via post och paket? 0.41: Kan smitta √∂verf√∂ras fr√•n mygg till m√§nniska? . Not bad, not amazing - I feel pretty happy about this. . So what can be done from now on if time and resources where available? . Add a classifier + TF-IDF | BERT / ALBERT QA (the State-of-the-Art right now) | . Thanks for this time, - Hampus Lond√∂g√•rd .",
            "url": "https://blog.londogard.com/jupyter/nlp/machine-learning/deep-learning/2020/08/01/faq-search-covid-2.html",
            "relUrl": "/jupyter/nlp/machine-learning/deep-learning/2020/08/01/faq-search-covid-2.html",
            "date": " ‚Ä¢ Aug 1, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "SQL - Different Abstraction Levels (& how I came to love SQLDelight)",
            "content": "SQL - different abstraction levels and how I came to love SQLDelight . In this blog I‚Äôll cover a few different abstraction levels of database access, focusing purely on SQL and not NoSQL / Reddis or anything like that. The purpose is to share the knowledge that there exist these types of abstractions and they do exist in all or at least most of the popular languages. . I‚Äôll try to move from &quot;raw SQL&quot; to the modern &quot;Object-Relational Mapping&quot;-style, a.k.a ORM. . In the end I wish to make a short piece leaving out a lot of details but maintaining a feel of each style and some pros/cons. I bet you already guessed my preferred approach straight from the title :wink:. . How to interact with a SQL Database from a programming language . Structured Query Language (SQL) is as the name, once spelled out, a Domain Specific Language (DSL) just like regex. It‚Äôs basically a programming language written to facilitate and simplify the experience with the underlying engine. By using a DSL you gain capabilities that would be natural to integrate with most languages, and it also makes the engine do the same with the same code across languages. . I think that Regex and SQL are the most famous DSLs and for good reason, having regex work (almost) the same across languages simplifies the guides and the same applies to SQL. . Going forward let‚Äôs see how we communicate with a SQL-db from a programming language like Java using their famous jdbc (Java Database Connectivity) which is the driver that communicates with the db. . try { System.out.println( &quot;Connecting to database... &quot;); conn = DriverManager.getConnection(DB_URL,USER,PASS); //STEP 4: Execute a query System.out.println( &quot;Creating statement... &quot;); stmt = conn.createStatement(); String sql; sql = &quot;SELECT id, first, last, age FROM Employees &quot;; ResultSet rs = stmt.executeQuery(sql); //STEP 5: Extract data from result set while(rs.next()){ //Retrieve by column name int id = rs.getInt( &quot;id &quot;); int age = rs.getInt( &quot;age &quot;); String first = rs.getString( &quot;first &quot;); String last = rs.getString( &quot;last &quot;); //Display values System.out.print( &quot;ID: &quot; + id); System.out.print( &quot;, Age: &quot; + age); System.out.print( &quot;, First: &quot; + first); System.out.println( &quot;, Last: &quot; + last); } //STEP 6: Clean-up environment rs.close(); stmt.close(); conn.close(); } catch (SQLException se) { .... . Not very convenient right? Personally I think this looks horrible, it‚Äôs filled with horrible getters &amp; setters like we‚Äôre stuck in the Middle Ages or something. Personally my mind directly flows to serialization and how that must work somehow with databases, and that‚Äôs right - we can move into the future today! . Moving one abstraction level up . Welcome Room &amp; slick (two libraries I‚Äôve experience with) to the room! Both of these libraries provide a type of serialization to classes and more convenient syntax to write the code. The first one heavily leans on annotation to make it work while the other one uses a more slick approach of &quot;copying&quot; the way you work with the standard Scala Collections (filter, map, flatMap, reduce etc). . I‚Äôd say that both do count as ORMs but they‚Äôre still not as abstract as other solutions such as peewee which we‚Äôll discuss later. Let‚Äôs get into Room and how it works. First you define entities like a class with the added annotation @Entity and then you define a Data Access Object (DAO) to interact with the table / object. The DAO is where you define your queries, let‚Äôs take a look. . @Dao interface UserDao { @Query( &quot;SELECT * FROM user &quot;) fun getAll(): List&lt;User&gt; @Query( &quot;SELECT * FROM user WHERE uid IN (:userIds) &quot;) fun loadAllByIds(userIds: IntArray): List&lt;User&gt; ... } . In my opinion this approach strikes a really good balance between simple-to-use but still powerful and very configurable because you still use SQL, a bonus here is that it‚Äôs safe from SQL-injection as you‚Äôre making use of so-called prepared-statements (wikipedia). The biggest drawback is that it‚Äôs hard to write easy-to-read SQL in the annotation and for the annotation-haters we‚Äôve a lot of annotations (which often slows down the compile-time noticeably among other things). . Moving on we‚Äôve slick which is also a really cool approach! slick allows you to this but instead you write your queries in something that feels like using the normal Scala Collection library. This allows you to use map, filter, reduce etc to create queries, and even for-comprehension. Let‚Äôs see! . // Read all coffees and print them to the console println( &quot;Coffees: &quot;) db.run(coffees.result).map(_.foreach { case (name, supID, price, sales, total) =&gt; println( &quot; &quot; + name + &quot; t &quot; + supID + &quot; t &quot; + price + &quot; t &quot; + sales + &quot; t &quot; + total) }) // Read coffee with price lower than 9 and join with matching supplier using for-comprehension val q2 = for { c &lt;- coffees if c.price &lt; 9.0 s &lt;- suppliers if s.id === c.supID } yield (c.name, s.name) // A find using filter def find(id: Int) = db.run( users .filter(_.id === id) .result.headOption ) . Pretty slick right? . Moving another level up (Python + Peewee) . Ok, maybe it‚Äôs not actually moving one level up from slick but I‚Äôd say it‚Äôs still a little bit further away from raw SQL as we make more use of objects, in the case of slick you can more easily see the generated SQL-code. Let‚Äôs take a look at peewee which supports most databases (sqlite, mysql, postgresql and cockroachdb). . So where do we begin? Create the database and tables! It‚Äôs done by initiating a database and then creating different classes which each maps to their own tables automatically. . db = SqliteDatabase(&#39;people.db&#39;) # create the db class Person(Model): name = CharField() birthday = DateField() class Meta: database = db # This model uses the &quot;people.db &quot; database. class Pet(Model): owner = ForeignKeyField(Person, backref=&#39;pets&#39;) name = CharField() animal_type = CharField() class Meta: database = db # this model uses the &quot;people.db &quot; database . And how would one create entries and then query them? It‚Äôs simply done through object creation as in the following examples. . uncle_bob = Person(name=&#39;Bob&#39;, birthday=date(1960, 1, 15)) uncle_bob.save() # Sometimes the class already has a &quot;create method &quot; as in Person.create(name=&#39;Sarah&#39;, birthday=date(1980, 10, 20)) # And create a pet which belongs to uncle_bob bob_dog = Pet.create(owner=uncle_bob, name=&#39;Doggy&#39;, animal_type=&#39;dog&#39;) . And to query the tables we also make use of the object fully, as in the following small example. . bobby = Person.select().where(Person.name == &#39;Bob&#39;).get() # or all persons! for person in Person.select(): print(person.name) . Now we‚Äôve gone through the different abstraction layers that you usually see available in most languages. Going forward I‚Äôd like to show SQLDelight which turns the abstraction a little bit upside down. . SQLDelight: Abstraction level left to the right . In SQLDelight I‚Äôd say we get the ideal balance of abstraction and configurability. We deal with raw SQL which is both a pro &amp; con, people will need to know SQL unlike in a abstracted ORM but you also get the full potential and it‚Äôs really simple to do complex joins (which is really messy in ORMs). . I was delighted at how simple it was to use from my Kotlin code while also providing a simple way to write my DB-interactions. No confusion and there‚Äôs a million guides out there showing how you write SQL code for complex joins if you ever need a hand. . Let‚Äôs begin with how you define a table and queries, through a so-called .sq-file. . -- .sq-file CREATE TABLE person ( name TEXT NOT NULL, birthday DATE NOT NULL ); -- You can actually also insert a Person directly in this file if you&#39;d like using the normal SQL insert statement. selectAll: SELECT * FROM person; insert: INSERT INTO person(name, birthday) VALUES (?, ?); insertPerson: INSERT INTO person(name, birthday) VALUES ?; . For those that don‚Äôt know SQL this does the following . Define the table | Create queries on the table These queries makes use of the custom format methodName: and then define the method using the SQL code beneath until it hits end ; . | | Now we have some SQL code defined in a .sq-file, how do we actually use this from our Kotlin-code? We build the project, while building the project the code is generated to our build project with the Kotlin-code. It‚Äôll provide . Data Classes (like structs / objects / case classes) | Queries for each table | . And on top of this you‚Äôll have full typing, which is pretty damn awesome! Let‚Äôs take a look at how we‚Äôd use this from Kotlin. . // Not optimal code, should use injection or something in reality for the db. val database = Database(driver) val personQueries: PersonQueries = database.personQueries println(personQueries.selectAll().executeAsList()) // Prints [] personQueries.insert(name = &quot;Bob &quot;, birthday = Date(2010, 1, 10)) println(personQueries.selectAll().executeAsList()) // Prints [Person.Impl( &quot;Bob &quot;, Date(2010, 1, 10))] val person = Person( &quot;Ronald McDonald &quot;, Date(2020, 1, 5)) personQueries.insertPerson(person) println(personQueries.selectAll().executeAsList()) // Prints [Person.Impl( &quot;Bob &quot;, Date(2010, 1, 10)), Person.Impl( &quot;Ronald McDonald &quot;, Date(2020, 1, 5))] . Let me just say, I‚Äôm amazed about this kind of reverse thinking of generating code from SQL. It gives us the convenience of a ORM but the flexibility of raw SQL :happy:. . Comparison Table . Database Simplicity Requires SQL knowledge Configurability (complex queries etc) Score (5) Comment . JDBC | I | III | III | 2 | To much overhead | . Room / Slick | II | II | II | 4 | Strikes a good balance between natural in normal code while configurable* | . Peewee | III | I | I | 3 | Really easy and fits into code great, but the complex queries becomes really hard and feels forced | . SQLDelight | II | III | III | 5 | Natural to use in the code, great customability &amp; little overhead* | . Both Room &amp; SQLDelight are enforcing SQLite right now which is a major con for those that needs postgresql etc. Personally I only use SQLite as was discussed in expensify‚Äôs blog SQLite can be squeezed to the extreme - expensify managed to handle up to 4 million queries per second! . Outro . In its essence today there‚Äôs a great variety of different kinds of wrappers for databases in almost all languages and it is all about finding one that strikes your balance of perfect. For a really simple database perhaps an ORM such as peewee where no SQL knowledge is really required could be enough. But be sure to know the trade-offs, once your database grows complex so does peewee grow complex fast, same applies to slick and others. Raw SQL as a fall-back is always good to have and a lot of the libraries are starting to add it (e.g. slick), but it never feels natural and always is a bit like a bandaid, ugly right? . Anyhow, I hope this was interesting and perhaps someone learned about a new abstraction-level for databases or was inspired to pick up their own. . ~Hampus .",
            "url": "https://blog.londogard.com/jvm/kotlin/sql/multiplatform/2020/06/01/sqldelight-kotlin.html",
            "relUrl": "/jvm/kotlin/sql/multiplatform/2020/06/01/sqldelight-kotlin.html",
            "date": " ‚Ä¢ Jun 1, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "A simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency",
            "content": "CoViD-19 Swedish QA . I decided to scratch a small itch I‚Äôve had for a while now - creating a search engine using an unsupervised approach. The final product, or the first iteration rather, ended up pretty good and I wanted to share what I‚Äôve done so far. . Introduction to the problem and requirements . An unsupervised approach where we never edit the data nor supply any manually annotated data? Every Data Scientist dream I suppose. There‚Äôs a reason as of why supervised approaches generally result in better performance but there is some light at the end of the tunnel for unsupervised approaches too. . Let‚Äôs begin with my own requirements, which are mainly created to only keep the fun problem-solving left. . The end-product must be unsupervised No manually annotated data | No heuristic applied (at least in first iteration) | . | It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end) | Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!) | . With this in mind I set out to build my own FAQ search engine. . What is required to answer questions using a FAQ? We need to find the most relevant Q/A to the question posed. . How do we do this? There is numerous types of ways to do this unsupervised. I‚Äôll account for a few here: . Latent Dirichlet Allocation (LDA) which is a way to find topics through clever statistical analysis (basically soft clusters of documents) | Embedding and Cosine Similarity, find the distance between the two arrays of numbers in the embedded space. One can also apply Euclidean Distance which isn‚Äôt especially good because of Curse of Dimensionality. Other possible approaches includes Word Mover Distance. | Simple word counting and Bag of Words | Tools Chosen . After a little research I found a few tools which fit my need. . fastText . fastText that came out of Facebook AI Research (FAIR) and this paper. It‚Äôs a type of Word Embeddings where also subwords are embedded through ngrams of characters, this means that we are able to embedd words that are out of vocabulary, which can be the reason because of either misspelling or just a missing word. On their homepage they have a plethora of models including a Swedish one that has been derived from Wikipedia, pretty awesome! . Smooth Inverse Frequency . Smooth Inverse Frequency (SIF) is an algorithm to embed sentences which was proposed in &quot;A Simple but Tough-To-Beat Baseline for Sentence Embeddings&quot; in 2017. In its essence they propose to embed the sentence using a weighted average and thereafter modify them a bit using PCA/SVD. . Folkh√§lsomyndigheten FAQ . Finally I need the FAQ to use, in my case it‚Äôs Covid-19 FAQ from Folkh√§lsomyndigheten. It was parsed into pandas dataframes using requests &amp; BeautifulSoup4 (bs4). . Final Result . So after all this was figured out I sat down an afternoon and cooked some code together, the result ended up more impressive than I had imagined. The questions posed are being responded with pretty good results. I‚Äôm especially impressed by question about astma, son and regler. Here‚Äôs a few of them: . &gt; Hur sjuk blir jag? Hur sjuk blir man av covid-19? - 0.98 Hur l√§nge √§r man sjuk av covid-19? - 0.97 Hur l√•ng √§r inkubationstiden? - 0.81 . &gt; Hur vet jag om det √§r astma? Hur vet jag om mina symtom beror p√• pollenallergi eller p√• covid-19? - 0.63 Hur sjuk blir man av covid-19? - 0.53 Hur l√§nge √§r man sjuk av covid-19? - 0.53 . &gt; Hur f√∂rklarar jag corona f√∂r min son? Hur pratar man med barn om det nya coronaviruset? - 0.58 Hur l√•ng √§r inkubationstiden? - 0.53 Hur sjuk blir man av covid-19? - 0.49 . &gt; Hur minskar vi spridningen i sverige? Hur g√∂r ni f√∂r att m√§ta f√∂rekomsten av covid-19 i samh√§llet? - 0.65 Hur √∂vervakar ni p√• Folkh√§lsomyndigheten spridningen av covid-19? - 0.57 Hur stor √§r d√∂dligheten till f√∂ljd av covid-19? - 0.56 . &gt; Vad f√∂r regler finns? Vad g√§ller f√∂r olika verksamheter? - 0.76 Vad g√§ller f√∂r handeln? - 0.75 Vad √§r covid-19? - 0.71 . One can directly note the correlation of the beginning. It seems like the first word has a high correlation with the most similar question. Weird. Removing stop words could probably improve this, but that‚Äôd be for the second implementation. . Further improvements for iteration 2, 3 and beyond! . Pre-processing . As mentioned right above we can apply some basic pre-processing such as removing stop words. In reality this should be handled by SIF but looking at our similarity scores there‚Äôs a 1-1 relation between the first word of the sentence. . Other improvements worth trying out is lemmatizing or stemming the words (&quot;cutting them to the root&quot; in simple terms) and further using a better tokenization is worth trying out (currently splitting on whitespace). spaCy offers a strong tokenizer, but I haven‚Äôt tried it out for Swedish yet. Once again fastText should handle this but it‚Äôs worth trying out if it improves or keep the result at the same level. . Different Embedding Techniques . There exist a certain Sentence Embedding that‚Äôs basically made for this task - MULE (Multimodal Universal Language Embeddings). MULE is even multilingual but unfortunately they‚Äôre not able to embed Swedish so we‚Äôd require a translation from Swedish to one of the 16 languages supported by MULE. This means that it is out of the question because of my requirements, but could still be fun to check out. . Other embeddings such as FLAIR (by Zalando), BERT (using BERT-as-a-service) or even training my own embeddings (perhaps using StarSpace) could prove interesting also. . Completely other technique . I mentioned first of all LDA, and I think LDA could be interesting. Most often LDA is applied to larger documents but as with everything it is never wrong to try out and verify the results. . Supervised approaches would certainly be able to show us some good performance but that requires annotating data in one way or another which is a boring task - but very important. Perhaps I‚Äôll revisit and label some data, with todays Transfer Learning we can achieve higher accuracy with less data using other pre-trained Language Models such as BERT or Multifit (from Ulmfit). . Ending words . This was a really fun task and I‚Äôm happy that I tried it out. I‚Äôm sure I‚Äôll revisit and improve it further by applying some of the possible improvements. Further I think I might actually try to do this for all FAQs available by our authorities to create a &quot;Multi FAQ&quot; which could prove pretty cool. With more data the results should also be better. . And as an ending note my model ended up using 2.5-3 GB of memory during run-time which means it‚Äôs possible to run on my Raspberry Pi 4! Further reduction of size can be done by removing the most uncommon words in the vocabulary (vocab is 2M words, which is very large). I applied a dimension reduction using the built in version of fastText (ending up using d=100 and still achieving good search results). . The implementation is available at my GitHub (Londogard) or directly launched in Google Colaboratory. . Thanks for this time, I‚Äôll be back with more! Hampus Lond√∂g√•rd .",
            "url": "https://blog.londogard.com/faq/nlp/machine-learning/2020/05/13/faq-search-covid-1.html",
            "relUrl": "/faq/nlp/machine-learning/2020/05/13/faq-search-covid-1.html",
            "date": " ‚Ä¢ May 13, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "How I created a email generator in Kotlin (for Afry Tipsrundan)",
            "content": "Email Generation - Tipsrundan - . At AFRY IT South I‚Äôm co-responsible with Hassan Ftouni at driving the competence. One of my initiatives that we both now drive is to have a biweekly, every second week for all you picky readers out there, newsletter called &quot;Tipsrundan&quot;. . Tipsrundan has lately gathered some fuss around and Afry IT West now wants to join in. This means new challenges to somehow build an email together with more people, collaborating on what to have and what to keep regional. Let me say that this is a fun challenge! . In this post I‚Äôll go through how I built our completely new &quot;Email Generator&quot; in Kotlin that I built a Sunday afternoon. This includes a few things such as . Learning how CSS works in emails (in comparison to browsers) | Kotlin | . Let me start at how Tipsrundan has evolved since we initiated it in October (crazy how time flies). . The evolution of Tipsrundan . When we sent the first Tipsrundan, called TL;DR back then, I used a &quot;email templating language&quot; called MJML and a pre-built template found on their homepage. With this we got a responsive email using their &quot;homemade&quot; templating language. I enjoyed it at the same time as I hated it, there was way to much manual labour copying the sections and inserting my own code, the indentation in the web-editor wasn‚Äôt great and so on. I bet it‚Äôs a great tool but it didn‚Äôt cut it for me, after two or three issues and a ton of research I found a new tool I liked, with a good free variant, called Stripo. Stripo is a really good tool which has excellent support with its drag n‚Äô drop editor where you can save modules and much more. We got a good looking &amp; responsive email that worked out great, everything good right? . It was really good until I realized we had to share the template with new people and Stripo requires premium for this (can‚Äôt blame them, they need their cut) which honestly I was to lazy to fix through management. . With this knowledge I set out to create a tool which we could use internally that is simple and keeps simple. Forward comes a solution I built over a Sunday afternoon where we generate emails from JSON. . Tipsrundan generation . I had some pretty simple requirement: . JSON or yaml as the filetype which we‚Äôd generate Tipsrundan-email from. | Have different sections and easily extendable | Decent looking &amp; responsive (i.e. work on a phone and desktop) | By having these requirements I knew that it‚Äôll be easy to sync over git or whatever tool we need and that we can potentially create themed Tipsrundan editions in the future. . I also knew that I wanted to do this in Kotlin, mainly because I really enjoy coding in Kotlin. . Step 1: Defining the format . The first step was to decide what format to use, or at least begin with. Both yaml and JSON was considered and in the end JSON felt like the best fit. . { &quot;title &quot;: string, &quot;issue &quot;: number, &quot;regional &quot;: [item], .. more categories } . where item is . { &quot;title &quot;: string, &quot;description &quot;: string, &quot;url &quot;: string } . Pretty straight-forward, as I said I prefer to keep it simple. . Step 2: Reading the data . Now that we have a definition of the data we need to read it, this is really a solved problem in most languages through some kind of library. In my case I choose the serialization library provided by Kotlin in the kotlinx-library. As a FYI this library can serialize using CBOR (Concise Binary Object Representation) and other formats. The name of the library is kotlinx-serialization and can be found here. It‚Äôs easiest installed through gradle (using the Kotlin DSL): . plugins { kotlin( &quot;plugin.serialization &quot;) version &quot;1.3.70 &quot; // same version as kotlin } dependencies { .. other dependencies implementation( &quot;org.jetbrains.kotlinx:kotlinx-serialization-runtime:0.20.0 &quot;) // Requires jcenter() as a repository } . kotlinx-serialization is actually cross-platform compatible meaning that it exists for Kotlin targeting JVM, Native &amp; JS (yes we can target all these platforms through Kotlin!). . Once installed it‚Äôs pretty easy to serialize &amp; deserialize, like any other library really. Create the data classes, which is the equivalent of a case class in Scala. . data class Item(val title: String, val description: String, val url: String) . Currently kotlinx-serialization can do the serialization through two different methods, either add an annotation to the class that we‚Äôll use Reflection - this does not work for native. Or we mark the data class as Serializable, the latter being preferred as it‚Äôs truly cross-platform and is more performant. If anyone is wondering a data class is basically a class that provides setters, getters, equality, toString and more! It‚Äôs really awesome. Adding the annotation we end up with the following: . @Serializable data class Item(val title: String, val description: String, val url: String) . Step 3: How to write html in Kotlin? . We all probably know about html-templating that‚Äôs available in most languages, I decided against that and went for a DSL. Kotlin is the language for DSL (Domain Specific Language), for good and bad. Through yet another kotlinx library we got kotlinx-html which provides this DSL. . It looks something like this . fun BODY.createFooter() = footer { hr { } section { p { b { + &quot;Thank you for this time see you in two weeks &quot; } br { } + &quot;Hampus &amp; Hassan &quot; } } } . By using a DSL we get types (as you can see on the BODY) and other bonuses. Although this DSL is pretty verbose it works pretty good. In the end using a DSL or html-template engine does not matter that much in my opinion. By the way, the way this function is typed is called a extension function in Kotlin and is one of my favorite tools. It means that we extend the class, BODY with a new method which is usable on a object of the class. Cool right? . Let‚Äôs move on to the styling and how CSS can be annoying. . Step 4: Styling . There was some important parts going into this, we want the email to look at least decent and also be responsive so that it‚Äôs viewable on both a phone and computer. . CSS and emails are not as simple as with a webpage I learned rather fast. I had great issues actually getting the HTML to look good in gmail/outlook. In the end I found this awesome post from Litmus which is one of the leading Email Marketing providers. I learned that . External CSS is a no-go for emails (a lot of the providers turned it off because of security concerns) | Embedded CSS (using style-tag in the header) works on most places today (not true a few years ago) | Inline CSS is the best | Because I want to keep it simple I went with the second approach, this mean that I can keep the code a bit cleaner and not write as many wrappers for the styled elements. . So knowing how I should implement my styling I needed to find a good style, in the end I remembered an old Reddit-post where I found &quot;MVP.css&quot; which is a small CSS that gives cards, buttons and more. Really brilliant in my opinion, made by Andy Brewer and can be found here. I‚Äôve personally tweaked it a bit to keep the email a bit more compact and informative as this is really made for webpages, but the essentials are the same. . Step 5: Wrapping it all up . Combining all this into a few files in a git repo we can now generate emails from a JSON easily and have multiple categories. . The JSON is used as a data structure | Kotlin used as language | kotlinx-serialization used as a JSON deserializer | kotlinx-html used to build the HTML directly in Kotlin with types | Embedded CSS used as it‚Äôs widely usable by today in email clients | . The repository can be found here. . I hope this was somewhat interesting &amp; something learned. If you‚Äôve any comments please reach out to me through any of the available channels! . Hampus Lond√∂g√•rd .",
            "url": "https://blog.londogard.com/email/kotlin/html/css/serialization/2020/03/31/email-generator-kotlin-tipsrundan.html",
            "relUrl": "/email/kotlin/html/css/serialization/2020/03/31/email-generator-kotlin-tipsrundan.html",
            "date": " ‚Ä¢ Mar 31, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "[2019-02-04] AFRY NLP Competence Meeting: Text Classification IMDB",
            "content": "2019-02-04 AFRY NLP Competence Meeting: Text Classification IMDB . I‚Äôve set a goal to create one blog post per Competence Meeting I‚Äôve held at AFRY to spread the knowledge further. This goal will also grab all the older meetings, my hope is that I‚Äôll be finished before summer 2020, but we‚Äôll see. . . Introduction . Most of my Competence Meetings take place in the form of Jupyter Notebooks (.ipynb). Notebooks are awesome as they allow us to: . Mix and match markdown &amp; code-blocks | Keep the state of the program, i.e. very explorative | This is really good in combination with the workshop-format that we usually have. Using services such as Google Colab one can take the file and open it in the browser and run it there. This means that we don‚Äôt need any downloads and pretty often we also have a speed gain because the node used is faster than a laptop with its GPU. . Let‚Äôs get on to the competence evening. . . Text Classification . Today we‚Äôll go through text classification, what it is, how it is used and how to make it yourself while trying to keep have a great mix of both theory and practical use. Text classification is just what the name suggest, a way to classify texts. Let it be spam or reviews, you train it and it‚Äôll predict what class the text belongs to. . . A good baseline . To have a good baseline is incredibly important in Machine Learning. In summary you want the following . Simple model to predict outcome | Use this model to compare your new, more complex model to | . This is to be able to know what progress you‚Äôre making. You don‚Äôt want to do anything more complex without any gains. . One pretty common simple baseline is just to pick a random class as prediction. . Classes &amp; Features . What is a class and feature? . Features are the input to the model, you can see a machine learning system as a &quot;consumer&quot; of features. You can view this as a cookie monster consuming cookies and then he says if they taste good or bad. He has the input, cookie, that can be a feature. He then has a output, class, that is good/bad. Repeat this a lot of times and you can retrieve statistics if Cookie Y is good or bad. . To generalize this system we would divide the feature into multiple feature, like what ingredients the cookie contains. So instead of saying this is a &quot;Chocolate Chip Cookie&quot; we know tell the system the features are: . chocolate: yes sugar:yes honey:no oat:no cinnamon: no sweet: yes sour: no &quot; . . In numerical input it would translate to something as [1,1,0,0,0,1,0]. . One-Hot-Encoding - how we represent features &amp; classes . As shown in the translation to numerical vectors we don‚Äôt represent words as actual words. We always use numbers, often we even use something called One-Hot-Encoding. . One-Hot-Encoding means that we have an array of one 1 and the rest is 0s. This is to optimize math performed by the GPU (or CPU). . Using the example of Good &amp; Bad cookies with the extension of Decent we will One-Hot-Encode these as the following . Good = [1,0,0] Bad = [0,1,0] Decent = [0,0,1] . The same is applied to our features. If you‚Äôre using a framework (such as Keras) it is pretty common that they include an method to do this, or even that it is done automatically for you. . Back to text classification . To classify a text we do what is called an sentiment analysis meaning that we try to estimate the sentiment polarity of a text body. In the first part of this workshop we‚Äôll be assuming that there‚Äôs only two sentiments, Negative and Positive. Then we can express this as the following classification problem: . Feature: String body Class: Bad|Good . The output, Classes, are easy to One-Hot-Encode but how do we succesfully One-Hot-Encode a string? A character can be seen as a class but is that really something we can learn from? To solve this we need to preprocess our input somehow. . Preprocessing . Preprocessing is an incredibly important part of Machine Learning. Combining preprocessing with Data Mining is actually around 70% of the workload (IBM) when developing models through the CRISP-DM. From my experience this is true. . Having good data and finding the most important features is incredibly important to have a competent system. In this task we need to preprocess the text to simplify the learning process for our system. We will do the following: . Clean the text | Vectorize the texts into numerical vectors | . Cleaning the text . Why do we need to clean the text? It is to remove weird stuff &amp; outliers. If we have the text I&#39;m a cat.we want to simplify this into [i&#39;m, a, cat] or even [im, a, cat]. . Removing data such as non-alphabetical characters and the letter case makes more data look a like and reduces the dimension of our input ‚Äì this simplifies the learning of the system. But removing features can be bad also, if someone writes in all CAPS we can guess that they‚Äôre angry. But let‚Äôs take that later. . import regex as re def clean_text(text): &quot; &quot; &quot; Applies some pre-processing on the given text. Steps : - Removing punctuation - Lowering text &quot; &quot; &quot; # remove the characters [ ], [&#39;] and [ &quot;] text = re.sub(r &quot; &quot;, &quot; &quot;, text) text = re.sub(r &quot; &#39; &quot;, &quot; &quot;, text) # Extra: Is regex needed? Other ways to accomplish this. text = re.sub(r &quot; &quot; &quot;, &quot; &quot;, text) # replace all non alphanumeric with space text = re.sub(r &quot; W+ &quot;, &quot; &quot;, text) # text = re.sub(r &quot;&lt;.+?&gt; &quot;, &quot; &quot;, text) # &lt;br&gt;&lt;/br&gt;hej&lt;br&gt;&lt;/br&gt; # Extra: How would we go ahead and remove HTML? Time to learn some Regex! return text.strip().lower() clean_text( &quot;Wow, we can clean text now. Isn&#39;t that amazing!? &quot;).split() . Vectorization . Now that we can extract text we need to be able to input it to the system. We have to vectorize it. In this part we‚Äôll vectorize each word as a number. The simplest approach to this is using Bag of Words (BOW). . Bag of Words creates a list of words which is called the Dictionary. The Dictionary is just a list of the words from the training data. . Training data: [ &quot;√ÖF is a big company &quot;, &quot;√ÖF making future &quot;] --&gt; Dictionary: [√ÖF, is, a, big, company, making, future] New text: &quot;√ÖF company is a future company &quot; --&gt; [1,1,1,0,2,0,1] . Our new text is vectorized on top of the dictionary. You take the dictionary and replace the words position with the count of it that is found in the new text. . Finalizing the preprocessing . We can actually do some more things to improve the system which I won‚Äôt go into detail about (read the code). We remove stop-words and so on. . from sklearn.feature_extraction.text import CountVectorizer training_texts = [ &quot;√ÖF is a big company &quot;, &quot;√ÖF making future &quot; ] test_texts = [ &quot;√ÖF company is a future company &quot; ] # this is the vectorizer vectorizer = CountVectorizer( stop_words= &quot;english &quot;, # Removes english stop words (such as &#39;a&#39;, &#39;is&#39; and so on.) preprocessor=clean_text # Customized preprocessor ) # fit the vectorizer on the training text vectorizer.fit(training_texts) # get the vectorizer&#39;s vocabulary inv_vocab = {v: k for k, v in vectorizer.vocabulary_.items()} vocabulary = [inv_vocab[i] for i in range(len(inv_vocab))] # vectorization example pd.DataFrame( data=vectorizer.transform(test_texts).toarray(), index=[ &quot;Test sentence &quot;], columns=vocabulary ) . Let‚Äôs do something fun out of this! . To begin with we need data. Luckily I know a perfect dataset for this ‚Äì the IMDB movie reviews from stanford. This is a widely used dataset throughout Sentiment Analysis. The data contains 50 000 reviews where 50 % is positive and the rest negative. First we fetch a dataset. Download this file and unpack it (into aclImdb) if the first code-snippet was unsuccessful. . import os import numpy as np def load_train_test_imdb_data(data_dir): &quot; &quot; &quot; Loads the IMDB train/test datasets from a folder path. Input: data_dir: path to the &quot;aclImdb &quot; folder. Returns: train/test datasets as pandas dataframes. &quot; &quot; &quot; data = {} for split in [ &quot;train &quot;, &quot;test &quot;]: data[split] = [] for sentiment in [ &quot;neg &quot;, &quot;pos &quot;]: score = 1 if sentiment == &quot;pos &quot; else 0 path = os.path.join(data_dir, split, sentiment) file_names = os.listdir(path) for f_name in file_names: with open(os.path.join(path, f_name), &quot;r &quot;) as f: review = f.read() data[split].append([review, score]) # We shuffle the data to make sure we don&#39;t train on sorted data. This results in some bad training. np.random.shuffle(data[ &quot;train &quot;]) data[ &quot;train &quot;] = pd.DataFrame(data[ &quot;train &quot;], columns=[&#39;text&#39;, &#39;sentiment&#39;]) np.random.shuffle(data[ &quot;test &quot;]) data[ &quot;test &quot;] = pd.DataFrame(data[ &quot;test &quot;], columns=[&#39;text&#39;, &#39;sentiment&#39;]) return data[ &quot;train &quot;], data[ &quot;test &quot;] train_data, test_data = load_train_test_imdb_data( data_dir= &quot;aclImdb/ &quot;) . Let‚Äôs create our classifier . We now have a dataset that we have successfully partitioned into a dictionary so that we can use it for our classifier. . Do you see an issue with our baseline right now? . ‚Ä¶As mentioned we want to only have important features to simplify training. Right now we have an enormous amount of features, our BOW-approach result in an 80 000-dimensional vector. Because of this we must use simple algorithms that learn fast &amp; easy, e.g. Linear SVM, Naive Bayes or Logistic Regression. . Let‚Äôs create some code that actually let‚Äôs us train a Linear SVM! . from sklearn.metrics import accuracy_score from sklearn.svm import LinearSVC # Transform each text into a vector of word counts vectorizer = CountVectorizer(stop_words= &quot;english &quot;, preprocessor=clean_text) training_features = vectorizer.fit_transform(train_data[ &quot;text &quot;]) test_features = vectorizer.transform(test_data[ &quot;text &quot;]) # Training model = LinearSVC() model.fit(training_features, train_data[ &quot;sentiment &quot;]) y_pred = model.predict(test_features) # Evaluation acc = accuracy_score(test_data[ &quot;sentiment &quot;], y_pred) print( &quot;Accuracy on the IMDB dataset: {:.2f} &quot;.format(acc*100)) . Comparison to state-of-the-art . Our accuracy is somewhere around 83.5-84 % which is really good! With this simple model and incredibly simplistic feature extraction we achieve a really high amount of correct answer! Comparing this to state-of-the-art we‚Äôre around 11 percent units beneat (~95% accuracy achieved here). . Incredible right? Exciting!? For me it is at least! . How do we improve from here? . Improving the model . We have some huge improvements to make outside of fine-tuning, so we‚Äôll skip the fine-tuning from now. . The first step is to improve our vectorization. . TF-IDF . If you were at first friday (@√ÖF) you have heard about TF-IDF earlier. TF-IDF stands for Term Frequence-Inverse Document Frequency and is a measurement that aims to fight imbalances in texts. . In our vectorization step we look at the word-count meaning that we‚Äôll have some biases to how much a word is present, the longer the text the more the bias. To reduce this we can take the word-count divided by the total amount of words in the text (TF). We also want to downscale the words that are incredibly frequent such as stop words and topic-related words, and upscale unusual words somewhat, e.g.glamorous might not be frequent but it is important to the text most likely. We use IDF for this. We then take these two and combine. . . Implementation details . This is actually really easy to do as sklearn already has a finished TfIdfVectorizer so all we have to do is to replace the CountVectorizer. Let‚Äôs see how it goes! . from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from sklearn.feature_extraction.text import TfidfVectorizer # Transform each text into a vector of word counts vectorizer = TfidfVectorizer(stop_words= &quot;english &quot;, preprocessor=clean_text) training_features = vectorizer.fit_transform(train_data[ &quot;text &quot;]) test_features = vectorizer.transform(test_data[ &quot;text &quot;]) # Training model = LinearSVC() model.fit(training_features, train_data[ &quot;sentiment &quot;]) y_pred = model.predict(test_features) # Evaluation acc = accuracy_score(test_data[ &quot;sentiment &quot;], y_pred) print( &quot;Accuracy on the IMDB dataset: {:.2f} &quot;.format(acc*100)) # Extra: Implement our own TfIdfVectorizer. . Conclusion of TF-IDF . The TfIdVectorizer improved our scoring with 2 percent units, that‚Äôs incredible for such an easy improvement! . This for me shows how important it is to understand the data and what is important. You really need to grasp how to extract the important and what tools are available. . But let‚Äôs not stop here, lets reiterate and improve further. . What is the next natural step? Context I believe. During my master-thesis on spell correction of Street Names it was very obvious how important context is to increase the models understanding. Unfortunately we couldn‚Äôt use the context of a sentence in the thesis (as of the nature of street names) but here we can! . Use of context . Words by themself prove some meaning but sometimes they‚Äôre used in a negated sense, e.g. not good. Good in itself would most likely be positive but if we can get the context around the word we can be more sure about in what manner it is applied. . We call this N-grams where N is equal to the amount of words taken into consideration for each word. Using bigrams (N=2) we get the following: . companies often use corporate bs =&gt; [companies, often, use, slogans, (companies, often), (often,use), (use,slogans)] . Sometimes you include a start &amp; ending word so that it would be ( t, companies) and (slogans, r) or such. In this case as we are not finetuning we won‚Äôt go into that. We‚Äôll keep it simple. . The all-mighty sklearn TfIdfVectorizer actually already have included N-gram support using the parameter ngram_range=(1, N). So let‚Äôs make it simple for us and make use of that! . from sklearn.svm import LinearSVC from sklearn.metrics import accuracy_score from sklearn.feature_extraction.text import TfidfVectorizer # Transform each text into a vector of word counts vectorizer = TfidfVectorizer(ngram_range=(1, 2), strip_accents=&#39;ascii&#39;, max_df=0.98) training_features = vectorizer.fit_transform(train_data[ &quot;text &quot;]) test_features = vectorizer.transform(test_data[ &quot;text &quot;]) # Training model = LinearSVC() model.fit(training_features, train_data[ &quot;sentiment &quot;]) y_pred = model.predict(test_features) # Evaluation acc = accuracy_score(test_data[ &quot;sentiment &quot;], y_pred) print( &quot;Accuracy on the IMDB dataset: {:.2f} &quot;.format(acc*100)) . Conclusion of N-gram . Once again we see a massive improvement. We‚Äôre almost touching 89 % now! That‚Äôs just a mere 6 percent units below state-of-the-art. What can we do to improve now? . Some possible improvements for you to try! . Use a custom threshold to reduce the dimensions | Play around with the ngram_range (don‚Äôt forget a threshold if you do this) | Improve the preprocessing | . # Try some fun things here if you want too :) . Conclusion of phase 1 . We have created a strong baseline for text classification with great accuracy for its simplicity. The following steps has been done . First a simple preprocessing step which is of great importance. We have to remember to not make it to complex, the complexity of preprocessing is like an evil circle in the end. In our case we remove punctuations, stopwords and lower the case. | Secondly we vectorize the data to make it readable by the system. A classifier requires numerical features. For this we had a TfIdfVectorizer that computes frequency of words while downsampling words that are to common &amp; upsampling unusual words. | Finally we added N-gram to the model to increase the understanding of the sentence by supplying context. | . Phase 2 . How do we improve from here? TF-IDF has its cons and pros. Some of the cons are that they: . Don‚Äôt account for any kind of positioning at all | The dimensions are ridiculous large | They can‚Äôt capture semantics. | . Improvements upon this is made by using neural networks and word embeddings. . Word Embeddings . Word Embeddings &amp; Neural Networks are where we left off. By change our model to instead utilize these two concepts we can improve the accuracy once again. . Word Embeddings . Word Embeddings (WE) are actually a type of Neural Network. It uses embedding to create the model. I quickly explained WE during my presentation on Summarization and how to build a great summarizer. Today we‚Äôll go a little more into depth. . To begin with I‚Äôll take the most common example, WE lets us do the following arithmetiric with words: . King - Man + Woman = Queen . This is, in my opinion, completely amazing and fascinating. How does this work? Where do I learn more? Those are my first thoughts. In fact the theory is pretty basic until you get to the nittygritty details, as with most things. . WE is built on the concept ot learn how words are related to eachother. What company do a word have? To make the example more complex we can redefine this too the following: A is to B what C is to D. . Currently there is three &quot;big&quot; models that are widely used. The first one Word2Vec (Mikolov et al 2013), the second is GloVe (MIT MIT, Pennington et al 2014) and the final one is fastText (facebook). . We will look into how you can achieve this without Deep Learning / Neural Networks unlike the models mentioned. . Step 1: How to represent words in a numerical vector . The first thing we have to do to actually understand/achieve word embeddings is to represent words in a numerical vector. In relation to this a quick explanation of sparse &amp; dense representations would be great. Read more in detail at Wikipedia: Sparse Matrix . Sparse representation is when we represent something very sparsely. It tells us that the points in the space is very few in regards to the dimensions and that most elements are empty. Think one-hot-encoding. . A Dense representation in comparison has few dimensions in comparison to possible values and most elements are filled. Think of something continuous. . The most simple way to represent words in a numerical vector is something we touched earlier, by one-hot-encoding them, i.e. a sparse representation. . (Source: Marco Bonzanini, 2017) . Because of how languages are structured having one-hot-encoding means that we will have an incredibly sparse matrix (can be good) but it will have an enormous amount of dimensions (bad). . On top of this how would we go ahead and measure the distance between words? Normally one would use the cosine similarity but if we have a one-hot-encoding all the words would be orthogonal against eachother meaning that the dot-product will be zero. . Creating a dense representation however would indeed capture similarity as we could make use of cosine-similarity and more. Introducing Word2Vec. . Step 2: Word2Vec, representing data densely . The goal of Word2Vec, at least to my understanding, is to actually predict the context of a word. Or in other words we learn embeddings by prediciting the context of the word. The context here being the same definition as in N-grams. Word2Vec uses shallow neural network to learn word vectors so that each word is good at predicting its own contexts (more about his in Skip-Grams) and how to predict a word given a context (more about this in CBOW). . Skip-gram . Skip-gram very simplified is when you train on the N-grams but without the real word. . As of now we have empirical results showing how this technique is very successful at learning the meaning of the words. On top of this the embedding that we get has both direction of semantic and syntatic meaning that are exposed in example such as King - Man.... . Another example would be: Vector(Madrid) - Vector(Spain) + Vector(Sweden) ~ Vector(Stockholm) . So how do the arithmetic of words actually work? . I won‚Äôt go into details (some complicated math, see Gittens et al) but if we assume the following to be true: . All words are distributed uniformly | The embedding model is linear | The conditional distributions of words are indepedent | . Then we can prove that the embedding of the paraphrase of a set of words is obtained by taking the sum over the embeddings of all of the individual words. . Using this result it‚Äôs easy to show how the famous man-woman, king-queen relationship works. . Extra note: You can show this then by havingn King and Queen having the same Male-Femalerelationship as the King then is the paraphrase of the set of words {Queen, X} . I want to note that these assumptions are not 100 percent accurate. In reality word distributions are thought to follow Zipf‚Äôs law. . GloVe . A year after Word2Vec was a fact to the world the scientist decided to reiterate again. This time we got GloVe. GloVe tried to improve upon Word2Vec by that given a word its relationship(s) can be recovered from co-occurence statistics of a large corpus. GloVe is expensive and memory hungry, but it‚Äôs only one load so the issue isn‚Äôt that big. Nitty bitty details . fastText . With fastText one of the biggest problems is solved, both GloVe and Word2Vec only learn embeddings of word of the vocabulary. Because of this we can‚Äôt find an embedding for a word that isn‚Äôt in the dictionary. . Bojanowski et al solved this by learning the word embeddings using subword information. To summarize fastText learns embeddings of character n-grams instead. . The simple way . A simple approach to create your own word embeddings without a neural network is by factorizing a co-occurence matrix using SVD (singular-value-decomposition). As mentioned Word2Vec is barely a neural network as it has no hidden layers nor an y non-linearities. GloVe factorizes a co-occurense matrix while gaining even better results. . I highly recommend you to go check this blog out: https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/ by Stitch Fix. An awesome read and we can go implement this too! .",
            "url": "https://blog.londogard.com/python/competence/machine-learning/nlp/text-classification/2020/02/23/competence-meeting-imdb-text-classification.html",
            "relUrl": "/python/competence/machine-learning/nlp/text-classification/2020/02/23/competence-meeting-imdb-text-classification.html",
            "date": " ‚Ä¢ Feb 23, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Gradle, JVM and GitHub Packages",
            "content": "Gradle, JVM and GitHub Packages . Initial comment this is mainly done as a reminder to myself. . So about 6 months ago GitHub launched a new exciting service; GitHub Package Registry. This service lets you as a GitHub-user upload your Open Source code for free on GitHubs registry supporting a wide array of languagues and build systems - JavaScript (npm), Java/JVM-languages (Maven/Gradle), Ruby (RubyGems), .NET (NuGet), and Docker images. Perhaps more have been added since I last verified. . In this post I‚Äôll try to keep to the point and give clear easy instruction that you‚Äôll be able to bookmark and go back to whenever you need to set this up. . I‚Äôll give the instruction directly beneath with some comments afterwards for the one who‚Äôd like to read some extra. . . 1. Add Maven Publish Plugin . build.gradle.kts . plugins { `maven-publish` // Add this kotlin( &quot;jvm &quot;) version &quot;1.3.60 &quot; } . build.gradle . plugins { id( &quot;maven-publish &quot;) } . build.gradle (using the old apply way of things) . apply plugin: &#39;maven-publish&#39; . 2. Add Publication Part . build.gradle.kts . publishing { repositories { maven { name = &quot;GitHubPackages &quot; url = uri( &quot;https://maven.pkg.github.com/OWNER/REPO &quot;) credentials { username = project.findProperty( &quot;gpr.user &quot;) as String? ?: System.getenv( &quot;GITHUB_ACTOR &quot;) password = project.findProperty( &quot;gpr.key &quot;) as String? ?: System.getenv( &quot;GITHUB_TOKEN &quot;) } } } publications { register&lt;MavenPublication&gt;( &quot;gpr &quot;){ from(components[ &quot;java &quot;]) } } } . build.gradle . publishing { repositories { maven { name = &quot;GitHubPackages &quot; url = uri( &quot;https://maven.pkg.github.com/OWNER/REPOSITORY &quot;) credentials { username = project.findProperty( &quot;gpr.user &quot;) ?: System.getenv( &quot;GITHUB_ACTOR &quot;) password = project.findProperty( &quot;gpr.key &quot;) ?: System.getenv( &quot;GITHUB_TOKEN &quot;) } } } publications { gpr(MavenPublication) { from(components.java) } } } . 3. Automating Release Workflow . To simplify our lifes further; . JitPack is already automated and tracking your repository automatically adding the new releases ones a release is created. | GitHub is not automated and we need to upload our assets | . Automating GitHub packages upload through release &amp; GitHub Actions . We‚Äôll use GitHub Actions to create a workflow where once a release passes stage ‚Äòpublished‚Äô the assets will be uploaded to the repository/artifactory of GitHub Packages. This integration is really awesome as once we‚Äôve set it up we only need to press &quot;Create Release&quot; on the GitHub page to deploy our library to both GitHub Packages &amp; JitPack! . Create the directory .github/workflows in your root-folder of the project if it doesn‚Äôt exist yet. Add the following file: . name: Release &amp; Publish Build on: release: types: [published] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 1.8 uses: actions/setup-java@v1 with: java-version: 1.8 - name: Clean Build run: ./gradlew clean build - name: Publish Build env: GITHUB_TOKEN: $ run: ./gradlew publish . The secrets.GITHUB_TOKEN is automatically supplied by GitHub itself during the run of the GitHub Actions-script. . Pretty awesome right? Go build your libraries and deploy! .",
            "url": "https://blog.londogard.com/gradle/jvm/library/2020/02/10/gradle-github-packages.html",
            "relUrl": "/gradle/jvm/library/2020/02/10/gradle-github-packages.html",
            "date": " ‚Ä¢ Feb 10, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About‚Üó",
          "content": "Should redirect to here. .",
          "url": "https://blog.londogard.com/about",
          "relUrl": "/about",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://blog.londogard.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}