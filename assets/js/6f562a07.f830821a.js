"use strict";(self.webpackChunklondogard=self.webpackChunklondogard||[]).push([[5675],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return m}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),d=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=d(e.components);return r.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=d(n),m=a,c=h["".concat(l,".").concat(m)]||h[m]||p[m]||o;return n?r.createElement(c,i(i({ref:t},u),{},{components:n})):r.createElement(c,i({ref:t},u))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var d=2;d<o;d++)i[d]=n[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},2143:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return d},assets:function(){return u},toc:function(){return p},default:function(){return m}});var r=n(7462),a=n(3366),o=(n(7294),n(3905)),i=["components"],s={description:"A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings & Smooth Inverse Frequency to embed sentences. Basically scratched an itch I've had for a while",tags:["nlp","machine-learning"],title:"A simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency",authors:"hlondogard"},l="CoViD-19 Swedish QA",d={permalink:"/blog/2020/05/13/faq-search-covid-1",editUrl:"https://github.com/londogard/londogard/blog/2020-05-13-faq-search-covid-1.md",source:"@site/blog/2020-05-13-faq-search-covid-1.md",title:"A simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency",description:"A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings & Smooth Inverse Frequency to embed sentences. Basically scratched an itch I've had for a while",date:"2020-05-13T00:00:00.000Z",formattedDate:"May 13, 2020",tags:[{label:"nlp",permalink:"/blog/tags/nlp"},{label:"machine-learning",permalink:"/blog/tags/machine-learning"}],readingTime:6.16,truncated:!0,authors:[{name:"Hampus Lond\xf6g\xe5rd",title:"Main Contributor of Londogard",url:"https://github.com/lundez",imageURL:"https://github.com/lundez.png",key:"hlondogard"}],frontMatter:{description:"A search engine for FAQs in Swedish. Completely unsupervised and making use of Word Embeddings & Smooth Inverse Frequency to embed sentences. Basically scratched an itch I've had for a while",tags:["nlp","machine-learning"],title:"A simple FAQ search engine in Swedish using fastText & Smooth Inverse Frequency",authors:"hlondogard"},prevItem:{title:"SQL - Different Abstraction Levels (& how I came to love SQLDelight)",permalink:"/blog/2020/06/01/sqldelight-kotlin"},nextItem:{title:"How I created a email generator in Kotlin (for Afry Tipsrundan)",permalink:"/blog/2020/03/31/email-generator-kotlin-tipsrundan"}},u={authorsImageUrls:[void 0]},p=[{value:"Introduction to the problem and requirements",id:"introduction-to-the-problem-and-requirements",children:[],level:3},{value:"Tools Chosen",id:"tools-chosen",children:[],level:3},{value:"Final Result",id:"final-result",children:[],level:3},{value:"Further improvements for iteration 2, 3 and beyond!",id:"further-improvements-for-iteration-2-3-and-beyond",children:[],level:3},{value:"Ending words",id:"ending-words",children:[],level:3}],h={toc:p};function m(e){var t=e.components,n=(0,a.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"I decided to scratch a small itch I've had for a while now - creating a search engine using an unsupervised approach. The final product, or the first iteration rather, ended up pretty good and I wanted to share what I've done so far."),(0,o.kt)("h3",{id:"introduction-to-the-problem-and-requirements"},"Introduction to the problem and requirements"),(0,o.kt)("p",null,"An unsupervised approach where we never edit the data nor supply any manually annotated data? Every Data Scientist dream I suppose. There's a reason as of why supervised approaches generally result in better performance but there is some light at the end of the tunnel for unsupervised approaches too."),(0,o.kt)("p",null,"Let's begin with my own requirements, which are mainly created to only keep the fun problem-solving left."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The end-product must be unsupervised",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"No manually annotated data"),(0,o.kt)("li",{parentName:"ul"},"No heuristic applied (at least in first iteration)"))),(0,o.kt)("li",{parentName:"ul"},"It should be light enough to run on a Raspberry Pi later on (hopefully on the JVM to keep it simple with my back-end)"),(0,o.kt)("li",{parentName:"ul"},"Must be Swedish all the way through - no translations (English models you can transfer knowledge from tends to be stronger, but I want to keep this fun!)")),(0,o.kt)("p",null,"With this in mind I set out to build my own FAQ search engine."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"What is required to answer questions using a FAQ?")," We need to find the most relevant Q/A to the question posed. "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"How do we do this?")," There is numerous types of ways to do this unsupervised. I'll account for a few here:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Latent Dirichlet Allocation (LDA) which is a way to find topics through clever statistical analysis (basically soft clusters of documents)"),(0,o.kt)("li",{parentName:"ol"},"Embedding and ",(0,o.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Cosine_similarity"},"Cosine Similarity"),", find the distance between the two arrays of numbers in the embedded space. One can also apply Euclidean Distance which isn't especially good because of ",(0,o.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Curse_of_dimensionality"},"Curse of Dimensionality"),". Other possible approaches includes ",(0,o.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1912.00509.pdf"},"Word Mover Distance"),". "),(0,o.kt)("li",{parentName:"ol"},"Simple word counting and Bag of Words")),(0,o.kt)("h3",{id:"tools-chosen"},"Tools Chosen"),(0,o.kt)("p",null,"After a little research I found a few tools which fit my need. "),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"fastText")),(0,o.kt)("p",null,"fastText that came out of Facebook AI Research (FAIR) and ",(0,o.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1607.04606"},"this")," paper. It's a type of Word Embeddings where also subwords are embedded through ngrams of characters, this means that we are able to embedd words that are out of vocabulary, which can be the reason because of either misspelling or just a missing word.\nOn their ",(0,o.kt)("a",{parentName:"p",href:"https://fasttext.cc/"},"homepage")," they have a plethora of models including a Swedish one that has been derived from Wikipedia, pretty awesome!"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Smooth Inverse Frequency")),(0,o.kt)("p",null,"Smooth Inverse Frequency (SIF) is an algorithm to embed sentences which was proposed in ",(0,o.kt)("a",{parentName:"p",href:"https://openreview.net/pdf?id=SyK00v5xx"},'\\"A Simple but Tough-To-Beat Baseline for Sentence Embeddings\\"')," in 2017. In its essence they propose to embed the sentence using a weighted average and thereafter modify them a bit using PCA/SVD."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Folkh\xe4lsomyndigheten FAQ")),(0,o.kt)("p",null,"Finally I need the FAQ to use, in my case it's Covid-19 FAQ from Folkh\xe4lsomyndigheten. It was parsed into pandas dataframes using requests & BeautifulSoup4 (bs4)."),(0,o.kt)("h3",{id:"final-result"},"Final Result"),(0,o.kt)("p",null,"So after all this was figured out I sat down an afternoon and cooked some code together, the result ended up more impressive than I had imagined. The questions posed are being responded with pretty good results. I'm especially impressed by question about ",(0,o.kt)("em",{parentName:"p"},"astma"),", ",(0,o.kt)("em",{parentName:"p"},"son")," and ",(0,o.kt)("em",{parentName:"p"},"regler"),". Here's a few of them:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"> Hur sjuk blir jag?\n\nHur sjuk blir man av covid-19? - 0.98\nHur l\xe4nge \xe4r man sjuk av covid-19? - 0.97\nHur l\xe5ng \xe4r inkubationstiden? - 0.81\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"> Hur vet jag om det \xe4r astma?\n\nHur vet jag om mina symtom beror p\xe5 pollenallergi eller p\xe5 covid-19? - 0.63\nHur sjuk blir man av covid-19? - 0.53\nHur l\xe4nge \xe4r man sjuk av covid-19? - 0.53\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"> Hur f\xf6rklarar jag corona f\xf6r min son?\n\nHur pratar man med barn om det nya coronaviruset? - 0.58\nHur l\xe5ng \xe4r inkubationstiden? - 0.53\nHur sjuk blir man av covid-19? - 0.49\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"> Hur minskar vi spridningen i sverige?\n\nHur g\xf6r ni f\xf6r att m\xe4ta f\xf6rekomsten av covid-19 i samh\xe4llet? - 0.65\nHur \xf6vervakar ni p\xe5 Folkh\xe4lsomyndigheten spridningen av covid-19? - 0.57\nHur stor \xe4r d\xf6dligheten till f\xf6ljd av covid-19? - 0.56\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"> Vad f\xf6r regler finns?\n\nVad g\xe4ller f\xf6r olika verksamheter? - 0.76\nVad g\xe4ller f\xf6r handeln? - 0.75\nVad \xe4r covid-19? - 0.71\n")),(0,o.kt)("p",null,"One can directly note the correlation of the beginning. It seems like the first word has a high correlation with the most similar question. Weird. Removing stop words could probably improve this, but that'd be for the second implementation."),(0,o.kt)("h3",{id:"further-improvements-for-iteration-2-3-and-beyond"},"Further improvements for iteration 2, 3 and beyond!"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Pre-processing")),(0,o.kt)("p",null,"As mentioned right above we can apply some basic pre-processing  such as removing stop words. In reality this should be handled by SIF but looking at our similarity scores there's a 1-1 relation between the first word of the sentence. "),(0,o.kt)("p",null,'Other improvements worth trying out is lemmatizing or stemming the words (\\"cutting them to the root\\" in simple terms) and further using a better tokenization is worth trying out (currently splitting on whitespace). ',(0,o.kt)("em",{parentName:"p"},"spaCy")," offers a strong tokenizer, but I haven't tried it out for Swedish yet. Once again ",(0,o.kt)("em",{parentName:"p"},"fastText")," should handle this but it's worth trying out if it improves or keep the result at the same level."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Different Embedding Techniques")),(0,o.kt)("p",null,"There exist a certain Sentence Embedding that's basically made for this task - MULE (Multimodal Universal Language Embeddings). MULE is even multilingual but unfortunately they're not able to embed Swedish so we'd require a translation from Swedish to one of the 16 languages supported by MULE. This means that it is out of the question because of my requirements, but could still be fun to check out. "),(0,o.kt)("p",null,"Other embeddings such as FLAIR (by Zalando), BERT (using BERT-as-a-service) or even training my own embeddings (perhaps using StarSpace) could prove interesting also."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Completely other technique")),(0,o.kt)("p",null,"I mentioned first of all LDA, and I think LDA could be interesting. Most often LDA is applied to larger documents but as with everything it is never wrong to try out and verify the results. "),(0,o.kt)("p",null,"Supervised approaches would certainly be able to show us some good performance but that requires annotating data in one way or another which is a boring task - but very important. Perhaps I'll revisit and label some data, with todays Transfer Learning we can achieve higher accuracy with less data using other pre-trained  Language Models such as BERT or Multifit (from Ulmfit)."),(0,o.kt)("h3",{id:"ending-words"},"Ending words"),(0,o.kt)("p",null,"This was a really fun task and I'm happy that I tried it out. I'm sure I'll revisit and improve it further by applying some of the possible improvements. Further I think I might actually try to do this for all FAQs available by our authorities to create a \\\"Multi FAQ\\\" which could prove pretty cool. With more data the results should also be better."),(0,o.kt)("p",null,"And as an ending note my model ended up using 2.5-3 GB of memory during run-time which means it's possible to run on my Raspberry Pi 4! Further reduction of size can be done by removing the most uncommon words in the vocabulary (vocab is 2M words, which is very large). I applied a dimension reduction using the built in version of ",(0,o.kt)("em",{parentName:"p"},"fastText")," (ending up using d=100 and still achieving good search results)."),(0,o.kt)("p",null,"The implementation is available at my ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/londogard/nlp-projects/blob/master/python/CoViD_19_QA.ipynb"},"GitHub (Londogard)")," or directly launched in ",(0,o.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/londogard/nlp-projects/blob/master/python/CoViD_19_QA.ipynb"},"Google Colaboratory"),"."),(0,o.kt)("p",null,"Thanks for this time, I'll be back with more!",(0,o.kt)("br",{parentName:"p"}),"\n","Hampus Lond\xf6g\xe5rd"))}m.isMDXComponent=!0}}]);