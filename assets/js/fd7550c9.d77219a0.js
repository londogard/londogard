"use strict";(self.webpackChunklondogard=self.webpackChunklondogard||[]).push([[5210],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return g}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=p(n),g=r,h=m["".concat(s,".").concat(g)]||m[g]||d[g]||i;return n?a.createElement(h,o(o({ref:t},u),{},{components:n})):a.createElement(h,o({ref:t},u))}));function g(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9044:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},assets:function(){return u},toc:function(){return d},default:function(){return g}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),o=["components"],l={title:"GPT2-snapsvisor - Generating Swedish Drinking Songs",description:"Snapsvisor is traditional Swedish Drinking Songs, sometimes they need some refreshing which I try to do through AI here! ;)",slug:"gpt2-snapsvisor",tags:["machine-learning","nlp","fun"],authors:"hlondogard"},s=void 0,p={permalink:"/blog/gpt2-snapsvisor",editUrl:"https://github.com/londogard/londogard/blog/2022-07-07-snapsvisor-generated/2022-07-07-snapsvisor-generator.md",source:"@site/blog/2022-07-07-snapsvisor-generated/2022-07-07-snapsvisor-generator.md",title:"GPT2-snapsvisor - Generating Swedish Drinking Songs",description:"Snapsvisor is traditional Swedish Drinking Songs, sometimes they need some refreshing which I try to do through AI here! ;)",date:"2022-07-07T00:00:00.000Z",formattedDate:"July 7, 2022",tags:[{label:"machine-learning",permalink:"/blog/tags/machine-learning"},{label:"nlp",permalink:"/blog/tags/nlp"},{label:"fun",permalink:"/blog/tags/fun"}],readingTime:8.6,truncated:!1,authors:[{name:"Hampus Lond\xf6g\xe5rd",title:"Main Contributor of Londogard",url:"https://github.com/lundez",imageURL:"https://github.com/lundez.png",key:"hlondogard"}],frontMatter:{title:"GPT2-snapsvisor - Generating Swedish Drinking Songs",description:"Snapsvisor is traditional Swedish Drinking Songs, sometimes they need some refreshing which I try to do through AI here! ;)",slug:"gpt2-snapsvisor",tags:["machine-learning","nlp","fun"],authors:"hlondogard"},nextItem:{title:"Forecasting Crypto Prices using Deep Learning (Time Series #3)",permalink:"/blog/timeseries-pt-3"}},u={authorsImageUrls:[void 0]},d=[{value:"GPT-3: Few-Shot Learning and Prompt Engineering",id:"gpt-3-few-shot-learning-and-prompt-engineering",children:[],level:3},{value:"Few-Shot Learning Explained",id:"few-shot-learning-explained",children:[],level:3},{value:"Fine-Tuning a model",id:"fine-tuning-a-model",children:[],level:3},{value:"Building the dataset of Snapsvisor",id:"building-the-dataset-of-snapsvisor",children:[{value:"Requests",id:"requests",children:[],level:5},{value:"BeautifulSoup",id:"beautifulsoup",children:[],level:5}],level:3},{value:"Fine-Tuning the model",id:"fine-tuning-the-model",children:[],level:3},{value:"TL;DR (Too Long; Didn\u2019t Read)",id:"tldr-too-long-didnt-read",children:[],level:2}],m={toc:d};function g(e){var t=e.components,l=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,a.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"This midsummer my friends gave me the idea that I should generate Swedish Drinking Songs, or\xa0",(0,i.kt)("em",{parentName:"p"},"Snapsvisor"),",\xa0using Machine Learning and I thought it\xa0could be a lot of fun! \ud83c\udf7b\xa0\xa0"),(0,i.kt)("p",null,"To achieve the best results I'd need access to GPT-3, or equivalent model, alas I don\u2019t and as such I needed\xa0 to do some extra work! Fun work though! \ud83e\udd13"),(0,i.kt)("p",null,"Why GPT-3?"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Possible to do\xa0",(0,i.kt)("em",{parentName:"li"},"prompt engineering"),", which gwern has a great\xa0",(0,i.kt)("a",{parentName:"li",href:"https://www.gwern.net/GPT-3#prompts-as-programming"},"blog on"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"TL;DR prompt engineering allows you to write a prompt and get a response, e.g. \u201cstep-by-step how to write a blog\u201d and the return the step-by-step."))),(0,i.kt)("li",{parentName:"ul"},"Much better zero-/one-/few-shot learning",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Because the model has a ton more parameters and trained on a larger dataset")))),(0,i.kt)("p",null,"Drawbacks with GPT-2:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The performance is noticable worse because of the lower parameters and less data, sometimes called tokens, used to train the model.")),(0,i.kt)("p",null,"As such the result is not amazing, but it's capable and really funny - based on the premise that you know Swedish!"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"It's available on the ",(0,i.kt)("a",{parentName:"strong",href:"https://huggingface.co/lunde/gpt2-snapsvisor"},"HuggingFace Hub"))," with a ",(0,i.kt)("strong",{parentName:"p"},"inference widget")," and ",(0,i.kt)("strong",{parentName:"p"},"as a pre-trained model"),", which can generate your own ",(0,i.kt)("em",{parentName:"p"},"Snapsvisor")," - N.B. it removes newlines. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("lunde/gpt2-snapsvisor")\nmodel = AutoModelForCausalLM.from_pretrained("lunde/gpt2-snapsvisor")\n')),(0,i.kt)("h3",{id:"gpt-3-few-shot-learning-and-prompt-engineering"},"GPT-3: Few-Shot Learning and Prompt Engineering"),(0,i.kt)("p",null,"Few-Shot Learning is the capability to solve an unknown task using either very few or no (training) data points at all to solve, which can successfully be done today."),(0,i.kt)("p",null,"In other words, we can achieve great accuracy with little or no data att all!"),(0,i.kt)("p",null,"Few-Shot Learning reminds me of how human learns, we are very fast at generalizing knowledge. By knowing the difference between a cat, dog and tiger we can very fast learn that a lion is a unique animal and if we are told the name we know that this unique animal is a lion!"),(0,i.kt)("p",null,'We are also able to make up words that we don\'t know about, like "car wheel" by seeing a wheel on a car!'),(0,i.kt)("h3",{id:"few-shot-learning-explained"},"Few-Shot Learning Explained"),(0,i.kt)("p",null,"Less theory, more examples!"),(0,i.kt)("p",null,(0,i.kt)("em",{parentName:"p"},"Prompt Engineering"),"\xa0 is a Few-Shot technique that grew increasingly powerful with each generation of Large Language Models (LLM) and with GPT-3 it became incredibly good."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Model:"),"\xa0The Large Language Model (LLM) being used, e.g. GPT-3 ."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Prompt:"),"\xa0The text given to the LLM to generate an answer from, or complete."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Zero-shot:"),"\xa0A prompt with no examples, e.g.\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"The name of a character from Star Wars is:"),"\xa0or\xa0",(0,i.kt)("inlineCode",{parentName:"li"},'[Swedish: "Snaps!", English: "')),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Few-shot:"),"\xa0A prompt with one (1-shot) or more (n-shot, few-shot) examples. See example below")),(0,i.kt)("p",null,"To then give a example of few-shot (4) prompt:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"This is a list of startup ideas:\n1. [Tag: Internet] A website that lets you post articles you've written, and other people can help you edit them.\n2. [Tag: Home] A website that lets you share a photo of something broken in your house, and then local people can offer to fix it for you.\n3. [Tag: Children] An online service that teaches children how to code.\n4. [Tag: Financial] An online service that allows people to rent out their unused durable goods to people who need them.\n5. [Tag: Machine Learning]\n")),(0,i.kt)("p",null,"Using\xa0",(0,i.kt)("em",{parentName:"p"},"Prompt Engineering"),"\xa0and GPT-3 I'm certain that it'd be possible to generate\xa0",(0,i.kt)("em",{parentName:"p"},"Snapsvisor"),"\xa0with little data, i.e. Few-Shot Learning."),(0,i.kt)("p",null,"GPT-2\u2019s few-shot capabilities are much smaller as such I need to\xa0",(0,i.kt)("em",{parentName:"p"},"Fine Tune"),"\xa0the model."),(0,i.kt)("h3",{id:"fine-tuning-a-model"},"Fine-Tuning a model"),(0,i.kt)("p",null,"Fine-Tuning a model can be done in multiple ways, three examples of fine-tuning is"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("em",{parentName:"li"},"Training"),"\xa0(like normally)"),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("em",{parentName:"li"},"Freezing")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("em",{parentName:"li"},"Dynamic Learning Rate"))),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Training")),(0,i.kt)("p",null,"Simply train your pre-trailed model with the new dataset. The Language Models parameters is already encoded with knowledge about language, structure and semantic meaning. This efficient representation and initiation means that it faster learns about new, similar, tasks\xa0than a pseudo-random weight initialization.\xa0\xa0"),(0,i.kt)("p",null,"It has to be noted that the pre-training task, scheme and data impacts the later fine-tuning."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Freezing"),"\xa0  "),(0,i.kt)("p",null,"Freezing layers is one of the most common approaches, usually freezing all layers except the\xa0",(0,i.kt)("em",{parentName:"p"},"head"),"\xa0","-"," the head is the final layer(s) and the rest is the\xa0",(0,i.kt)("em",{parentName:"p"},"backbone"),". Usually the head is equal to the classification layer.\xa0"),(0,i.kt)("p",null,"In other words, the head takes an internal representation, embedding, and learns to decode\xa0it in the optimal way to solve the task."),(0,i.kt)("p",null,"By only changing your head and training that part we train the model faster and don't risk forgetting important information in the network."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(5406).Z,width:"676",height:"526"}),"  "),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},(0,i.kt)("strong",{parentName:"p"},"Important info early in the network:"),(0,i.kt)("br",{parentName:"p"}),"\n","Early in the network important information is chopped into large pieces and unimportant information is largely removed.",(0,i.kt)("br",{parentName:"p"}),"\n","Like carpeting you first chop the large pieces and the further you get the smaller and finer details are built. The same can be said about neural networks.\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","This means that if we learn to remove important information at the early stages we'll loose it. This is catastrophic forgetting.")),(0,i.kt)("p",null,"Once the head is trained we can improve the results further by gradually unfreezing the last layers, one at a time. But make sure to have a low learning rate as otherwise the model might forget important information."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(4836).Z,width:"697",height:"504"}),"  "),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Dynamic Learning Rate"),"\xa0  "),(0,i.kt)("p",null,"Rather than freezing and unfreezing layer there's a technique where you apply different learning rates, lower in early layers and larger at the final ones."),(0,i.kt)("p",null,"This means that we don't risk forgetting important information in the early layers."),(0,i.kt)("p",null,(0,i.kt)("img",{src:n(9562).Z,width:"762",height:"489"}),"  "),(0,i.kt)("h3",{id:"building-the-dataset-of-snapsvisor"},"Building the dataset of Snapsvisor"),(0,i.kt)("p",null,"Because only GPT-2 was accessible we must\xa0",(0,i.kt)("em",{parentName:"p"},"Fine-Tune"),"\xa0the model. Gathering the data becomes the first and most important step, and we need many\xa0",(0,i.kt)("em",{parentName:"p"},"Snapsvisor"),"."),(0,i.kt)("p",null,"To build this dataset I decided to do what most software developers decide to do in this stage,\xa0",(0,i.kt)("em",{parentName:"p"},"scrape the internet"),". The internet is really a wonderful source of data and I found multiple sites that had\xa0",(0,i.kt)("em",{parentName:"p"},"Snapsvisor"),"."),(0,i.kt)("p",null,"To query and parse these I needed my necessary tools, which in Python naturally is\xa0"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"requests")),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"BeautifulSoup4"))),(0,i.kt)("h5",{id:"requests"},"Requests"),(0,i.kt)("p",null,"It\u2019s very easy to do HTTP-requests using\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"requests"),", simply call\xa0",(0,i.kt)("inlineCode",{parentName:"p"},".get"),"\xa0or\xa0",(0,i.kt)("inlineCode",{parentName:"p"},".post")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import requests\n\nurl = "https://www.website.com/path/to/query"\nresp = requests.get(url)\nresp.status_code, resp.text # also possible to run resp.json\n')),(0,i.kt)("p",null,"And if the web-page blocks you by some reason most of the times it can be solved by updating the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"headers"),"\xa0 supplied."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'headers = { "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0" }\nresp = requests.get(url, headers=headers).text\n')),(0,i.kt)("p",null,"That\u2019s all we need to learn about\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"requests"),"\xa0for now!"),(0,i.kt)("h5",{id:"beautifulsoup"},"BeautifulSoup"),(0,i.kt)("p",null,"And how about BeautifulSoup, or bs4 as it's sometimes called?"),(0,i.kt)("p",null,"It\u2019s also rather easy, the webpage is returned almost like a dictionary with tools to query in-memory."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'resp = requests.get(url).text\nsoup = BeautifulSoup(resp, "html.parser")\n\n# now you can query `soup.find_all("a")` to get all a-elements in the HTML page etc\n')),(0,i.kt)("p",null,"To learn more see the documentation at\xa0",(0,i.kt)("a",{parentName:"p",href:"https://www.crummy.com/software/BeautifulSoup/bs4/doc/"},"https://www.crummy.com/software/BeautifulSoup/bs4/doc/"),"."),(0,i.kt)("h3",{id:"fine-tuning-the-model"},"Fine-Tuning the model"),(0,i.kt)("p",null,"I found a swedish version of GPT-2 in the HuggingFace Hub \ud83e\udd73"),(0,i.kt)("p",null,"Fine-Tuning the model on my data to generate\xa0",(0,i.kt)("em",{parentName:"p"},"Snapsvisor"),"\xa0is not far away! \ud83d\ude0e"),(0,i.kt)("p",null,"To move fast and make it easy I chose to use\xa0the HuggingFace Trainer-API. A better tutorial than this one is available in the HuggingFace documentation."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import AutoTokenizer, TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n\nmodel_name = "flax-community/swe-gpt-wiki" # replace with your model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(gpt_model)\n\n')),(0,i.kt)("p",null,"With the pretrained model and tokenizer locked and loaded we're ready to fine-tune with our own data.\xa0"),(0,i.kt)("p",null,"Loading our dataset can be done in multiple ways, but the easiest is most likely to either use HuggingFace\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"[datasets](https://huggingface.co/docs/datasets)"),"\xa0or through their\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"DataCollatorForLanguageModelling"),"\xa0."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def load_dataset(train_path, tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=128)\n\n    data_collator = DataCollatorForLanguageModeling(\n         tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset, data_collator\n\ntrain_path = "all_text.txt" # replace with your training file\ntrain_dataset, data_collator = load_dataset(train_path, tokenizer)\n')),(0,i.kt)("p",null,"With our model, tokenizer and (training) dataset ready we can start fine-tuning the model! This is easiest done using HuggingFace\u2019s\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Trainer"),"\xa0."),(0,i.kt)("p",null,"HuggingFace has used a\xa0",(0,i.kt)("em",{parentName:"p"},"Argument-Object"),"\xa0pattern to reduce the number of arguments to the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Trainer"),". The\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"TrainingArguments"),"\xa0 class wraps a lot of the arguments, fully typed. \ud83d\udc4c"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'training_args = TrainingArguments(\n    output_dir="./gpt2-snaps", # The output directory\n    overwrite_output_dir=True, # Overwrite the content of the output directory\n    num_train_epochs=300, # Number of training epochs\n    per_device_train_batch_size=8, # Batch size for training\n    save_steps=1000, # After # steps model is saved\n    warmup_steps=500, # Number of warmup steps for learning rate scheduler\n    fp16=True # Activate float-point=16 precision to train faster\n    )\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n')),(0,i.kt)("p",null,"Training is one click away!\xa0",(0,i.kt)("strong",{parentName:"p"},"But we don't have a validation dataset?")),(0,i.kt)("p",null,"As I have little data I chose to train with all data, I don't really care if we overfit the data (unless it looks bad) for this small task. As such we use all the data to train our Language Model."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"trainer.train(resume_from_checkpoint=True)\ntrainer.save_model()\n")),(0,i.kt)("p",null,"Generating text, or\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"inference"),", is smooth using HuggingFace\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"pipelines"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from transformers import pipeline\n\ntrainer.model.cpu()\nsnaps = pipeline('text-generation', model=trainer.model, tokenizer=gpt_model)\n\nresult = snaps('Nu tar vi en nubbe')\n")),(0,i.kt)("p",null,"That's all there is really!"),(0,i.kt)("p",null,"Some examples:"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Nu tar vi en nubbe,\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","Den ska hellre krypa och br\xe4nna upp.\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","Vi br\xe4nna, men inte mj\xf6lka oss,\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","och br\xe4nner till bords")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Nu tar vi en nubbe \u2013 sl\xe5ss h\xe4r i ett svep.",(0,i.kt)("br",{parentName:"p"}),"\n","Byssa luderorgon, snapsar!",(0,i.kt)("br",{parentName:"p"}),"\n","N\xe4r den evigt l\xe5ga solen tar oss en ljus natt,",(0,i.kt)("br",{parentName:"p"}),"\n","ingen blir s\xe5 dragen vid n\xe4san")),(0,i.kt)("h2",{id:"tldr-too-long-didnt-read"},"TL;DR (Too Long; Didn\u2019t Read)"),(0,i.kt)("p",null,"For those that are not interested in details or writeups I thought I\u2019d leave a small TL;DR"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Find a pre-trained Language Model (e.g. GPT-2) (",(0,i.kt)("a",{parentName:"li",href:"https://huggingface.co/flax-community/swe-gpt-wiki"},"link"),")"),(0,i.kt)("li",{parentName:"ol"},"Scrape the web for text data (in my case \u201cSnapsvisor\u201d) using\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"requests"),"\xa0 (",(0,i.kt)("a",{parentName:"li",href:"https://requests.readthedocs.io/"},"link"),") and\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"beautifulsoup4"),"\xa0(",(0,i.kt)("a",{parentName:"li",href:"https://www.crummy.com/software/BeautifulSoup/bs4/doc/"},"link"),")"),(0,i.kt)("li",{parentName:"ol"},"Fine-Tune the model using HuggingFace\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"Trainer"),"-","api (",(0,i.kt)("a",{parentName:"li",href:"https://huggingface.co/docs/transformers/main_classes/trainer"},"link"),")"),(0,i.kt)("li",{parentName:"ol"},"Generate text using HuggingFace\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"pipelines"),"\xa0(",(0,i.kt)("a",{parentName:"li",href:"https://huggingface.co/docs/transformers/main_classes/pipelines"},"link"),")")),(0,i.kt)("p",null,"And in the end we can generate some\xa0",(0,i.kt)("em",{parentName:"p"},"Snapsvisor"),", like the following examples"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Sn\xe4lla sk\xe5lar till en nubbe, buguperrens till en sk\xe5l.",(0,i.kt)("br",{parentName:"p"}),"\n","Snapsen f\xe5r ta en nubbe,",(0,i.kt)("br",{parentName:"p"}),"\n","n\xe4r nubben inte tagit visdomstr\xe4ngar,",(0,i.kt)("br",{parentName:"p"}),"\n","sen f\xe5r ta nubben hellre.",(0,i.kt)("br",{parentName:"p"}),"\n","Hinka lilla magen, ta nubben,",(0,i.kt)("br",{parentName:"p"}),"\n","d\xe4r ska det g\xe5 till en nubbe.  ")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Nu tar vi en nubbe,\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","Den ska hellre krypa och br\xe4nna upp.\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","Vi br\xe4nna, men inte mj\xf6lka oss,\xa0",(0,i.kt)("br",{parentName:"p"}),"\n","och br\xe4nner till bords")),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"Nu tar vi en nubbe \u2013 sl\xe5ss h\xe4r i ett svep.",(0,i.kt)("br",{parentName:"p"}),"\n","Byssa luderorgon, snapsar!",(0,i.kt)("br",{parentName:"p"}),"\n","N\xe4r den evigt l\xe5ga solen tar oss en ljus natt,",(0,i.kt)("br",{parentName:"p"}),"\n","ingen blir s\xe5 dragen vid n\xe4san")),(0,i.kt)("p",null,"And ",(0,i.kt)("strong",{parentName:"p"},"it's available on the ",(0,i.kt)("a",{parentName:"strong",href:"https://huggingface.co/lunde/gpt2-snapsvisor"},"HuggingFace Hub"))," with a ",(0,i.kt)("strong",{parentName:"p"},"inference widget"),", which can generate your own ",(0,i.kt)("em",{parentName:"p"},"Snapsvisor")," - N.B. it removes newlines. "),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("lunde/gpt2-snapsvisor")\nmodel = AutoModelForCausalLM.from_pretrained("lunde/gpt2-snapsvisor")\n')),(0,i.kt)("p",null,"Until next time!"),(0,i.kt)("p",null,"~Hampus Lond\xf6g\xe5rd"))}g.isMDXComponent=!0},5406:function(e,t,n){t.Z=n.p+"assets/images/06406d83-410b-4e79-b2c8-025c53dbf9b2-f81b488e9be1175647320ce40bffb42f.png"},9562:function(e,t,n){t.Z=n.p+"assets/images/e449ace7-df38-4bda-bc44-38a8cad9ad18-e57b5e256d9a69944498a4211fc2c4cc.png"},4836:function(e,t,n){t.Z=n.p+"assets/images/ed771f34-3726-43f3-8d73-194f9e94e42d-58c36830ef3b7be797166dc1ea1950f1.png"}}]);