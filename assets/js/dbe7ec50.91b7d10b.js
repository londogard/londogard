"use strict";(self.webpackChunklondogard=self.webpackChunklondogard||[]).push([[9595],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return m}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=a.createContext({}),p=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=p(n),m=r,f=u["".concat(s,".").concat(m)]||u[m]||c[m]||o;return n?a.createElement(f,i(i({ref:t},d),{},{components:n})):a.createElement(f,i({ref:t},d))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=u;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},7890:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return p},assets:function(){return d},toc:function(){return c},default:function(){return m}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],l={description:"nlp (londogard-nlp-toolkit) has had it's 1.1.0 release recently with a lot of new functionality and multiple improvements to efficiency, dive in to understand more!",tags:["nlp","jvm","kotlin"],title:"Release nlp (londogard-nlp-toolkit) 1.1.0",authors:"hlondogard"},s="Release nlp 1.1.0",p={permalink:"/blog/2022/01/16/nlp-toolkit-release",editUrl:"https://github.com/londogard/londogard/blog/2022-01-16-nlp-toolkit-release.mdx",source:"@site/blog/2022-01-16-nlp-toolkit-release.mdx",title:"Release nlp (londogard-nlp-toolkit) 1.1.0",description:"nlp (londogard-nlp-toolkit) has had it's 1.1.0 release recently with a lot of new functionality and multiple improvements to efficiency, dive in to understand more!",date:"2022-01-16T00:00:00.000Z",formattedDate:"January 16, 2022",tags:[{label:"nlp",permalink:"/blog/tags/nlp"},{label:"jvm",permalink:"/blog/tags/jvm"},{label:"kotlin",permalink:"/blog/tags/kotlin"}],readingTime:3.635,truncated:!0,authors:[{name:"Hampus Lond\xf6g\xe5rd",title:"Main Contributor of Londogard",url:"https://github.com/lundez",imageURL:"https://github.com/lundez.png",key:"hlondogard"}],frontMatter:{description:"nlp (londogard-nlp-toolkit) has had it's 1.1.0 release recently with a lot of new functionality and multiple improvements to efficiency, dive in to understand more!",tags:["nlp","jvm","kotlin"],title:"Release nlp (londogard-nlp-toolkit) 1.1.0",authors:"hlondogard"},prevItem:{title:"KotlinJS, ONNX and Deep Learning in the browser",permalink:"/blog/kotlinjs-onnx-deep-learning-browser"},nextItem:{title:"Seam Carving (Presentation & Workshop)",permalink:"/blog/2021/05/17/seam-carving"}},d={authorsImageUrls:[void 0]},c=[{value:"Usage of Vectorizers",id:"usage-of-vectorizers",children:[],level:3},{value:"Classifiers",id:"classifiers",children:[{value:"Usage of Classifiers",id:"usage-of-classifiers",children:[],level:3}],level:2},{value:"Unsupervised Keyword Extraction",id:"unsupervised-keyword-extraction",children:[{value:"Usage of Keyword Extraction",id:"usage-of-keyword-extraction",children:[],level:3}],level:2},{value:"Embedding Improvements",id:"embedding-improvements",children:[],level:2}],u={toc:c};function m(e){var t=e.components,n=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"The 1.1.0 release of nlp (",(0,o.kt)("a",{parentName:"p",href:"https://github.com/londogard/londogard-nlp-toolkit"},"londogard-nlp-toolkit"),") by londogard is ",(0,o.kt)("strong",{parentName:"p"},"finally")," here!"),(0,o.kt)("p",null,"I\u2019m writing this small blog-post mainly to showcase some of the new things possible now that we\u2019re moving into classifer-space!",(0,o.kt)("br",{parentName:"p"}),"\n","This release took some time to complete because there was some big restructuring and custom implementations required. One thing that I wasn't expecting was to implement my own Sparse Matrix on top of ",(0,o.kt)("inlineCode",{parentName:"p"},"multik")," because there's currently no support. Without sparsity text features will make your memory dissapear before you take your second breath! \ud83d\ude05",(0,o.kt)("br",{parentName:"p"}),"\n","Luckily I managed to get something up and running. The code is now cleaner and more efficient than previously on top of all the new features."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"N.B."),(0,o.kt)("br",{parentName:"p"}),"\n","Most of the examples are taken from ",(0,o.kt)("inlineCode",{parentName:"p"},"/src/test"),"."),(0,o.kt)("h1",{id:"vectorizers"},"Vectorizers"),(0,o.kt)("p",null,"The first part I'd like to present is the tooling that required sparse matrices, ",(0,o.kt)("em",{parentName:"p"},"vectorizers"),". TF-IDF, Bag of Words & BM-25 requires huge matrices that are very sparse, having it all in memory would be crazy as > 90% is empty (=0.0).",(0,o.kt)("br",{parentName:"p"}),"\n","Let's look at the vectorizers that now exists:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"Bag of Words"),", also called Count Vectorizer in ",(0,o.kt)("inlineCode",{parentName:"p"},"sklearn"),"."),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"This vectorizer takes words and assign a unique number to each, which is then filled in the final vector"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"TF-IDF")),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"This vectorizer assigns values to word based on their ",(0,o.kt)("em",{parentName:"li"},"term frequency & inverse-document frequency.")," Which is a ",(0,o.kt)("strong",{parentName:"li"},"incredible strong baseline.")," (",(0,o.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Tf%E2%80%93idf"},"Wikipedia.org"),")"))),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},(0,o.kt)("strong",{parentName:"p"},"BM-25")),(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"This vectorizer is a improvement on top of TF-IDF used by Elastic Search among others. The difference is that BM-25 also base the magnitude on the sentences length, in TF-IDF sometimes long sentences tend to get very high magnitude. (",(0,o.kt)("a",{parentName:"li",href:"https://en.wikipedia.org/wiki/Okapi_BM25"},"Wikipedia"),")")))),(0,o.kt)("p",null,"And yes, it\u2019s possible to vectorize ",(0,o.kt)("strong",{parentName:"p"},"with ngrams"),"! \ud83e\udd73",(0,o.kt)("br",{parentName:"p"}),"\n","And yes (x2), it\u2019s using ",(0,o.kt)("strong",{parentName:"p"},"Sparse Matrices")," to keep performance at top! \ud83e\udd29"),(0,o.kt)("p",null,"All in all this puts us very close to the famous ",(0,o.kt)("strong",{parentName:"p"},"sklearn")," in terms of versatility."),(0,o.kt)("h3",{id:"usage-of-vectorizers"},"Usage of Vectorizers"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-kotlin"},'val simpleTok = SimpleTokenizer()  \nval simpleTexts = listOf("hello world!", "this is a few sentences")  \n    .map(simpleTok::split)  \nval tfidf = TfIdfVectorizer<Float>() // replace by CountVectorizer or Bm25Vectorizer  \n  \nval lhs = tfidf.fitTransform(simpleTexts)  \nprintln("Vectorized: $lhs")\n')),(0,o.kt)("h2",{id:"classifiers"},"Classifiers"),(0,o.kt)("p",null,"And the first feature built on top of the new vectors... ",(0,o.kt)("strong",{parentName:"p"},"classifiers"),"!",(0,o.kt)("br",{parentName:"p"}),"\n","To be able to figure out if a tweet is negative or positive we need to classify the text, based on the vectorized data.",(0,o.kt)("br",{parentName:"p"}),"\n","The following classifiers are added for now:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic Regression using Stochastic Gradient Descent as optimizer"),(0,o.kt)("li",{parentName:"ul"},"Na\xefve Bayes classifier"),(0,o.kt)("li",{parentName:"ul"},"Hidden Markov Model to classify sequences with a sequence output, e.g. ",(0,o.kt)("em",{parentName:"li"},"Part of Speech")," (PoS) or ",(0,o.kt)("em",{parentName:"li"},"Named Entitiy Recognition")," (NER).")),(0,o.kt)("h3",{id:"usage-of-classifiers"},"Usage of Classifiers"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-kotlin"},"val tfidf = TfIdfVectorizer<Float>()  \nval naiveBayes = NaiveBayes() // replace by LogisticRegression if needed  \n  \nval out = tfidf.fitTransform(simpleTexts)  \nnaiveBayes.fit(out, y)  \n  \nnaiveBayes.predict(out) shouldBeEqualTo y\n")),(0,o.kt)("p",null,"and for sequences:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-kotlin"},"val (tokensText, tagsText) = text  \n    .split('\\\\n')  \n    .map {  \n        val (a, b) = it.split('\\\\t')  \n        a to b  \n    }.unzip()  \nval tokenMap = (tokensText).toSet().withIndex().associate { elem -> elem.value to elem.index }  \nval tagMap = (tagsText + \"BOS\").toSet().withIndex().associate { elem -> elem.value to elem.index }  \nval reversetagMap = tagMap.asIterable().associate { (key, value) -> value to key }  \nval hmm = HiddenMarkovModel(  \n    tagMap.asIterable().associate { (key, value) -> value to key },  \n    tokenMap.asIterable().associate { (key, value) -> value to key },  \n    BegginingOfSentence = tokenMap.getOrDefault(\"BOS\", 0)  \n    )  \n  \nval x = listOf(mk.ndarray(tokensText.mapNotNull(tokenMap::get).toIntArray()))  \nval y = listOf(mk.ndarray(tagsText.mapNotNull(tagMap::get).toIntArray()))  \n  \n  \nhmm.fit(x, y)  \n// predict.map { t -> t.data.map { reversetagMap\\[it\\] } } to get the real labels!  \nhmm.predict(x) shouldBeEqualTo y\n")),(0,o.kt)("h2",{id:"unsupervised-keyword-extraction"},"Unsupervised Keyword Extraction"),(0,o.kt)("p",null,"I couldn't keep my release small enough... so I added a little gem, ",(0,o.kt)("strong",{parentName:"p"},"automatic keyword extraction"),"! This tool is very fast and efficient at doing what it\u2019s doing and is based on a Co-Occurrence Statistical Information algorithm proposed by Y. Matsuo & M. Ishizuka in the following ",(0,o.kt)("a",{parentName:"p",href:"https://www.researchgate.net/publication/2572200_Keyword_Extraction_from_a_Single_Document_using_Word_Co-occurrence_Statistical_Information"},"paper"),".",(0,o.kt)("br",{parentName:"p"}),"\n","I think this is incredibly useful when you need something fast, cheap and that takes you 90% of the way!"),(0,o.kt)("h3",{id:"usage-of-keyword-extraction"},"Usage of Keyword Extraction"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-kotlin"},'val keywords = CooccurrenceKeywords.keywords("Londogard NLP toolkit is works on multiple languages.\\\\nAn amazing piece of NLP tech.\\\\nThis is how to fetch keywords! ")  \n  \nkeywords shouldBeEqualTo listOf(listOf("nlp") to 2)\n')),(0,o.kt)("h2",{id:"embedding-improvements"},"Embedding Improvements"),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"LightWordEmbeddings"),"\xa0 have had their cache updated into a optimal cache by\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"caffeine")," , which instead of being randomly deleted from cache takes the least used and remove. This will improve performance greatly!"),(0,o.kt)("hr",null),(0,o.kt)("p",null,"That\u2019s it, I\u2019m hoping to release a spaCy-like API during 2022, including Neural Networks. Here\u2019s to the future! \ud83c\udf7e"))}m.isMDXComponent=!0}}]);